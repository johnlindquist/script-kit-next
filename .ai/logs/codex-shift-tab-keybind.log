==== spawn @ 2026-02-08T02:46:51.515Z ====
cmd: codex "exec" "--cd" "/Users/johnlindquist/dev/script-kit-gpui" "--model" "gpt-5.3-codex" "-c" "model_reasoning_effort=\"xhigh\"" "-c" "model_auto_compact_token_limit=120000" "-c" "developer_instructions=\"You are Codex implementer agent \\\"codex-shift-tab-keybind\\\".\\n\\n## Coordination\\nThis repo runs parallel agents in ONE working tree. Before editing any file, you MUST claim it.\\nCommands (run via shell):\\n  - View roster: ~/.claude/bin/swarm.ts status\\n  - Claim files (blocks until free): ~/.claude/bin/swarm.ts claim --wait --id codex-shift-tab-keybind path/to/file\\n  - Release: ~/.claude/bin/swarm.ts release --id codex-shift-tab-keybind path/to/file\\n  - Update status/notes: ~/.claude/bin/swarm.ts update --id codex-shift-tab-keybind --status in_progress --notes \\\"...\\\"\\nRules:\\n  - Never edit a file you haven't claimed.\\n  - Keep claims tight (claim only what you're actively changing).\\n  - If blocked, set status=blocked and say what you're waiting on.\\n  - When finished, set status=done and release claims.\\n\\n## Development practices (this code is maintained by AI agents)\\n  - Log state transitions with enough context to diagnose failures from logs alone.\\n  - Structured errors: include what was attempted, what failed, and current state.\\n  - Name things for grepability — unique identifiers agents can find on the first search.\\n  - Every behavior change gets a test. Untested code is invisible to the next agent.\\n  - Test names describe the scenario: test_X_does_Y_when_Z.\\n  - Keep functions small and single-purpose. 500-line functions burn agent context.\\n  - Use types to encode constraints (enums > strings for errors).\\n\\n## CRITICAL: Parallel-safe verification\\n  - Other agents are modifying other files RIGHT NOW. Full test suites WILL show their failures.\\n  - ONLY run tests that cover YOUR changed files. Scope every test/check command:\\n    Rust: cargo test -p your-crate -- module::test_name | JS: npx jest your/file.test.ts\\n  - Do NOT run: cargo test (unscoped), cargo check (whole workspace), npm test (unscoped).\\n  - If a scoped test fails, it's YOUR bug — fix it. If tests outside your scope fail, ignore them.\\n\\n## Git commit discipline\\n  - NEVER commit unverified work. Run tests/lints/type-checks FIRST. If it fails, fix it before committing.\\n  - A commit is a declaration: \\\"this works and here's the proof.\\\"\\n  - Commit FREQUENTLY — after each meaningful unit of VERIFIED work, not just at the end.\\n  - A \\\"unit\\\" = one logical change that passes verification (new function, bug fix, refactor, test added).\\n  - NEVER batch an entire task into one giant commit. Small commits are searchable; big ones aren't.\\n  - Commit message format:\\n      Line 1: <type>(<scope>): <what changed> (imperative, max 72 chars)\\n      Line 3+: WHY this change was made, WHAT was verified, and HOW to test it.\\n    Types: feat, fix, refactor, test, docs, chore\\n    Example:\\n      feat(auth): add refresh token rotation\\n      \\n      Tokens now rotate on each refresh call to prevent replay attacks.\\n      The old token is invalidated immediately on rotation.\\n      \\n      Verified: cargo test --lib -- auth::refresh (4 tests pass)\\n      Verified: manual smoke test via curl — token rotates, old token returns 401\\n  - The \\\"Verified:\\\" lines are REQUIRED. Future agents will read git log to understand\\n    what was tested and how to re-verify. This is the most valuable part of the message.\\n  - In your final message, list all commits you made with their hashes.\\n\\n## Time budget\\nYou have approximately 10 minutes. If your task is too broad to complete in time:\\n(1) Commit any verified progress.\\n(2) Run: ~/.claude/bin/swarm.ts update --id YOUR_ID --status needs_split --notes 'suggest: 1) sub-task-desc scope:files 2) sub-task-desc scope:files'.\\n(3) Exit cleanly. The dispatcher will read your suggestions and spawn narrower workers.\"" "--output-last-message" "/Users/johnlindquist/dev/script-kit-gpui/.ai/logs/codex-shift-tab-keybind.final.md" "--yolo" "You are agent codex-shift-tab-keybind.\n\nCurrent parallel-agent roster (read this first):\n```\nSWARM SNAPSHOT @ 2026-02-08T02:46:51.512Z\n- codex-shift-tab-keybind [starting] scope=src/actions/dialog/**,src/app_impl/**,src/ai/**,src/execute_script/** task=In Script Kit GPUI, pressing Tab in the main prompt triggers an AI chat scenario. Add a Shift+Tab keyboard shortcut tha…\n```\n\nYour assignment:\n- Task: In Script Kit GPUI, pressing Tab in the main prompt triggers an AI chat scenario. Add a Shift+Tab keyboard shortcut that instead triggers AI-powered script generation. The keyboard handling is in the actions/dialog modules and app_impl. Find where Tab is handled for AI chat (likely in key event handlers in dialog or app_impl), and add a parallel Shift+Tab path. When Shift+Tab is pressed: 1) Capture the current input text as the script description/prompt 2) Route to a new 'generate script' action that sends the prompt to the AI backend with instructions to generate a Script Kit script following best practices (use metadata comments like // Name:, // Description:, await arg(), await div(), etc.) 3) The generated script should be saved and opened for editing. Look at how Tab triggers AI chat for the pattern to follow. Key convention: match both variants like 'tab' | 'Tab'. Check for shift modifier in the key event. The AI prompt should instruct the model to generate a complete Script Kit script with proper metadata headers and idiomatic Script Kit APIs.\n- Scope: src/actions/dialog/**,src/app_impl/**,src/ai/**,src/execute_script/**\n\nOperating constraints:\n- Single working tree (no worktrees, no patches).\n- You MUST claim files before editing (~/.claude/bin/swarm.ts claim --wait ...).\n\nDeliverables:\n- Make the code changes required by the task.\n- Run the most relevant tests/linters.\n- In your FINAL message, include: changed files, how to test, and any risks/known gaps."

OpenAI Codex v0.98.0 (research preview)
--------
workdir: /Users/johnlindquist/dev/script-kit-gpui
model: gpt-5.3-codex
provider: openai
approval: never
sandbox: danger-full-access
reasoning effort: xhigh
reasoning summaries: auto
session id: 019c3b25-13af-7b10-ba74-09445c3cb972
--------
user
You are agent codex-shift-tab-keybind.

Current parallel-agent roster (read this first):
```
SWARM SNAPSHOT @ 2026-02-08T02:46:51.512Z
- codex-shift-tab-keybind [starting] scope=src/actions/dialog/**,src/app_impl/**,src/ai/**,src/execute_script/** task=In Script Kit GPUI, pressing Tab in the main prompt triggers an AI chat scenario. Add a Shift+Tab keyboard shortcut tha…
```

Your assignment:
- Task: In Script Kit GPUI, pressing Tab in the main prompt triggers an AI chat scenario. Add a Shift+Tab keyboard shortcut that instead triggers AI-powered script generation. The keyboard handling is in the actions/dialog modules and app_impl. Find where Tab is handled for AI chat (likely in key event handlers in dialog or app_impl), and add a parallel Shift+Tab path. When Shift+Tab is pressed: 1) Capture the current input text as the script description/prompt 2) Route to a new 'generate script' action that sends the prompt to the AI backend with instructions to generate a Script Kit script following best practices (use metadata comments like // Name:, // Description:, await arg(), await div(), etc.) 3) The generated script should be saved and opened for editing. Look at how Tab triggers AI chat for the pattern to follow. Key convention: match both variants like 'tab' | 'Tab'. Check for shift modifier in the key event. The AI prompt should instruct the model to generate a complete Script Kit script with proper metadata headers and idiomatic Script Kit APIs.
- Scope: src/actions/dialog/**,src/app_impl/**,src/ai/**,src/execute_script/**

Operating constraints:
- Single working tree (no worktrees, no patches).
- You MUST claim files before editing (~/.claude/bin/swarm.ts claim --wait ...).

Deliverables:
- Make the code changes required by the task.
- Run the most relevant tests/linters.
- In your FINAL message, include: changed files, how to test, and any risks/known gaps.
warning: Under-development features enabled: responses_websockets. Under-development features are incomplete and may behave unpredictably. To suppress this warning, set `suppress_unstable_features_warning = true` in /Users/johnlindquist/.codex/config.toml.
mcp: deepwiki starting
mcp: deepwiki ready
mcp startup: ready: deepwiki

thinking
**Planning claim and commit strategy**
codex
I’m going to locate the existing `Tab` AI-chat key path in `src/actions/dialog` and `src/app_impl`, then mirror it with a `Shift+Tab` path for script generation. I’ll inspect handlers first, then claim only the files I need before editing.

thinking
**Checking repo status**
exec
/bin/zsh -lc '~/.claude/bin/swarm.ts status' in /Users/johnlindquist/dev/script-kit-gpui succeeded in 52ms:
Repo: /Users/johnlindquist/dev/script-kit-gpui
Updated: 2026-02-08T02:47:02.176Z

Active agents (1):
codex-shift-tab-keybind [in_progress] codex/gpt-5.3-codex/exec
  task: In Script Kit GPUI, pressing Tab in the main prompt triggers an AI chat scenario. Add a Shift+Tab keyboard shortcut tha…
  scope: src/actions/dialog/**,src/app_impl/**,src/ai/**,src/execute_script/**
  claims: (none)
  notes: spawned codex (exec)

Inactive agents (253):
- codex-accessibility [done] done. exit=0. final=.ai/logs/codex-accessibility.final.md. See final message file.
- codex-action-labels [failed] failed. exit=1. final=.ai/logs/codex-action-labels.final.md.
- codex-actions-audit [failed] failed. exit=1. final=.ai/logs/codex-actions-audit.final.md. See final message file.
- codex-actions-consistency [failed] failed. exit=1. final=.ai/logs/codex-actions-consistency.final.md.
- codex-actions-dialog [done] done. exit=0. final=.ai/logs/codex-actions-dialog.final.md. See final message file.
- codex-actions-feedback [failed] failed. exit=1. final=.ai/logs/codex-actions-feedback.final.md.
- codex-actions-feedback-v2 [done] done. exit=0. final=.ai/logs/codex-actions-feedback-v2.final.md. See final message file.
- codex-actions-ux [done] done. exit=0. final=.ai/logs/codex-actions-ux.final.md. See final message file.
- codex-agents-md [done] done. exit=0. final=.ai/logs/codex-agents-md.final.md. See final message file.

thinking
**Planning parallel code searches**
codex
I confirmed no files are claimed yet. Next I’m scanning for where `Tab` invokes AI chat so I can add a parallel `Shift+Tab` generate-script branch in the same flow.
exec
/bin/zsh -lc "rg -n \"\\btab\\b|\\bTab\\b|ai chat|AI chat|shift\" src/actions/dialog src/app_impl src/ai src/execute_script" in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc 'rg -n "BuiltInFeature|execute_builtin|get_builtin_entries|chat|generate" src/actions/dialog src/app_impl src/ai src/execute_script' in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
src/ai/model.rs:3://! Core data structures for the AI chat window feature.
src/actions/dialog/part_01.rs:263:/// - `search_position`: Top (AI chat style) or Bottom (main menu style)
src/app_impl/startup_new_actions.rs:16:                let has_shift = event.keystroke.modifiers.shift;
src/app_impl/startup_new_actions.rs:22:                        // NOTE: Tab is handled by the dedicated Tab interceptor above, so
src/app_impl/startup_new_actions.rs:24:                        if key != "tab"
src/app_impl/startup_new_actions.rs:35:                        if has_cmd && key == "k" && !has_shift {
src/app_impl/startup_new_actions.rs:126:                        if has_cmd && key == "k" && has_shift
src/app_impl/startup_new_actions.rs:142:                            if has_cmd && !has_shift && (key == "-" || key == "minus") {
src/app_impl/startup_new_actions.rs:158:                            if has_cmd && !has_shift && key == "m" {
src/app_impl/startup_new_actions.rs:171:                            if has_cmd && has_shift && key == "a" {
src/ai/storage.rs:3://! SQLite-backed persistence for AI chats with CRUD operations and FTS5 search.
src/ai/storage.rs:15:/// Global database connection for AI chats
src/ai/storage.rs:18:/// Get the path to the AI chats database (~/.scriptkit/db/ai-chats.sqlite)
src/ai/storage.rs:27:/// Initialize the AI chats database
src/ai/storage.rs:45:    let conn = Connection::open(&db_path).context("Failed to open AI chats database")?;
src/ai/storage.rs:200:    info!(db_path = %db_path.display(), "AI chats database initialized");
src/app_impl/shortcuts_hud_grid.rs:18:        let has_shift = event.keystroke.modifiers.shift;
src/app_impl/shortcuts_hud_grid.rs:28:        if has_cmd && has_shift && key_str == "m" {
src/app_impl/shortcuts_hud_grid.rs:37:        if has_cmd && has_shift && key_str == "p" {
src/actions/dialog/part_04/body_part_03.rs:104:            .overflow_hidden() // Prevent any content from causing shifts
 succeeded in 51ms:
Total output lines: 1221

src/actions/dialog/part_01.rs:20:    format_shortcut_hint as format_shortcut_hint_shared, get_chat_context_actions,
src/actions/dialog/part_01.rs:263:/// - `search_position`: Top (AI chat style) or Bottom (main menu style)
src/ai/model.rs:3://! Core data structures for the AI chat window feature.
src/ai/model.rs:10:/// Unique identifier for a chat conversation
src/ai/model.rs:43:/// Role of a message in a chat conversation
src/ai/model.rs:90:/// Source of a chat (where it originated from)
src/ai/model.rs:97:    /// Chat from the chat() SDK prompt
src/ai/model.rs:108:            ChatSource::ChatPrompt => "chat_prompt",
src/ai/model.rs:116:            "chat_prompt" => ChatSource::ChatPrompt,
src/ai/model.rs:123:/// A chat conversation
src/ai/model.rs:129:    /// Chat title (auto-generated from first message or user-set)
src/ai/model.rs:132:    /// When the chat was created
src/ai/model.rs:135:    /// When the chat was last modified
src/ai/model.rs:138:    /// When the chat was soft-deleted (None = not deleted)
src/ai/model.rs:147:    /// Source of the chat (ai_window, chat_prompt, script)
src/ai/model.rs:153:    /// Create a new empty chat with the specified model and provider
src/ai/model.rs:168:    /// Create a new chat with a specific source
src/ai/model.rs:185:    /// Check if this chat is in the trash

thinking
**Exploring app_impl directory structure**
exec
/bin/zsh -lc "ls -1 src/app_impl | sed -n '1,200p'" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
actions_dialog.rs
actions_toggle.rs
alias_input.rs
chat_actions.rs
clipboard_actions_focus_routing_tests.rs
execution_paths.rs
execution_scripts.rs
filter_input_change.rs
filter_input_core.rs
filter_input_updates.rs
filtering_cache.rs
keyboard_routing_tests.rs
lifecycle_reset.rs
mod.rs
prompt_ai.rs
refresh_scriptlets.rs
registries_state.rs
selection_fallback.rs
shortcut_recorder.rs
shortcuts_hud_grid.rs

thinking
**Inspecting startup file inclusion**
codex
I found a dedicated `startup_new_tab.rs` interceptor that likely contains the exact `Tab` logic we need. I’m reading that path and nearby AI prompt code to implement `Shift+Tab` without changing unrelated behavior.
exec
/bin/zsh -lc "sed -n '520,820p' src/app_impl/startup.rs" in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc "sed -n '1,260p' src/app_impl/startup_new_tab.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 50ms:
        // Add Tab key interceptor for "Ask AI" feature and file search directory navigation
        // This fires BEFORE normal key handling, allowing us to intercept Tab
        // even when the Input component has focus
        let app_entity_for_tab = cx.entity().downgrade();
        let tab_interceptor = cx.intercept_keystrokes({
            let app_entity = app_entity_for_tab;
            move |event, window, cx| {
                let key = event.keystroke.key.to_lowercase();
                let has_shift = event.keystroke.modifiers.shift;
                // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
                if key == "tab"
                    && !event.keystroke.modifiers.platform
                    && !event.keystroke.modifiers.alt
                    && !event.keystroke.modifiers.control
                {
                    if let Some(app) = app_entity.upgrade() {
                        app.update(cx, |this, cx| {
                            // FIRST: If confirm dialog is open, route Tab to it for button switching
                            let confirm_open = crate::confirm::is_confirm_window_open();
                            crate::logging::log(
 succeeded in 50ms:
                    );
                }
            });

            cx.spawn(async move |this, cx| {
                let Ok(registry) = rx.recv().await else {
                    logging::log(
                        "APP",
                        "Background provider registry build failed: channel closed",
                    );
                    return;
                };

                let provider_count = registry.provider_ids().len();
                let _ = cx.update(|cx| {
                    this.update(cx, |app, _cx| {
                        app.cached_provider_registry = Some(registry);
                        logging::log(
                            "APP",
                            &format!(

thinking
**Inspecting startup file includes**
exec
/bin/zsh -lc "sed -n '1,220p' src/app_impl/startup.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 52ms:
use super::*;

pub(super) fn calculate_fallback_error_message(expression: &str) -> String {
    format!(
        "Could not evaluate expression \"{}\". Check the syntax and try again.",
        expression
    )
}

impl ScriptListApp {
    pub(crate) fn new(
        config: config::Config,
        bun_available: bool,
        window: &mut Window,
        cx: &mut Context<Self>,
    ) -> Self {
        // PERF: Parallelize script + scriptlet loading to reduce startup wall time.
        let load_start = std::time::Instant::now();
        let (scripts, scriptlets, scripts_elapsed, scriptlets_elapsed) = std::thread::scope(
            |scope| {

thinking
**Checking module includes**
exec
/bin/zsh -lc "sed -n '1,260p' src/app_impl/mod.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 50ms:
#[path = "startup.rs"]
mod startup;
#[path = "theme_focus.rs"]
mod theme_focus;
#[path = "refresh_scriptlets.rs"]
mod refresh_scriptlets;
#[path = "filtering_cache.rs"]
mod filtering_cache;
#[path = "selection_fallback.rs"]
mod selection_fallback;
#[path = "filter_input_core.rs"]
mod filter_input_core;
#[path = "filter_input_change.rs"]
mod filter_input_change;
#[path = "filter_input_updates.rs"]
mod filter_input_updates;
#[path = "ui_window.rs"]
mod ui_window;
#[path = "actions_toggle.rs"]
mod actions_toggle;

thinking
**Inspecting startup file references**
exec
/bin/zsh -lc "rg -n \"startup_new_tab|startup_new_actions|startup_new_arrow|startup_new_state|startup_new_prelude\" src/app_impl src | sed -n '1,200p'" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
src/app_impl/clipboard_actions_focus_routing_tests.rs:13:            "src/app_impl/startup_new_arrow.rs",
src/app_impl/clipboard_actions_focus_routing_tests.rs:14:            "src/app_impl/startup_new_actions.rs",
src/app_impl/keyboard_routing_tests.rs:24:            "src/app_impl/startup_new_prelude.rs",
src/app_impl/keyboard_routing_tests.rs:25:            "src/app_impl/startup_new_arrow.rs",
src/app_impl/keyboard_routing_tests.rs:26:            "src/app_impl/startup_new_actions.rs",
src/app_impl/tests/list_state_init.rs:26:            "src/app_impl/startup_new_prelude.rs",
src/app_impl/tests/list_state_init.rs:27:            "src/app_impl/startup_new_state.rs",
src/app_impl/tests/list_state_init.rs:28:            "src/app_impl/startup_new_tab.rs",
src/app_impl/tests/list_state_init.rs:29:            "src/app_impl/startup_new_arrow.rs",
src/app_impl/tests/list_state_init.rs:30:            "src/app_impl/startup_new_actions.rs",
src/keyboard_routing_tests.rs:24:            "src/app_impl/startup_new_prelude.rs",
src/keyboard_routing_tests.rs:25:            "src/app_impl/startup_new_arrow.rs",
src/keyboard_routing_tests.rs:26:            "src/app_impl/startup_new_actions.rs",
src/app_impl/clipboard_actions_focus_routing_tests.rs:13:            "src/app_impl/startup_new_arrow.rs",
src/app_impl/clipboard_actions_focus_routing_tests.rs:14:            "src/app_impl/startup_new_actions.rs",
src/app_impl/keyboard_routing_tests.rs:24:            "src/app_impl/startup_new_prelude.rs",
src/app_impl/keyboard_routing_tests.rs:25:            "src/app_impl/startup_new_arrow.rs",
src/app_impl/keyboard_routing_tests.rs:26:            "src/app_impl/startup_new_actions.rs",
src/app_impl/tests/list_state_init.rs:26:            "src/app_impl/startup_new_prelude.rs",
src/app_impl/tests/list_state_init.rs:27:            "src/app_impl/startup_new_state.rs",

thinking
**Reviewing test fixtures for updates**
codex
I confirmed `startup.rs` is the compiled path, but `startup_new_tab.rs` is also checked by tests as a source mirror. I’m now inspecting those tests so we can keep both files consistent and avoid regressions.
exec
/bin/zsh -lc "sed -n '1,220p' src/app_impl/tests/list_state_init.rs" in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc "sed -n '1,220p' src/app_impl/keyboard_routing_tests.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
// Regression tests for list state initialization.
//
// These tests ensure that the main list state is properly initialized
// during ScriptListApp::new() so that scripts are visible on first render.
//
// ## Background
// A bug was introduced when state mutations were moved from render() to event handlers.
// The fix moved selection validation and list sync to event handlers, but forgot to
// call these methods during initialization. This caused the first open of the main menu
// to show "No scripts or snippets found" because main_list_state started with 0 items.
//
// The fix is to call sync_list_state() and validate_selection_bounds() at the end
// of ScriptListApp::new() to ensure the list state is properly initialized.
//
// ## Related Commits
// - "fix: move state mutations from render to event handlers" (introduced bug)
// - "fix: sync list state on initialization" (fixed bug)

#[cfg(test)]
mod tests {
 succeeded in 51ms:
//! Regression tests for keyboard routing in various app views.
//!
//! These tests ensure that keyboard events are routed correctly based on:
//! - Current view (ScriptList, FileSearchView, etc.)
//! - Whether actions popup is open
//! - Modifier key state
//!
//! ## Background
//! A bug was discovered where arrow keys in ScriptList view were navigating
//! the script list instead of the actions dialog when the popup was open.
//! This was fixed by adding a `show_actions_popup` check before handling
//! arrow keys in the ScriptList case.
//!
//! ## Code Audit Tests
//! These tests verify that the keyboard routing patterns are consistent
//! across all views that support actions popups.

#[cfg(test)]
mod tests {
    use std::fs;

thinking
**Investigating script generation features**
exec
/bin/zsh -lc 'rg -n "generate script|script generation|AI.*script|script.*AI|show_inline_ai_chat|generate.*script" src/app_impl src/actions/dialog src/ai src/execute_script' in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc "rg -n \"BuiltInFeature|AiChat|Ask AI|execute_builtin\\(|chat prompt|ChatPrompt\" src | sed -n '1,260p'" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
src/ai/storage.rs:18:/// Get the path to the AI chats database (~/.scriptkit/db/ai-chats.sqlite)
src/app_impl/prompt_ai.rs:158:    pub fn show_inline_ai_chat(&mut self, initial_query: Option<String>, cx: &mut Context<Self>) {
src/app_impl/startup.rs:736:                                this.show_inline_ai_chat(Some(query), cx);
src/ai/window/types.rs:153:                description: "Helpful AI assistant for any task",
src/ai/window/types.rs:311:        return "I'm here to help! In demo mode, I can explain Script Kit concepts:\n\n• **Scripts** live in `~/.scriptkit/scripts/`\n• **SDK** provides `arg()`, `div()`, `editor()`, and more\n• **Hotkeys** are configured in script metadata\n• **This AI chat** works with Claude or GPT when you add an API key\n\nWhat would you like to know more about?".to_string();
src/ai/window/types.rs:319:        return "To enable real AI responses, configure an API key:\n\n**For Claude (Anthropic):**\n```bash\nexport SCRIPT_KIT_ANTHROPIC_API_KEY=\"sk-ant-...\"\n```\n\n**For GPT (OpenAI):**\n```bash\nexport SCRIPT_KIT_OPENAI_API_KEY=\"sk-...\"\n```\n\nAdd these to your `~/.zshrc` or `~/.scriptkit/.env` file, then restart Script Kit.".to_string();
src/ai/mod.rs:20://! The AI chats database is stored at `~/.scriptkit/ai-chats.db`.
src/app_impl/startup_new_tab.rs:187:                                this.show_inline_ai_chat(Some(query), cx);
src/ai/sdk_handlers.rs:3://! This module handles AI SDK protocol messages from scripts.
 succeeded in 51ms:
src/app_actions/tests/actions_button_visibility.rs:64:            .expect("Ask AI hint container not found in src/render_script_list/part_000.rs");
src/app_actions/tests/actions_button_visibility.rs:69:            "Ask AI hint should be non-clickable (cursor_default). Section:\n{}",
src/app_actions/tests/actions_button_visibility.rs:74:            "Ask AI hint should not imply clickability with cursor_pointer. Section:\n{}",
src/list_item/part_000.rs:199:// Header Area Constants (Ask AI button, Tab badge, indicators)
src/list_item/part_000.rs:202:/// Gap between "Ask AI" text and "Tab" badge
src/list_item/part_000.rs:204:/// Ask AI button horizontal padding
src/list_item/part_000.rs:206:/// Ask AI button vertical padding
src/list_item/part_000.rs:208:/// Ask AI button corner radius
src/prompt_handler/part_001.rs:825:                    AppView::ChatPrompt { id, .. } => (
src/prompt_handler/part_001.rs:1651:                // Create submit callback for chat prompt
src/prompt_handler/part_001.rs:1676:                // Create ChatPrompt entity with configured models
src/prompt_handler/part_001.rs:1678:                let mut chat_prompt = prompts::ChatPrompt::new(
src/prompt_handler/part_001.rs:1760:                // The ChatPrompt's on_show_actions callback is not needed when main app handles it
src/prompt_handler/part_001.rs:1762:                logging::bench_log("ChatPrompt_creating");
src/prompt_handler/part_001.rs:1764:                self.current_view = AppView::ChatPrompt { id, entity };
src/prompt_handler/part_001.rs:1766:                self.pending_focus = Some(FocusTarget::ChatPrompt);
src/prompt_handler/part_001.rs:1767:                logging::bench_log("ChatPrompt_created");
src/prompt_handler/part_001.rs:1777:                if let AppView::ChatPrompt {
src/prompt_handler/part_001.rs:1798:                if let AppView::ChatPrompt {
src/prompt_handler/part_001.rs:1815:                if let AppView::ChatPrompt {

thinking
**Inspecting AI prompt handling**
exec
/bin/zsh -lc "sed -n '1,320p' src/app_impl/prompt_ai.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
use super::*;

impl ScriptListApp {
    pub(crate) fn is_in_prompt(&self) -> bool {
        matches!(
            self.current_view,
            AppView::ArgPrompt { .. }
                | AppView::DivPrompt { .. }
                | AppView::FormPrompt { .. }
                | AppView::TermPrompt { .. }
                | AppView::EditorPrompt { .. }
                | AppView::ClipboardHistoryView { .. }
                | AppView::AppLauncherView { .. }
                | AppView::WindowSwitcherView { .. }
                | AppView::DesignGalleryView { .. }
                | AppView::ScratchPadView { .. }
                | AppView::QuickTerminalView { .. }
        )
    }


thinking
**Exploring AI script generation logic**
exec
/bin/zsh -lc 'rg -n "with_builtin_ai|with_pending_submit|pending_submit|on_ai_response|ai_response|script generation|generate.*script" src/prompts src/app_impl src/ai src/execute_script' in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
src/app_impl/prompt_ai.rs:258:        .with_builtin_ai(registry, true); // true = prefer Vercel AI Gateway
src/app_impl/prompt_ai.rs:263:            chat_prompt = chat_prompt.with_pending_submit(true);
src/prompts/chat/prompt.rs:39:    pub(super) pending_submit: bool,
src/prompts/chat/prompt.rs:117:            pending_submit: false,
src/prompts/chat/prompt.rs:274:    pub fn with_builtin_ai(mut self, registry: ProviderRegistry, prefer_vercel: bool) -> Self {
src/prompts/chat/prompt.rs:310:    /// Set pending_submit flag - when true, auto-submit input on first render
src/prompts/chat/prompt.rs:312:    pub fn with_pending_submit(mut self, submit: bool) -> Self {
src/prompts/chat/prompt.rs:313:        self.pending_submit = submit;
src/prompts/chat/render_core.rs:53:        // Process pending_submit on first render (used when Tab opens chat with query)
src/prompts/chat/render_core.rs:57:            && self.pending_submit
src/prompts/chat/render_core.rs:60:            self.pending_submit = false;
src/prompts/chat/render_core.rs:63:                "Processing pending_submit - auto-submitting query from Tab",

thinking
**Inspecting script generation builtins**
codex
I found `show_inline_ai_chat`, which auto-submits a query, but I still need the “save generated script and open editor” path. I’m now tracing existing script-creation flows (`ScriptCommandType::NewScript` and builtin handlers) so we can reuse that behavior instead of inventing a new pipeline.
exec
/bin/zsh -lc 'rg -n "ScriptCommandType::NewScript|NewScript|new script|create script|open.*editor|editor" src/app_impl src/execute_script src/builtins src/actions/dialog src/ai' in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc 'rg -n "AiCommandType|send.*ai|OpenAi|open_ai|chat" src/app_impl src/builtins src/ai src/execute_script' in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
src/builtins/part_000.rs:119:    NewScript,
src/builtins/part_000.rs:151:    /// Open scratch pad - auto-saving editor
src/builtins/part_001_entries/entries_000.rs:56:        "Open quick notes and a scratchpad editor",
src/builtins/part_001_entries/entries_002.rs:143:        BuiltInFeature::ScriptCommand(ScriptCommandType::NewScript),
src/ai/window/setup.rs:239:        use crate::config::editor::{self, WriteOutcome};
src/ai/window/setup.rs:246:        match editor::enable_claude_code_safely(&config_path, bun_path) {
src/ai/window/setup.rs:255:                if let Err(recover_err) = editor::recover_from_backup(&config_path, bun_path) {
src/builtins/part_001_entries/entries_003.rs:93:        "Open a scratch pad editor for notes and code (auto-saves to disk)",
src/builtins/part_001_entries/entries_003.rs:99:            "editor",
src/builtins/mod.rs:14://! - **Script Commands**: Create new scripts and scriptlets
src/builtins/tests/part_001.rs:74:            "Open quick notes and a scratchpad editor"
src/ai/window/types.rs:170:                system_prompt: "You are a skilled writer and editor. Help improve writing clarity, grammar, and style.",
src/ai/window/types.rs:307:        return "Script Kit is a powerful automation tool! Here are some things you can do:\n\n1. **Create scripts** - Write TypeScript/JavaScript to automate tasks\n2. **Use prompts** - `arg()`, `editor()`, `div()` for interactive UIs\n3. **Hotkeys** - Bind scripts to global keyboard shortcuts\n4. **Snippets** - Text expansion with dynamic content\n\nTry running a script with `Cmd+;` to see it in action!".to_string();
src/ai/window/types.rs:311:        return "I'm here to help! In demo mode, I can explain Script Kit concepts:\n\n• **Scripts** live in `~/.scriptkit/scripts/`\n• **SDK** provides `arg()`, `div()`, `editor()`, and more\n• **Hotkeys** are configured in script metadata\n• **This AI chat** works with Claude or GPT when you add an API key\n\nWhat would you like to know more about?".to_string();
src/app_impl/refresh_scriptlets.rs:288:        // Parse new scriptlets from file (empty if deleted)
src/app_impl/refresh_scriptlets.rs:403:        // Add new scriptlets from this file
src/app_impl/execution_paths.rs:228:            "open_in_editor" => {
src/app_impl/execution_paths.rs:229:                // Open in configured editor
src/app_impl/execution_paths.rs:230:                let editor = self.config.get_editor();
src/app_impl/execution_paths.rs:234:                    &format!("Opening in editor '{}': {}", editor, path_str),
 succeeded in 50ms:
Total output lines: 1249

src/builtins/part_000.rs:95:pub enum AiCommandType {
src/builtins/part_000.rs:96:    OpenAi,
src/builtins/part_000.rs:109:    /// Create a new AI chat preset/template
src/builtins/part_000.rs:111:    /// Import AI chat presets from file
src/builtins/part_000.rs:113:    /// Search through saved AI chat presets
src/builtins/part_000.rs:142:    ConfigureOpenAiApiKey,
src/builtins/part_000.rs:217:    AiCommand(AiCommandType),
src/builtins/part_001_entries/entries_000.rs:34:        "builtin-ai-chat",
src/builtins/part_001_entries/entries_000.rs:39:            "chat",
src/builtins/part_001_entries/entries_002.rs:49:        vec!["open", "ai", "chat", "assistant", "window"],
src/builtins/part_001_entries/entries_002.rs:50:        BuiltInFeature::AiCommand(AiCommandType::OpenAi),
src/builtins/part_001_entries/entries_002.rs:58:        vec!["new", "conversation", "chat", "ai"],
src/builtins/part_001_entries/entries_002.rs:59:        BuiltInFeature::AiCommand(AiCommandType::NewConversation),
src/builtins/part_001_entries/entries_002.rs:64:        "builtin-send-screen-to-ai",
src/builtins/part_001_entries/entries_002.rs:72:            "chat",
src/builtins/part_001_entries/entries_002.rs:76:        BuiltInFeature::AiCommand(AiCommandType::SendScreenToAi),
src/builtins/part_001_entries/entries_002.rs:81:        "builtin-send-window-to-ai",
src/builtins/part_001_entries/entries_002.rs:89:            "chat",

thinking
**Locating new script execution path**
exec
/bin/zsh -lc "rg -n \"ScriptCommandType::NewScript|ScriptCommandType::NewExtension|AiCommandType::NewConversation|AiCommandType::OpenAi|execute_builtin\" src/app_impl src/execute_script src/ai src/actions/dialog | sed -n '1,240p'" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
src/app_impl/filter_input_change.rs:438:                            self.execute_builtin(&entry, cx);
src/app_impl/selection_fallback.rs:180:                        self.execute_builtin(&builtin_match.entry, cx);
src/app_impl/selection_fallback.rs:235:                self.execute_builtin_fallback_inline(&fallback_id, &input, cx);
src/app_impl/selection_fallback.rs:275:                    self.execute_builtin_fallback_inline(&fallback_id, &input, cx);
src/app_impl/selection_fallback.rs:290:    pub(crate) fn execute_builtin_fallback_inline(
src/app_impl/execution_scripts.rs:378:                        self.execute_builtin(entry, cx);

thinking
**Searching for builtin executions**
exec
/bin/zsh -lc "rg -n \"fn execute_builtin\\(|execute_builtin\\(\" src | sed -n '1,260p'" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
src/app_impl/filter_input_change.rs:438:                            self.execute_builtin(&entry, cx);
src/app_impl/selection_fallback.rs:180:                        self.execute_builtin(&builtin_match.entry, cx);
src/app_impl/execution_scripts.rs:378:                        self.execute_builtin(entry, cx);
src/app_execute/builtin_execution.rs:20:    fn execute_builtin(&mut self, entry: &builtins::BuiltInEntry, cx: &mut Context<Self>) {

thinking
**Assessing scope limitations for edits**
exec
/bin/zsh -lc "sed -n '1,340p' src/app_execute/builtin_execution.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
fn ai_open_failure_message(error: impl std::fmt::Display) -> String {
    format!("Failed to open AI: {}", error)
}

impl ScriptListApp {
    fn system_action_feedback_message(
        &self,
        action_type: &builtins::SystemActionType,
    ) -> Option<String> {
        let dark_mode_enabled = if matches!(action_type, builtins::SystemActionType::ToggleDarkMode)
        {
            system_actions::is_dark_mode().ok()
        } else {
            None
        };

        builtins::system_action_hud_message(*action_type, dark_mode_enabled)
    }

    fn execute_builtin(&mut self, entry: &builtins::BuiltInEntry, cx: &mut Context<Self>) {

thinking
**Targeting specific code regions**
exec
/bin/zsh -lc 'rg -n "ScriptCommandType|AiCommandType|NewScript|NewExtension|OpenAi|SendSelectedTextToAi|SendScreenToAi|Generate" src/app_execute/builtin_execution.rs' in /Users/johnlindquist/dev/script-kit-gpui succeeded in 52ms:
545:                use builtins::AiCommandType;
553:                    AiCommandType::OpenAi | AiCommandType::NewConversation => {
568:                    AiCommandType::ClearConversation => {
617:                    AiCommandType::SendScreenToAi => {
667:                    AiCommandType::SendFocusedWindowToAi => {
717:                    AiCommandType::SendSelectedTextToAi => {
771:                    AiCommandType::SendBrowserTabToAi => {
808:                    AiCommandType::SendScreenAreaToAi => {
821:                    AiCommandType::CreateAiPreset
822:                    | AiCommandType::ImportAiPresets
823:                    | AiCommandType::SearchAiPresets => {
853:                use builtins::ScriptCommandType;
856:                    ScriptCommandType::NewScript => {
859:                    ScriptCommandType::NewExtension => {
860:                        // Generate a unique name with timestamp
1075:                    SettingsCommandType::ConfigureOpenAiApiKey => {
exec
/bin/zsh -lc "sed -n '820,930p' src/app_execute/builtin_execution.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:

                    AiCommandType::CreateAiPreset
                    | AiCommandType::ImportAiPresets
                    | AiCommandType::SearchAiPresets => {
                        // Preset management - open AI window with a future preset UI
                        self.toast_manager.push(
                            components::toast::Toast::info(
                                "AI Presets feature coming soon!",
                                &self.theme,
                            )
                            .duration_ms(Some(3000)),
                        );
                        if let Err(e) = ai::open_ai_window(cx) {
                            logging::log("ERROR", &ai_open_failure_message(&e));
                            self.toast_manager.push(
                                components::toast::Toast::error(
                                    ai_open_failure_message(&e),
                                    &self.theme,
                                )
                                .duration_ms(Some(5000)),

thinking
**Scoping AI script generation integration**
exec
/bin/zsh -lc "rg -n \"trait AiProvider|struct ProviderRegistry|fn generate_response|stream|complete|generate\\(\" src/ai/providers.rs" in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc "sed -n '1,260p' src/ai/providers.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 50ms:
236:/// Parse SSE (Server-Sent Events) stream and process data lines.
246:/// * `on_data` - Callback invoked for each complete data payload; returns true to continue, false to stop
247:fn stream_sse_lines<R: BufRead>(
368:/// Callback type for streaming responses.
380:pub trait AiProvider: Send + Sync {
390:    /// Send a message and get a response (non-streaming).
402:    /// Send a message with streaming response.
414:    fn stream_message(
470:        stream: bool,
514:            "stream": stream,
528:        // Check for stream end
536:        // OpenAI streaming format:
568:            "Sending non-streaming request to OpenAI"
605:            "Received non-streaming response from OpenAI"
611:    fn stream_message(
623:            "Starting streaming request to OpenAI"
626:        let response = send_json_with_retry("OpenAI", "stream_message", || {
634:                .header("Accept", "text/event-stream")
642:        // Read the SSE stream using the helper
645:        stream_sse_lines(reader, |data| {
 succeeded in 50ms:
//! AI provider abstraction layer.
//!
//! This module provides a trait-based abstraction for AI providers, allowing
//! Script Kit to work with multiple AI services (OpenAI, Anthropic, etc.) through
//! a unified interface.
//!
//! # Architecture
//!
//! - `AiProvider` trait defines the interface all providers must implement
//! - `ProviderRegistry` manages available providers based on detected API keys
//! - Individual provider implementations (OpenAI, Anthropic, etc.) implement the trait
//!

use anyhow::{anyhow, Context, Result};
use std::collections::HashMap;
use std::io::{BufRead, BufReader};
use std::sync::Arc;
use std::time::Duration;

use super::config::{default_models, DetectedKeys, ModelInfo, ProviderConfig};

thinking
**Locating registry methods**
exec
/bin/zsh -lc "sed -n '1980,2150p' src/ai/providers.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 50ms:
}

/// Registry of available AI providers.
///
/// The registry automatically discovers available providers based on
/// environment variables and provides a unified interface to access them.
#[derive(Clone)]
pub struct ProviderRegistry {
    providers: HashMap<String, Arc<dyn AiProvider>>,
}

impl ProviderRegistry {
    /// Create an empty registry.
    pub fn new() -> Self {
        Self {
            providers: HashMap::new(),
        }
    }

    /// Create a registry populated from environment variables only.

thinking
**Identifying message struct and signature**
exec
/bin/zsh -lc "sed -n '340,470p' src/ai/providers.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 50ms:
            images,
        }
    }

    /// Create a new assistant message.
    pub fn assistant(content: impl Into<String>) -> Self {
        Self {
            role: "assistant".to_string(),
            content: content.into(),
            images: Vec::new(),
        }
    }

    /// Create a new system message.
    pub fn system(content: impl Into<String>) -> Self {
        Self {
            role: "system".to_string(),
            content: content.into(),
            images: Vec::new(),
        }

thinking
**Investigating default model selection**
exec
/bin/zsh -lc "sed -n '220,420p' src/prompts/chat/prompt.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:

    /// Set models from string names (creates ChatModel entries with name=id)
    pub fn with_model_names(mut self, model_names: Vec<String>) -> Self {
        if !model_names.is_empty() {
            self.models = model_names
                .into_iter()
                .map(|name| ChatModel::new(name.clone(), name.clone(), "Custom"))
                .collect();
            if self.model.is_none() {
                self.model = self.models.first().map(|m| m.name.clone());
            }
        }
        self
    }

    /// Set the default model
    pub fn with_default_model(mut self, model: String) -> Self {
        self.model = Some(model);
        self
    }

thinking
**Identifying message submission location**
exec
/bin/zsh -lc "rg -n \"send_message\\(|provider_registry|selected_model|builtin AI|with_builtin_ai|on_chunk|stream_message\" src/prompts/chat" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 52ms:
src/prompts/chat/streaming.rs:72:        let (model_id, provider) = match &self.selected_model {
src/prompts/chat/streaming.rs:87:        let registry = match &self.provider_registry {
src/prompts/chat/streaming.rs:197:        let (model_id, provider) = match &self.selected_model {
src/prompts/chat/streaming.rs:212:        let registry = match &self.provider_registry {
src/prompts/chat/streaming.rs:307:            let result = ai_provider.stream_message(
src/prompts/chat/streaming.rs:348:                // if done is true, stream_message has returned, so all chunks have
src/prompts/chat/prompt.rs:27:    pub(super) provider_registry: Option<ProviderRegistry>,
src/prompts/chat/prompt.rs:29:    pub(super) selected_model: Option<ModelInfo>,
src/prompts/chat/prompt.rs:109:            provider_registry: None,
src/prompts/chat/prompt.rs:111:            selected_model: None,
src/prompts/chat/prompt.rs:274:    pub fn with_builtin_ai(mut self, registry: ProviderRegistry, prefer_vercel: bool) -> Self {
src/prompts/chat/prompt.rs:278:        let selected_model = if prefer_vercel {
src/prompts/chat/prompt.rs:293:        self.model = selected_model.as_ref().map(|m| m.display_name.clone());
src/prompts/chat/prompt.rs:300:                selected_model.as_ref().map(|m| &m.display_name)
src/prompts/chat/prompt.rs:304:        self.provider_registry = Some(registry);
src/prompts/chat/prompt.rs:306:        self.selected_model = selected_model;
src/prompts/chat/prompt.rs:348:    pub fn set_provider_registry(
src/prompts/chat/prompt.rs:356:        let selected_model = if prefer_vercel {
src/prompts/chat/prompt.rs:370:        self.model = selected_model.as_ref().map(|m| m.display_name.clone());
src/prompts/chat/prompt.rs:375:                "set_provider_registry: {} models, selected={:?}",

thinking
**Inspecting streaming logic for reuse**
exec
/bin/zsh -lc "sed -n '1,260p' src/prompts/chat/streaming.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 50ms:
use super::*;

impl ChatPrompt {
    pub(super) fn handle_submit(&mut self, cx: &mut Context<Self>) {
        let text = self.input.text().to_string();
        let pending_image = self.pending_image.take();
        let pending_render = self.pending_image_render.take();

        if text.trim().is_empty() && pending_image.is_none() {
            return;
        }
        logging::log("CHAT", &format!("User submitted: {}", text));
        self.input.clear();

        // If built-in AI mode is enabled, handle the AI call directly
        if self.has_builtin_ai() {
            // Cache the render image for conversation history display
            // We need the user message ID, which will be generated in handle_builtin_ai_submit
            self.handle_builtin_ai_submit(text, pending_image, pending_render, cx);
        } else {

thinking
**Evaluating script creation approach**
exec
/bin/zsh -lc "rg -n \"mod script_creation|pub fn create_new_script|open_in_editor|write|template\" src | sed -n '1,220p'" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 50ms:
src/scriptlet_metadata/tests/part_000.rs:402:        let languages = vec!["bash", "python", "ruby", "js", "kit", "template"];
src/app_actions/handle_action.rs:150:        // Take ownership of stdin, write, then drop to signal EOF
src/app_actions/handle_action.rs:152:            stdin.write_all(text.as_bytes())?;
src/app_actions/handle_action.rs:1434:                    if let Err(e) = std::fs::write(&temp_path, png_bytes) {
src/app_actions/handle_action.rs:1435:                        logging::log("ERROR", &format!("Failed to write temp image: {}", e));
src/app_actions/handle_action.rs:1961:                match std::fs::write(&save_path, &file_content) {
src/app_actions/handle_action.rs:2049:                            f.write_all(snippet_entry.as_bytes())
src/app_actions/handle_action.rs:2054:                    std::fs::write(&snippets_file, format!("{}{}", header, snippet_entry))
src/app_actions/handle_action/main_actions_03.rs:187:                    if let Err(e) = std::fs::write(&temp_path, png_bytes) {
src/app_actions/handle_action/main_actions_03.rs:188:                        logging::log("ERROR", &format!("Failed to write temp image: {}", e));
src/app_actions/handle_action/main_actions_05.rs:38:                match std::fs::write(&save_path, &file_content) {
src/app_actions/handle_action/main_actions_05.rs:126:                            f.write_all(snippet_entry.as_bytes())
src/app_actions/handle_action/main_actions_05.rs:131:                    std::fs::write(&snippets_file, format!("{}{}", header, snippet_entry))
src/favorites/mod.rs:42:    fs::write(path, json)
src/favorites/mod.rs:43:        .with_context(|| format!("failed to write favorites file at {}", path.display()))?;
src/template_variables/part_000.rs:77:/// Substitute template variables in content using default context
src/template_variables/part_000.rs:85:/// * `content` - The template string with variable placeholders
src/template_variables/part_000.rs:94:/// Substitute template variables with a custom context
src/template_variables/part_000.rs:100:/// * `content` - The template string with variable placeholders
src/template_variables/part_000.rs:128:    // Handle special JavaScript-style patterns that may appear in Script Kit templates

thinking
**Searching for script_creation files**
exec
/bin/zsh -lc 'rg --files src | rg "script_creation|script creation|create_new_script|open_in_editor"' in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
src/script_creation/part_000.rs
src/script_creation/part_001.rs
src/script_creation/mod.rs

thinking
**Inspecting script creation functions**
exec
/bin/zsh -lc "sed -n '1,260p' src/script_creation/mod.rs" in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc "sed -n '1,320p' src/script_creation/part_000.rs" in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc "sed -n '1,260p' src/script_creation/part_001.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
//! Script and Extension Creation Module
//!
//! This module provides functions to create new scripts and extensions
//! in the Script Kit environment, as well as opening files in the configured editor.
//!
//! # Usage
//!
//! ```rust,ignore
//! use script_kit_gpui::script_creation::{create_new_script, create_new_extension, open_in_editor};
//! use script_kit_gpui::config::Config;
//!
//! // Create a new script
//! let script_path = create_new_script("my-script")?;
//!
//! // Create a new extension
//! let extension_path = create_new_extension("my-extension")?;
//!
//! // Open in editor
//! let config = Config::default();
//! open_in_editor(&script_path, &config)?;
 succeeded in 51ms:
use crate::config::Config;
use anyhow::{Context, Result};
use std::fs::{self, OpenOptions};
use std::io::{ErrorKind, Write};
use std::path::{Path, PathBuf};
use std::process::Command;
use tracing::{info, instrument, warn};
/// Scripts directory under ~/.scriptkit/kit/main/
const SCRIPTS_DIR: &str = "~/.scriptkit/kit/main/scripts";
/// Extensions directory under ~/.scriptkit/kit/main/
const EXTENSIONS_DIR: &str = "~/.scriptkit/kit/main/extensions";
/// Maximum filename size on most filesystems (bytes, not chars).
const MAX_FILENAME_BYTES: usize = 255;
/// Reserved filenames on Windows that are invalid even with an extension.
const WINDOWS_RESERVED_FILENAMES: [&str; 22] = [
    "con", "prn", "aux", "nul", "com1", "com2", "com3", "com4", "com5", "com6", "com7", "com8",
    "com9", "lpt1", "lpt2", "lpt3", "lpt4", "lpt5", "lpt6", "lpt7", "lpt8", "lpt9",
];
fn scripts_dir() -> PathBuf {
    PathBuf::from(shellexpand::tilde(SCRIPTS_DIR).as_ref())
 succeeded in 51ms:
#[cfg(test)]
mod tests {
    use super::*;
    use std::env;
    use tempfile::tempdir;

    #[test]
    fn test_sanitize_name_basic() {
        assert_eq!(sanitize_name("hello"), "hello");
        assert_eq!(sanitize_name("Hello World"), "hello-world");
        assert_eq!(sanitize_name("my_script_name"), "my-script-name");
    }

    #[test]
    fn test_sanitize_name_special_chars() {
        assert_eq!(sanitize_name("hello@world!"), "helloworld");
        assert_eq!(sanitize_name("test#$%script"), "testscript");
        assert_eq!(sanitize_name("foo & bar"), "foo-bar");
    }


thinking
**Planning editor integration**
exec
/bin/zsh -lc "rg -n \"pub fn open_in_editor|parse_editor_command|write_script|create_new_script_in_dir|generate_script_template\" src/script_creation/part_000.rs src/script_creation/part_001.rs && sed -n '260,520p' src/script_creation/part_000.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
src/script_creation/part_001.rs:57:    fn test_generate_script_template() {
src/script_creation/part_001.rs:58:        let template = generate_script_template("my-script");
src/script_creation/part_001.rs:121:        let script_path = create_new_script_in_dir("test-script", &scripts_dir).unwrap();
src/script_creation/part_001.rs:151:    fn test_create_new_script_in_dir_generates_unique_name_when_base_exists() {
src/script_creation/part_001.rs:155:        let first = create_new_script_in_dir("untitled", &scripts_dir).unwrap();
src/script_creation/part_001.rs:156:        let second = create_new_script_in_dir("untitled", &scripts_dir).unwrap();
src/script_creation/part_001.rs:185:    fn test_create_new_script_in_dir_rejects_windows_reserved_name_after_sanitization() {
src/script_creation/part_001.rs:189:        let err = create_new_script_in_dir("CON!!!", &scripts_dir).unwrap_err();
src/script_creation/part_001.rs:201:    fn test_parse_editor_command_splits_flags_and_quotes() {
src/script_creation/part_001.rs:202:        let parts = parse_editor_command(r#"code --reuse-window --goto "src/main.rs:10""#).unwrap();
src/script_creation/part_000.rs:172:fn generate_script_template(name: &str) -> String {
src/script_creation/part_000.rs:253:    create_new_script_in_dir(name, &scripts_dir())
src/script_creation/part_000.rs:255:fn create_new_script_in_dir(name: &str, scripts_dir: &Path) -> Result<PathBuf> {
src/script_creation/part_000.rs:273:        generate_script_template,
src/script_creation/part_000.rs:348:fn parse_editor_command(editor: &str) -> Result<Vec<String>> {
src/script_creation/part_000.rs:403:pub fn open_in_editor(path: &Path, config: &Config) -> Result<()> {
src/script_creation/part_000.rs:405:    let command_parts = parse_editor_command(&editor)?;
    fs::create_dir_all(scripts_dir).with_context(|| {
        format!(
            "Failed to create scripts directory: {}",

thinking
**Planning script creation implementation**
exec
/bin/zsh -lc "rg -n \"kit/main/scripts|scripts_dir|untitled\\.ts|create_new_script|script_creation::|open_in_editor\\(\" src/app_impl src/execute_script src/ai src/actions/dialog" in /Users/johnlindquist/dev/script-kit-gpui exited 1 in 50ms:


thinking
**Planning AI script generation integration**
exec
/bin/zsh -lc "rg -n \"struct ScriptListApp|toast_manager|show_hud\\(|close_and_reset_window\\(|filter_text|cached_provider_registry|inline_chat\" src/app.rs src/app_*.rs src/app_impl/*.rs | sed -n '1,260p'" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
rg: src/app.rs: No such file or directory (os error 2)
src/app_impl/shortcuts_hud_grid.rs:23:            self.close_and_reset_window(cx);
src/app_impl/shortcuts_hud_grid.rs:32:            self.show_hud(result, None, cx);
src/app_impl/shortcuts_hud_grid.rs:45:            self.show_hud(status.to_string(), None, cx);
src/app_impl/shortcuts_hud_grid.rs:56:            self.close_and_reset_window(cx);
src/app_impl/shortcuts_hud_grid.rs:95:    pub(crate) fn show_hud(&mut self, text: String, duration_ms: Option<u64>, cx: &mut Context<Self>) {
src/app_impl/shortcuts_hud_grid.rs:98:        hud_manager::show_hud(text, duration_ms, cx);
src/app_impl/shortcut_recorder.rs:410:                        self.show_hud(
src/app_impl/shortcut_recorder.rs:422:                        self.show_hud(
src/app_impl/shortcut_recorder.rs:432:                self.show_hud(format!("Failed to save shortcut: {}", e), Some(4000), cx);
src/app_impl/filter_input_change.rs:21:            self.filter_text = new_text.clone();
src/app_impl/filter_input_change.rs:407:        if new_text == self.filter_text {
src/app_impl/filter_input_change.rs:411:        let previous_text = std::mem::replace(&mut self.filter_text, new_text.clone());
src/app_impl/filter_input_change.rs:417:        // AFTER computed_filter_text is updated. This prevents a race condition where:
src/app_impl/startup_new_prelude.rs:157:                                app.computed_filter_text
src/app_impl/startup_new_arrow.rs:218:                                        if this.filter_text.is_empty() && this.selected_index == 0 {
src/app_impl/startup_new_arrow.rs:224:                                                this.filter_text = text.clone();
src/app_impl/startup_new_arrow.rs:255:                                                this.filter_text = text.clone();
src/app_impl/startup_new_arrow.rs:277:                                                this.filter_text.clear();
src/app_impl/startup.rs:173:                                app.computed_filter_text

thinking
**Locating struct definition**
exec
/bin/zsh -lc "rg -n \"pub struct ScriptListApp|struct ScriptListApp\" src | sed -n '1,80p'" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 52ms:
src/main_sections/app_state.rs:1:struct ScriptListApp {

thinking
**Reviewing application state structure**
exec
/bin/zsh -lc "sed -n '1,260p' src/main_sections/app_state.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
struct ScriptListApp {
    /// H1 Optimization: Arc-wrapped scripts for cheap cloning during filter operations
    scripts: Vec<std::sync::Arc<scripts::Script>>,
    /// H1 Optimization: Arc-wrapped scriptlets for cheap cloning during filter operations
    scriptlets: Vec<std::sync::Arc<scripts::Scriptlet>>,
    builtin_entries: Vec<builtins::BuiltInEntry>,
    /// Cached list of installed applications for main search and AppLauncherView
    apps: Vec<app_launcher::AppInfo>,
    /// P0 FIX: Cached clipboard entries for ClipboardHistoryView (avoids cloning per frame)
    cached_clipboard_entries: Vec<clipboard_history::ClipboardEntryMeta>,
    /// Focused clipboard entry ID for action handling in ClipboardHistoryView
    #[allow(dead_code)]
    focused_clipboard_entry_id: Option<String>,
    /// P0 FIX: Cached windows for WindowSwitcherView (avoids cloning per frame)
    cached_windows: Vec<window_control::WindowInfo>,
    /// Cached file results for FileSearchView (avoids cloning per frame)
    cached_file_results: Vec<file_search::FileResult>,
    selected_index: usize,
    /// Main menu filter text (mirrors gpui-component input state)
    filter_text: String,

thinking
**Evaluating fallback and toast handling**
exec
/bin/zsh -lc "sed -n '300,430p' src/app_impl/execution_paths.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
                                gpui::Bounds {
                                    origin: gpui::Point {
                                        x: gpui::px(100.0),
                                        y: gpui::px(100.0),
                                    },
                                    size: gpui::Size {
                                        width: gpui::px(600.0),
                                        height: gpui::px(400.0),
                                    },
                                }
                            };

                        let sender = confirm_tx.clone();
                        let on_choice: ConfirmCallback = std::sync::Arc::new(move |confirmed| {
                            let _ = sender.try_send(confirmed);
                        });

                        open_confirm_window(
                            cx,
                            main_bounds,

thinking
**Assessing keyboard handling for Tab AI**
exec
/bin/zsh -lc "rg -n \"Ask AI|Tab|Shift\\+Tab|show_inline_ai_chat|startup_new_tab|tab interceptor|ScriptList\" src/app_impl/tests src/*tests* src/app_actions/tests | sed -n '1,260p'" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 52ms:
src/actions_button_visibility_tests.rs:14:            "Expected ScriptListApp::has_actions() helper to exist"
src/actions_button_visibility_tests.rs:64:            .expect("Ask AI hint container not found in src/render_script_list/part_000.rs");
src/actions_button_visibility_tests.rs:69:            "Ask AI hint should be non-clickable (cursor_default). Section:\n{}",
src/actions_button_visibility_tests.rs:74:            "Ask AI hint should not imply clickability with cursor_pointer. Section:\n{}",
src/clipboard_actions_menu_coverage_tests.rs:4://! in `ScriptListApp::handle_action`.
src/list_state_init_tests.rs:4://! during ScriptListApp::new() so that scripts are visible on first render.
src/list_state_init_tests.rs:13://! of ScriptListApp::new() to ensure the list state is properly initialized.
src/list_state_init_tests.rs:23:    /// Read startup fragments used by ScriptListApp::new() in execution order.
src/list_state_init_tests.rs:28:            "src/app_impl/startup_new_tab.rs",
src/list_state_init_tests.rs:54:            "ScriptListApp::new() must call sync_list_state() to initialize the list state. \
src/list_state_init_tests.rs:71:            "ScriptListApp::new() must call validate_selection_bounds() to ensure \
src/keyboard_routing_tests.rs:4://! - Current view (ScriptList, FileSearchView, etc.)
src/keyboard_routing_tests.rs:9://! A bug was discovered where arrow keys in ScriptList view were navigating
src/keyboard_routing_tests.rs:12://! arrow keys in the ScriptList case.
src/keyboard_routing_tests.rs:56:    /// Verify that ScriptList arrow key handling checks for actions popup.
src/keyboard_routing_tests.rs:66:        // Find the ScriptList case within the arrow interceptor
src/keyboard_routing_tests.rs:68:            .find("AppView::ScriptList =>")
src/keyboard_routing_tests.rs:69:            .expect("ScriptList case not found in arrow interceptor");
src/keyboard_routing_tests.rs:71:        // Get the section after ScriptList match (next ~600 chars should have the check)
src/keyboard_routing_tests.rs:79:            "ScriptList arrow key handler must check show_actions_popup before navigation. \

thinking
**Designing AI script generation flow**
exec
/bin/zsh -lc "rg -n \"mod keyboard_routing_tests|list_state_init_tests|actions_button_visibility_tests\" src | sed -n '1,200p'" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
src/lib.rs:262:mod actions_button_visibility_tests;
src/main.rs:38:mod actions_button_visibility_tests;
src/main.rs:60:mod keyboard_routing_tests;
src/main.rs:65:mod list_state_init_tests;

thinking
**Assessing test locations and edit scope**
exec
/bin/zsh -lc "sed -n '1,240p' src/app_impl/tests.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
#[cfg(test)]
mod app_impl_state_sync_tests {
    use super::{calculate_fallback_error_message, ScriptListApp};

    #[test]
    fn test_sync_builtin_query_state_updates_query_and_selection_when_changed() {
        let mut query = String::from("old");
        let mut selected_index = 3;

        let changed =
            ScriptListApp::sync_builtin_query_state(&mut query, &mut selected_index, "new");

        assert!(changed);
        assert_eq!(query, "new");
        assert_eq!(selected_index, 0);
    }

    #[test]
    fn test_sync_builtin_query_state_noop_when_query_is_unchanged() {
        let mut query = String::from("same");

thinking
**Planning test additions and edits**
codex
I’ve mapped the implementation path: add a `Shift+Tab` branch in the ScriptList tab interceptor, then add a new `app_impl` action that calls the AI provider, writes a new script file, and opens it in the editor. I’m claiming `startup.rs`, `startup_new_tab.rs`, and `prompt_ai.rs` now before making edits.
exec
/bin/zsh -lc '~/.claude/bin/swarm.ts claim --wait --id codex-shift-tab-keybind src/app_impl/startup.rs' in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc '~/.claude/bin/swarm.ts claim --wait --id codex-shift-tab-keybind src/app_impl/prompt_ai.rs' in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
Claimed for codex-shift-tab-keybind: src/app_impl/startup.rs
 succeeded in 51ms:
Claimed for codex-shift-tab-keybind: src/app_impl/prompt_ai.rs
exec
/bin/zsh -lc '~/.claude/bin/swarm.ts claim --wait --id codex-shift-tab-keybind src/app_impl/startup_new_tab.rs' in /Users/johnlindquist/dev/script-kit-gpui succeeded in 89ms:
Claimed for codex-shift-tab-keybind: src/app_impl/startup_new_tab.rs

thinking
**Updating swarm status notes**
exec
/bin/zsh -lc '~/.claude/bin/swarm.ts update --id codex-shift-tab-keybind --status in_progress --notes "Implementing Shift+Tab AI script generation in startup tab interceptor + prompt_ai backend action."' in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
Updated: codex-shift-tab-keybind

thinking
**Designing prompt helper additions**
exec
/bin/zsh -lc "sed -n '1,260p' src/ai/mod.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
//! AI Chat Module
//!
//! This module provides the data layer for the AI chat window feature.
//! It includes data models, SQLite storage with FTS5 search support,
//! and provider abstraction for BYOK (Bring Your Own Key) AI integration.
//!
//! # Architecture
//!
//! ```text
//! src/ai/
//! ├── mod.rs       - Module exports and documentation
//! ├── model.rs     - Data models (Chat, Message, ChatId, MessageRole)
//! ├── storage.rs   - SQLite persistence layer
//! ├── config.rs    - Environment variable detection and model configuration
//! └── providers.rs - Provider trait and implementations (OpenAI, Anthropic, etc.)
//! ```
//!
//! # Database Location
//!
//! The AI chats database is stored at `~/.scriptkit/ai-chats.db`.

thinking
**Designing AI script generation helpers**
codex
I’m now editing `prompt_ai.rs` to add the new AI script-generation action, including provider/model selection, prompt construction, response parsing, file save/open, and unit tests for the new helper logic.
file update
M /Users/johnlindquist/dev/script-kit-gpui/src/app_impl/prompt_ai.rs
@@ -1,2 +1,189 @@
 use super::*;
+use anyhow::{anyhow, Context as AnyhowContext, Result};
+
+#[derive(Debug, Clone, Copy)]
+enum AiScriptGenerationStage {
+    SelectModel,
+    ResolveProvider,
+    RequestCompletion,
+    ExtractScript,
+    CreateScriptFile,
+    WriteScriptFile,
+    OpenEditor,
+}
+
+impl AiScriptGenerationStage {
+    fn as_str(self) -> &'static str {
+        match self {
+            Self::SelectModel => "select_model",
+            Self::ResolveProvider => "resolve_provider",
+            Self::RequestCompletion => "request_completion",
+            Self::ExtractScript => "extract_script",
+            Self::CreateScriptFile => "create_script_file",
+            Self::WriteScriptFile => "write_script_file",
+            Self::OpenEditor => "open_editor",
+        }
+    }
+}
+
+const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You are ScriptKitScriptGenerator.
+Generate a complete Script Kit TypeScript script and return code only.
+Requirements:
+- Include metadata comments at the top:
+  // Name: <script name>
+  // Description: <short summary>
+- Include: import "@scriptkit/sdk";
+- Use idiomatic Script Kit APIs (for example await arg(), await div(), await editor()) when useful.
+- Prefer clear async/await flow and practical defaults.
+- Return a full runnable script with no extra explanation."#;
+
+fn build_ai_script_generation_user_prompt(description: &str) -> String {
+    format!(
+        "Generate a complete Script Kit script for this request:\n\n{}\n\nReturn only the TypeScript script source.",
+        description.trim()
+    )
+}
+
+fn select_default_ai_script_model(registry: &crate::ai::ProviderRegistry) -> Option<crate::ai::ModelInfo> {
+    registry
+        .get_all_models()
+        .into_iter()
+        .find(|model| model.provider.eq_ignore_ascii_case("vercel"))
+        .or_else(|| registry.get_all_models().into_iter().next())
+}
+
+fn extract_generated_script_source(raw_response: &str) -> Option<String> {
+    let trimmed = raw_response.trim();
+    if trimmed.is_empty() {
+        return None;
+    }
+
+    // Prefer typed code fences first, then generic fences.
+    for fence in ["```typescript", "```ts", "```javascript", "```js", "```"] {
+        if let Some(content) = extract_first_fenced_block(trimmed, fence) {
+            return normalize_generated_script(content);
+        }
+    }
+
+    normalize_generated_script(trimmed.to_string())
+}
+
+fn extract_first_fenced_block(response: &str, fence_start: &str) -> Option<String> {
+    let start_index = response.find(fence_start)?;
+    let after_fence = &response[start_index + fence_start.len()..];
+    let after_newline = after_fence
+        .strip_prefix("\r\n")
+        .or_else(|| after_fence.strip_prefix('\n'))
+        .unwrap_or(after_fence);
+    let end_index = after_newline.find("```")?;
+    Some(after_newline[..end_index].trim().to_string())
+}
+
+fn normalize_generated_script(content: String) -> Option<String> {
+    let trimmed = content.trim();
+    if trimmed.is_empty() {
+        None
+    } else {
+        Some(format!("{trimmed}\n"))
+    }
+}
+
+fn derive_script_name_from_description(description: &str) -> String {
+    let mut slug = String::new();
+    let mut last_was_dash = false;
+
+    for ch in description.chars() {
+        let lower = ch.to_ascii_lowercase();
+        if lower.is_ascii_alphanumeric() {
+            slug.push(lower);
+            last_was_dash = false;
+        } else if !last_was_dash && !slug.is_empty() {
+            slug.push('-');
+            last_was_dash = true;
+        }
+
+        if slug.len() >= 48 {
+            break;
+        }
+    }
+
+    while slug.ends_with('-') {
+        slug.pop();
+    }
+
+    if slug.is_empty() {
+        "ai-generated-script".to_string()
+    } else {
+        slug
+    }
+}
+
+fn generate_script_via_ai_backend(
+    registry: &crate::ai::ProviderRegistry,
+    model_id: &str,
+    prompt_description: &str,
+    config: &crate::config::Config,
+) -> Result<std::path::PathBuf> {
+    let provider = registry
+        .find_provider_for_model(model_id)
+        .cloned()
+        .ok_or_else(|| {
+            anyhow!(
+                "state={} attempted=find_provider_for_model model_id={} failure=provider_not_found",
+                AiScriptGenerationStage::ResolveProvider.as_str(),
+                model_id
+            )
+        })?;
+
+    let request_messages = vec![
+        crate::ai::ProviderMessage::system(AI_SCRIPT_GENERATION_SYSTEM_PROMPT),
+        crate::ai::ProviderMessage::user(build_ai_script_generation_user_prompt(prompt_description)),
+    ];
+
+    let ai_response = provider
+        .send_message(&request_messages, model_id)
+        .with_context(|| {
+            format!(
+                "state={} attempted=send_message model_id={} provider={} failure=provider_call_failed",
+                AiScriptGenerationStage::RequestCompletion.as_str(),
+                model_id,
+                provider.provider_id()
+            )
+        })?;
+
+    let generated_script = extract_generated_script_source(&ai_response).ok_or_else(|| {
+        anyhow!(
+            "state={} attempted=extract_generated_script model_id={} failure=empty_script_response",
+            AiScriptGenerationStage::ExtractScript.as_str(),
+            model_id
+        )
+    })?;
+
+    let script_name = derive_script_name_from_description(prompt_description);
+    let script_path = crate::script_creation::create_new_script(&script_name).with_context(|| {
+        format!(
+            "state={} attempted=create_new_script name={} failure=create_script_failed",
+            AiScriptGenerationStage::CreateScriptFile.as_str(),
+            script_name
+        )
+    })?;
+
+    std::fs::write(&script_path, generated_script).with_context(|| {
+        format!(
+            "state={} attempted=write_script path={} failure=write_failed",
+            AiScriptGenerationStage::WriteScriptFile.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    crate::script_creation::open_in_editor(&script_path, config).with_context(|| {
+        format!(
+            "state={} attempted=open_in_editor path={} failure=open_editor_failed",
+            AiScriptGenerationStage::OpenEditor.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
 
@@ -276,2 +463,142 @@
 
+    /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
+    /// The generated script is saved to disk and opened in the configured editor.
+    pub fn generate_script_from_ai_prompt(
+        &mut self,
+        prompt_description: String,
+        cx: &mut Context<Self>,
+    ) {
+        let prompt_description = prompt_description.trim().to_string();
+        if prompt_description.is_empty() {
+            return;
+        }
+
+        let registry = self
+            .cached_provider_registry
+            .clone()
+            .unwrap_or_else(|| crate::ai::ProviderRegistry::from_environment_with_config(Some(&self.config)));
+
+        if !registry.has_any_provider() {
+            self.toast_manager.push(
+                components::toast::Toast::error("No AI providers configured for script generation", &self.theme)
+                    .duration_ms(Some(5000)),
+            );
+            cx.notify();
+            return;
+        }
+
+        let selected_model = match select_default_ai_script_model(&registry) {
+            Some(model) => model,
+            None => {
+                let stage = AiScriptGenerationStage::SelectModel.as_str();
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    &format!(
+                        "state={} attempted=select_default_model failure=no_available_models",
+                        stage
+                    ),
+                );
+                self.toast_manager.push(
+                    components::toast::Toast::error(
+                        "No AI models available for script generation",
+                        &self.theme,
+                    )
+                    .duration_ms(Some(5000)),
+                );
+                cx.notify();
+                return;
+            }
+        };
+
+        let model_id = selected_model.id.clone();
+        let provider = selected_model.provider.clone();
+        let config = self.config.clone();
+        let (tx, rx) = async_channel::bounded::<Result<std::path::PathBuf, String>>(1);
+
+        logging::log(
+            "AI_SCRIPT_GEN",
+            &format!(
+                "state=queued attempted=shift_tab_script_generation model_id={} provider={} prompt_len={}",
+                model_id,
+                provider,
+                prompt_description.len()
+            ),
+        );
+        self.show_hud("Generating script with AI...".to_string(), Some(1500), cx);
+
+        std::thread::spawn(move || {
+            logging::log(
+                "AI_SCRIPT_GEN",
+                &format!(
+                    "state=running attempted=generate_script model_id={} provider={}",
+                    model_id, provider
+                ),
+            );
+
+            let generation_result =
+                generate_script_via_ai_backend(&registry, &model_id, &prompt_description, &config)
+                    .map_err(|error| error.to_string());
+
+            if tx.send_blocking(generation_result).is_err() {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=send_result failure=result_channel_closed",
+                );
+            }
+        });
+
+        cx.spawn(async move |this, cx| {
+            let Ok(result) = rx.recv().await else {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=recv_result failure=result_channel_closed",
+                );
+                return;
+            };
+
+            let _ = cx.update(|cx| {
+                this.update(cx, |app, cx| {
+                    match result {
+                        Ok(script_path) => {
+                            let script_name = script_path
+                                .file_name()
+                                .and_then(|name| name.to_str())
+                                .unwrap_or("generated script");
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!(
+                                    "state=completed attempted=generate_script path={}",
+                                    script_path.display()
+                                ),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::success(
+                                    format!("Generated and opened {}", script_name),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(3500)),
+                            );
+                            app.close_and_reset_window(cx);
+                        }
+                        Err(error) => {
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!("state=failed attempted=generate_script error={}", error),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::error(
+                                    format!("Failed to generate script: {}", error),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(7000)),
+                            );
+                            cx.notify();
+                        }
+                    }
+                })
+            });
+        })
+        .detach();
+    }
+
     /// Rebuild the cached provider registry in a background thread.
@@ -314 +641,40 @@
 }
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_build_ai_script_generation_user_prompt_includes_description() {
+        let prompt = build_ai_script_generation_user_prompt("create a weather script");
+        assert!(prompt.contains("create a weather script"));
+        assert!(prompt.contains("Script Kit script"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_prefers_typescript_fence() {
+        let response = r#"
+Here's your script:
+```typescript
+// Name: Weather
+import "@scriptkit/sdk";
+```
+"#;
+        let extracted = extract_generated_script_source(response).expect("script should be extracted");
+        assert!(extracted.contains("// Name: Weather"));
+        assert!(!extracted.contains("```"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_falls_back_to_plain_text() {
+        let response = "// Name: Plain\nimport \"@scriptkit/sdk\";";
+        let extracted = extract_generated_script_source(response).expect("script should be extracted");
+        assert_eq!(extracted, "// Name: Plain\nimport \"@scriptkit/sdk\";\n");
+    }
+
+    #[test]
+    fn test_derive_script_name_from_description_uses_fallback_for_symbols() {
+        let name = derive_script_name_from_description("@@@ !!!");
+        assert_eq!(name, "ai-generated-script");
+    }
+}
apply_patch(auto_approved=true) exited 0 in 11ms:
Success. Updated the following files:
M src/app_impl/prompt_ai.rs
file update:
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index 98a0c9cd1db5a933992c0f99cb20a3c2ca436b41..c734de610c7778bf1a236ddfec8b6cc41b4dc804
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -1,4 +1,191 @@
 use super::*;
+use anyhow::{anyhow, Context as AnyhowContext, Result};
+
+#[derive(Debug, Clone, Copy)]
+enum AiScriptGenerationStage {
+    SelectModel,
+    ResolveProvider,
+    RequestCompletion,
+    ExtractScript,
+    CreateScriptFile,
+    WriteScriptFile,
+    OpenEditor,
+}
+
+impl AiScriptGenerationStage {
+    fn as_str(self) -> &'static str {
+        match self {
+            Self::SelectModel => "select_model",
+            Self::ResolveProvider => "resolve_provider",
+            Self::RequestCompletion => "request_completion",
+            Self::ExtractScript => "extract_script",
+            Self::CreateScriptFile => "create_script_file",
+            Self::WriteScriptFile => "write_script_file",
+            Self::OpenEditor => "open_editor",
+        }
+    }
+}
+
+const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You are ScriptKitScriptGenerator.
+Generate a complete Script Kit TypeScript script and return code only.
+Requirements:
+- Include metadata comments at the top:
+  // Name: <script name>
+  // Description: <short summary>
+- Include: import "@scriptkit/sdk";
+- Use idiomatic Script Kit APIs (for example await arg(), await div(), await editor()) when useful.
+- Prefer clear async/await flow and practical defaults.
+- Return a full runnable script with no extra explanation."#;
+
+fn build_ai_script_generation_user_prompt(description: &str) -> String {
+    format!(
+        "Generate a complete Script Kit script for this request:\n\n{}\n\nReturn only the TypeScript script source.",
+        description.trim()
+    )
+}
+
+fn select_default_ai_script_model(registry: &crate::ai::ProviderRegistry) -> Option<crate::ai::ModelInfo> {
+    registry
+        .get_all_models()
+        .into_iter()
+        .find(|model| model.provider.eq_ignore_ascii_case("vercel"))
+        .or_else(|| registry.get_all_models().into_iter().next())
+}
+
+fn extract_generated_script_source(raw_response: &str) -> Option<String> {
+    let trimmed = raw_response.trim();
+    if trimmed.is_empty() {
+        return None;
+    }
+
+    // Prefer typed code fences first, then generic fences.
+    for fence in ["```typescript", "```ts", "```javascript", "```js", "```"] {
+        if let Some(content) = extract_first_fenced_block(trimmed, fence) {
+            return normalize_generated_script(content);
+        }
+    }
+
+    normalize_generated_script(trimmed.to_string())
+}
+
+fn extract_first_fenced_block(response: &str, fence_start: &str) -> Option<String> {
+    let start_index = response.find(fence_start)?;
+    let after_fence = &response[start_index + fence_start.len()..];
+    let after_newline = after_fence
+        .strip_prefix("\r\n")
+        .or_else(|| after_fence.strip_prefix('\n'))
+        .unwrap_or(after_fence);
+    let end_index = after_newline.find("```")?;
+    Some(after_newline[..end_index].trim().to_string())
+}
+
+fn normalize_generated_script(content: String) -> Option<String> {
+    let trimmed = content.trim();
+    if trimmed.is_empty() {
+        None
+    } else {
+        Some(format!("{trimmed}\n"))
+    }
+}
+
+fn derive_script_name_from_description(description: &str) -> String {
+    let mut slug = String::new();
+    let mut last_was_dash = false;
+
+    for ch in description.chars() {
+        let lower = ch.to_ascii_lowercase();
+        if lower.is_ascii_alphanumeric() {
+            slug.push(lower);
+            last_was_dash = false;
+        } else if !last_was_dash && !slug.is_empty() {
+            slug.push('-');
+            last_was_dash = true;
+        }
+
+        if slug.len() >= 48 {
+            break;
+        }
+    }
+
+    while slug.ends_with('-') {
+        slug.pop();
+    }
+
+    if slug.is_empty() {
+        "ai-generated-script".to_string()
+    } else {
+        slug
+    }
+}
+
+fn generate_script_via_ai_backend(
+    registry: &crate::ai::ProviderRegistry,
+    model_id: &str,
+    prompt_description: &str,
+    config: &crate::config::Config,
+) -> Result<std::path::PathBuf> {
+    let provider = registry
+        .find_provider_for_model(model_id)
+        .cloned()
+        .ok_or_else(|| {
+            anyhow!(
+                "state={} attempted=find_provider_for_model model_id={} failure=provider_not_found",
+                AiScriptGenerationStage::ResolveProvider.as_str(),
+                model_id
+            )
+        })?;
+
+    let request_messages = vec![
+        crate::ai::ProviderMessage::system(AI_SCRIPT_GENERATION_SYSTEM_PROMPT),
+        crate::ai::ProviderMessage::user(build_ai_script_generation_user_prompt(prompt_description)),
+    ];
+
+    let ai_response = provider
+        .send_message(&request_messages, model_id)
+        .with_context(|| {
+            format!(
+                "state={} attempted=send_message model_id={} provider={} failure=provider_call_failed",
+                AiScriptGenerationStage::RequestCompletion.as_str(),
+                model_id,
+                provider.provider_id()
+            )
+        })?;
+
+    let generated_script = extract_generated_script_source(&ai_response).ok_or_else(|| {
+        anyhow!(
+            "state={} attempted=extract_generated_script model_id={} failure=empty_script_response",
+            AiScriptGenerationStage::ExtractScript.as_str(),
+            model_id
+        )
+    })?;
+
+    let script_name = derive_script_name_from_description(prompt_description);
+    let script_path = crate::script_creation::create_new_script(&script_name).with_context(|| {
+        format!(
+            "state={} attempted=create_new_script name={} failure=create_script_failed",
+            AiScriptGenerationStage::CreateScriptFile.as_str(),
+            script_name
+        )
+    })?;
+
+    std::fs::write(&script_path, generated_script).with_context(|| {
+        format!(
+            "state={} attempted=write_script path={} failure=write_failed",
+            AiScriptGenerationStage::WriteScriptFile.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    crate::script_creation::open_in_editor(&script_path, config).with_context(|| {
+        format!(
+            "state={} attempted=open_in_editor path={} failure=open_editor_failed",
+            AiScriptGenerationStage::OpenEditor.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
 
 impl ScriptListApp {
     pub(crate) fn is_in_prompt(&self) -> bool {
@@ -274,6 +461,146 @@
         cx.notify();
     }
 
+    /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
+    /// The generated script is saved to disk and opened in the configured editor.
+    pub fn generate_script_from_ai_prompt(
+        &mut self,
+        prompt_description: String,
+        cx: &mut Context<Self>,
+    ) {
+        let prompt_description = prompt_description.trim().to_string();
+        if prompt_description.is_empty() {
+            return;
+        }
+
+        let registry = self
+            .cached_provider_registry
+            .clone()
+            .unwrap_or_else(|| crate::ai::ProviderRegistry::from_environment_with_config(Some(&self.config)));
+
+        if !registry.has_any_provider() {
+            self.toast_manager.push(
+                components::toast::Toast::error("No AI providers configured for script generation", &self.theme)
+                    .duration_ms(Some(5000)),
+            );
+            cx.notify();
+            return;
+        }
+
+        let selected_model = match select_default_ai_script_model(&registry) {
+            Some(model) => model,
+            None => {
+                let stage = AiScriptGenerationStage::SelectModel.as_str();
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    &format!(
+                        "state={} attempted=select_default_model failure=no_available_models",
+                        stage
+                    ),
+                );
+                self.toast_manager.push(
+                    components::toast::Toast::error(
+                        "No AI models available for script generation",
+                        &self.theme,
+                    )
+                    .duration_ms(Some(5000)),
+                );
+                cx.notify();
+                return;
+            }
+        };
+
+        let model_id = selected_model.id.clone();
+        let provider = selected_model.provider.clone();
+        let config = self.config.clone();
+        let (tx, rx) = async_channel::bounded::<Result<std::path::PathBuf, String>>(1);
+
+        logging::log(
+            "AI_SCRIPT_GEN",
+            &format!(
+                "state=queued attempted=shift_tab_script_generation model_id={} provider={} prompt_len={}",
+                model_id,
+                provider,
+                prompt_description.len()
+            ),
+        );
+        self.show_hud("Generating script with AI...".to_string(), Some(1500), cx);
+
+        std::thread::spawn(move || {
+            logging::log(
+                "AI_SCRIPT_GEN",
+                &format!(
+                    "state=running attempted=generate_script model_id={} provider={}",
+                    model_id, provider
+                ),
+            );
+
+            let generation_result =
+                generate_script_via_ai_backend(&registry, &model_id, &prompt_description, &config)
+                    .map_err(|error| error.to_string());
+
+            if tx.send_blocking(generation_result).is_err() {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=send_result failure=result_channel_closed",
+                );
+            }
+        });
+
+        cx.spawn(async move |this, cx| {
+            let Ok(result) = rx.recv().await else {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=recv_result failure=result_channel_closed",
+                );
+                return;
+            };
+
+            let _ = cx.update(|cx| {
+                this.update(cx, |app, cx| {
+                    match result {
+                        Ok(script_path) => {
+                            let script_name = script_path
+                                .file_name()
+                                .and_then(|name| name.to_str())
+                                .unwrap_or("generated script");
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!(
+                                    "state=completed attempted=generate_script path={}",
+                                    script_path.display()
+                                ),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::success(
+                                    format!("Generated and opened {}", script_name),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(3500)),
+                            );
+                            app.close_and_reset_window(cx);
+                        }
+                        Err(error) => {
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!("state=failed attempted=generate_script error={}", error),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::error(
+                                    format!("Failed to generate script: {}", error),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(7000)),
+                            );
+                            cx.notify();
+                        }
+                    }
+                })
+            });
+        })
+        .detach();
+    }
+
     /// Rebuild the cached provider registry in a background thread.
     /// Called after config changes (API key saved, Claude Code enabled, etc.)
     pub fn rebuild_provider_registry_async(&mut self, cx: &mut Context<Self>) {
@@ -312,3 +639,42 @@
         .detach();
     }
 }
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_build_ai_script_generation_user_prompt_includes_description() {
+        let prompt = build_ai_script_generation_user_prompt("create a weather script");
+        assert!(prompt.contains("create a weather script"));
+        assert!(prompt.contains("Script Kit script"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_prefers_typescript_fence() {
+        let response = r#"
+Here's your script:
+```typescript
+// Name: Weather
+import "@scriptkit/sdk";
+```
+"#;
+        let extracted = extract_generated_script_source(response).expect("script should be extracted");
+        assert!(extracted.contains("// Name: Weather"));
+        assert!(!extracted.contains("```"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_falls_back_to_plain_text() {
+        let response = "// Name: Plain\nimport \"@scriptkit/sdk\";";
+        let extracted = extract_generated_script_source(response).expect("script should be extracted");
+        assert_eq!(extracted, "// Name: Plain\nimport \"@scriptkit/sdk\";\n");
+    }
+
+    #[test]
+    fn test_derive_script_name_from_description_uses_fallback_for_symbols() {
+        let name = derive_script_name_from_description("@@@ !!!");
+        assert_eq!(name, "ai-generated-script");
+    }
+}

file update:
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index 98a0c9cd1db5a933992c0f99cb20a3c2ca436b41..c734de610c7778bf1a236ddfec8b6cc41b4dc804
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -1,4 +1,191 @@
 use super::*;
+use anyhow::{anyhow, Context as AnyhowContext, Result};
+
+#[derive(Debug, Clone, Copy)]
+enum AiScriptGenerationStage {
+    SelectModel,
+    ResolveProvider,
+    RequestCompletion,
+    ExtractScript,
+    CreateScriptFile,
+    WriteScriptFile,
+    OpenEditor,
+}
+
+impl AiScriptGenerationStage {
+    fn as_str(self) -> &'static str {
+        match self {
+            Self::SelectModel => "select_model",
+            Self::ResolveProvider => "resolve_provider",
+            Self::RequestCompletion => "request_completion",
+            Self::ExtractScript => "extract_script",
+            Self::CreateScriptFile => "create_script_file",
+            Self::WriteScriptFile => "write_script_file",
+            Self::OpenEditor => "open_editor",
+        }
+    }
+}
+
+const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You are ScriptKitScriptGenerator.
+Generate a complete Script Kit TypeScript script and return code only.
+Requirements:
+- Include metadata comments at the top:
+  // Name: <script name>
+  // Description: <short summary>
+- Include: import "@scriptkit/sdk";
+- Use idiomatic Script Kit APIs (for example await arg(), await div(), await editor()) when useful.
+- Prefer clear async/await flow and practical defaults.
+- Return a full runnable script with no extra explanation."#;
+
+fn build_ai_script_generation_user_prompt(description: &str) -> String {
+    format!(
+        "Generate a complete Script Kit script for this request:\n\n{}\n\nReturn only the TypeScript script source.",
+        description.trim()
+    )
+}
+
+fn select_default_ai_script_model(registry: &crate::ai::ProviderRegistry) -> Option<crate::ai::ModelInfo> {
+    registry
+        .get_all_models()
+        .into_iter()
+        .find(|model| model.provider.eq_ignore_ascii_case("vercel"))
+        .or_else(|| registry.get_all_models().into_iter().next())
+}
+
+fn extract_generated_script_source(raw_response: &str) -> Option<String> {
+    let trimmed = raw_response.trim();
+    if trimmed.is_empty() {
+        return None;
+    }
+
+    // Prefer typed code fences first, then generic fences.
+    for fence in ["```typescript", "```ts", "```javascript", "```js", "```"] {
+        if let Some(content) = extract_first_fenced_block(trimmed, fence) {
+            return normalize_generated_script(content);
+        }
+    }
+
+    normalize_generated_script(trimmed.to_string())
+}
+
+fn extract_first_fenced_block(response: &str, fence_start: &str) -> Option<String> {
+    let start_index = response.find(fence_start)?;
+    let after_fence = &response[start_index + fence_start.len()..];
+    let after_newline = after_fence
+        .strip_prefix("\r\n")
+        .or_else(|| after_fence.strip_prefix('\n'))
+        .unwrap_or(after_fence);
+    let end_index = after_newline.find("```")?;
+    Some(after_newline[..end_index].trim().to_string())
+}
+
+fn normalize_generated_script(content: String) -> Option<String> {
+    let trimmed = content.trim();
+    if trimmed.is_empty() {
+        None
+    } else {
+        Some(format!("{trimmed}\n"))
+    }
+}
+
+fn derive_script_name_from_description(description: &str) -> String {
+    let mut slug = String::new();
+    let mut last_was_dash = false;
+
+    for ch in description.chars() {
+        let lower = ch.to_ascii_lowercase();
+        if lower.is_ascii_alphanumeric() {
+            slug.push(lower);
+            last_was_dash = false;
+        } else if !last_was_dash && !slug.is_empty() {
+            slug.push('-');
+            last_was_dash = true;
+        }
+
+        if slug.len() >= 48 {
+            break;
+        }
+    }
+
+    while slug.ends_with('-') {
+        slug.pop();
+    }
+
+    if slug.is_empty() {
+        "ai-generated-script".to_string()
+    } else {
+        slug
+    }
+}
+
+fn generate_script_via_ai_backend(
+    registry: &crate::ai::ProviderRegistry,
+    model_id: &str,
+    prompt_description: &str,
+    config: &crate::config::Config,
+) -> Result<std::path::PathBuf> {
+    let provider = registry
+        .find_provider_for_model(model_id)
+        .cloned()
+        .ok_or_else(|| {
+            anyhow!(
+                "state={} attempted=find_provider_for_model model_id={} failure=provider_not_found",
+                AiScriptGenerationStage::ResolveProvider.as_str(),
+                model_id
+            )
+        })?;
+
+    let request_messages = vec![
+        crate::ai::ProviderMessage::system(AI_SCRIPT_GENERATION_SYSTEM_PROMPT),
+        crate::ai::ProviderMessage::user(build_ai_script_generation_user_prompt(prompt_description)),
+    ];
+
+    let ai_response = provider
+        .send_message(&request_messages, model_id)
+        .with_context(|| {
+            format!(
+                "state={} attempted=send_message model_id={} provider={} failure=provider_call_failed",
+                AiScriptGenerationStage::RequestCompletion.as_str(),
+                model_id,
+                provider.provider_id()
+            )
+        })?;
+
+    let generated_script = extract_generated_script_source(&ai_response).ok_or_else(|| {
+        anyhow!(
+            "state={} attempted=extract_generated_script model_id={} failure=empty_script_response",
+            AiScriptGenerationStage::ExtractScript.as_str(),
+            model_id
+        )
+    })?;
+
+    let script_name = derive_script_name_from_description(prompt_description);
+    let script_path = crate::script_creation::create_new_script(&script_name).with_context(|| {
+        format!(
+            "state={} attempted=create_new_script name={} failure=create_script_failed",
+            AiScriptGenerationStage::CreateScriptFile.as_str(),
+            script_name
+        )
+    })?;
+
+    std::fs::write(&script_path, generated_script).with_context(|| {
+        format!(
+            "state={} attempted=write_script path={} failure=write_failed",
+            AiScriptGenerationStage::WriteScriptFile.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    crate::script_creation::open_in_editor(&script_path, config).with_context(|| {
+        format!(
+            "state={} attempted=open_in_editor path={} failure=open_editor_failed",
+            AiScriptGenerationStage::OpenEditor.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
 
 impl ScriptListApp {
     pub(crate) fn is_in_prompt(&self) -> bool {
@@ -274,6 +461,146 @@
         cx.notify();
     }
 
+    /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
+    /// The generated script is saved to disk and opened in the configured editor.
+    pub fn generate_script_from_ai_prompt(
+        &mut self,
+        prompt_description: String,
+        cx: &mut Context<Self>,
+    ) {
+        let prompt_description = prompt_description.trim().to_string();
+        if prompt_description.is_empty() {
+            return;
+        }
+
+        let registry = self
+            .cached_provider_registry
+            .clone()
+            .unwrap_or_else(|| crate::ai::ProviderRegistry::from_environment_with_config(Some(&self.config)));
+
+        if !registry.has_any_provider() {
+            self.toast_manager.push(
+                components::toast::Toast::error("No AI providers configured for script generation", &self.theme)
+                    .duration_ms(Some(5000)),
+            );
+            cx.notify();
+            return;
+        }
+
+        let selected_model = match select_default_ai_script_model(&registry) {
+            Some(model) => model,
+            None => {
+                let stage = AiScriptGenerationStage::SelectModel.as_str();
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    &format!(
+                        "state={} attempted=select_default_model failure=no_available_models",
+                        stage
+                    ),
+                );
+                self.toast_manager.push(
+                    components::toast::Toast::error(
+                        "No AI models available for script generation",
+                        &self.theme,
+                    )
+                    .duration_ms(Some(5000)),
+                );
+                cx.notify();
+                return;
+            }
+        };
+
+        let model_id = selected_model.id.clone();
+        let provider = selected_model.provider.clone();
+        let config = self.config.clone();
+        let (tx, rx) = async_channel::bounded::<Result<std::path::PathBuf, String>>(1);
+
+        logging::log(
+            "AI_SCRIPT_GEN",
+            &format!(
+                "state=queued attempted=shift_tab_script_generation model_id={} provider={} prompt_len={}",
+                model_id,
+                provider,
+                prompt_description.len()
+            ),
+        );
+        self.show_hud("Generating script with AI...".to_string(), Some(1500), cx);
+
+        std::thread::spawn(move || {
+            logging::log(
+                "AI_SCRIPT_GEN",
+                &format!(
+                    "state=running attempted=generate_script model_id={} provider={}",
+                    model_id, provider
+                ),
+            );
+
+            let generation_result =
+                generate_script_via_ai_backend(&registry, &model_id, &prompt_description, &config)
+                    .map_err(|error| error.to_string());
+
+            if tx.send_blocking(generation_result).is_err() {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=send_result failure=result_channel_closed",
+                );
+            }
+        });
+
+        cx.spawn(async move |this, cx| {
+            let Ok(result) = rx.recv().await else {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=recv_result failure=result_channel_closed",
+                );
+                return;
+            };
+
+            let _ = cx.update(|cx| {
+                this.update(cx, |app, cx| {
+                    match result {
+                        Ok(script_path) => {
+                            let script_name = script_path
+                                .file_name()
+                                .and_then(|name| name.to_str())
+                                .unwrap_or("generated script");
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!(
+                                    "state=completed attempted=generate_script path={}",
+                                    script_path.display()
+                                ),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::success(
+                                    format!("Generated and opened {}", script_name),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(3500)),
+                            );
+                            app.close_and_reset_window(cx);
+                        }
+                        Err(error) => {
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!("state=failed attempted=generate_script error={}", error),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::error(
+                                    format!("Failed to generate script: {}", error),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(7000)),
+                            );
+                            cx.notify();
+                        }
+                    }
+                })
+            });
+        })
+        .detach();
+    }
+
     /// Rebuild the cached provider registry in a background thread.
     /// Called after config changes (API key saved, Claude Code enabled, etc.)
     pub fn rebuild_provider_registry_async(&mut self, cx: &mut Context<Self>) {
@@ -312,3 +639,42 @@
         .detach();
     }
 }
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_build_ai_script_generation_user_prompt_includes_description() {
+        let prompt = build_ai_script_generation_user_prompt("create a weather script");
+        assert!(prompt.contains("create a weather script"));
+        assert!(prompt.contains("Script Kit script"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_prefers_typescript_fence() {
+        let response = r#"
+Here's your script:
+```typescript
+// Name: Weather
+import "@scriptkit/sdk";
+```
+"#;
+        let extracted = extract_generated_script_source(response).expect("script should be extracted");
+        assert!(extracted.contains("// Name: Weather"));
+        assert!(!extracted.contains("```"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_falls_back_to_plain_text() {
+        let response = "// Name: Plain\nimport \"@scriptkit/sdk\";";
+        let extracted = extract_generated_script_source(response).expect("script should be extracted");
+        assert_eq!(extracted, "// Name: Plain\nimport \"@scriptkit/sdk\";\n");
+    }
+
+    #[test]
+    fn test_derive_script_name_from_description_uses_fallback_for_symbols() {
+        let name = derive_script_name_from_description("@@@ !!!");
+        assert_eq!(name, "ai-generated-script");
+    }
+}


thinking
**Fixing double model fetch and result type mismatch**
codex
I’ve added the new generation method and helper tests in `prompt_ai.rs`. Next I’m wiring `Shift+Tab` in the ScriptList tab interceptor (`startup.rs` and its mirrored `startup_new_tab.rs`) to call this method while keeping existing Tab chat behavior unchanged.
file update
M /Users/johnlindquist/dev/script-kit-gpui/src/app_impl/startup.rs
@@ -557,5 +557,7 @@
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
@@ -722,4 +724,4 @@
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
@@ -727,3 +729,2 @@
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
@@ -734,4 +735,8 @@
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
apply_patch(auto_approved=true) exited 0 in 16ms:
Success. Updated the following files:
M src/app_impl/startup.rs
file update:
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index 98a0c9cd1db5a933992c0f99cb20a3c2ca436b41..c734de610c7778bf1a236ddfec8b6cc41b4dc804
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -1,4 +1,191 @@
 use super::*;
+use anyhow::{anyhow, Context as AnyhowContext, Result};
+
+#[derive(Debug, Clone, Copy)]
+enum AiScriptGenerationStage {
+    SelectModel,
+    ResolveProvider,
+    RequestCompletion,
+    ExtractScript,
+    CreateScriptFile,
+    WriteScriptFile,
+    OpenEditor,
+}
+
+impl AiScriptGenerationStage {
+    fn as_str(self) -> &'static str {
+        match self {
+            Self::SelectModel => "select_model",
+            Self::ResolveProvider => "resolve_provider",
+            Self::RequestCompletion => "request_completion",
+            Self::ExtractScript => "extract_script",
+            Self::CreateScriptFile => "create_script_file",
+            Self::WriteScriptFile => "write_script_file",
+            Self::OpenEditor => "open_editor",
+        }
+    }
+}
+
+const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You are ScriptKitScriptGenerator.
+Generate a complete Script Kit TypeScript script and return code only.
+Requirements:
+- Include metadata comments at the top:
+  // Name: <script name>
+  // Description: <short summary>
+- Include: import "@scriptkit/sdk";
+- Use idiomatic Script Kit APIs (for example await arg(), await div(), await editor()) when useful.
+- Prefer clear async/await flow and practical defaults.
+- Return a full runnable script with no extra explanation."#;
+
+fn build_ai_script_generation_user_prompt(description: &str) -> String {
+    format!(
+        "Generate a complete Script Kit script for this request:\n\n{}\n\nReturn only the TypeScript script source.",
+        description.trim()
+    )
+}
+
+fn select_default_ai_script_model(registry: &crate::ai::ProviderRegistry) -> Option<crate::ai::ModelInfo> {
+    registry
+        .get_all_models()
+        .into_iter()
+        .find(|model| model.provider.eq_ignore_ascii_case("vercel"))
+        .or_else(|| registry.get_all_models().into_iter().next())
+}
+
+fn extract_generated_script_source(raw_response: &str) -> Option<String> {
+    let trimmed = raw_response.trim();
+    if trimmed.is_empty() {
+        return None;
+    }
+
+    // Prefer typed code fences first, then generic fences.
+    for fence in ["```typescript", "```ts", "```javascript", "```js", "```"] {
+        if let Some(content) = extract_first_fenced_block(trimmed, fence) {
+            return normalize_generated_script(content);
+        }
+    }
+
+    normalize_generated_script(trimmed.to_string())
+}
+
+fn extract_first_fenced_block(response: &str, fence_start: &str) -> Option<String> {
+    let start_index = response.find(fence_start)?;
+    let after_fence = &response[start_index + fence_start.len()..];
+    let after_newline = after_fence
+        .strip_prefix("\r\n")
+        .or_else(|| after_fence.strip_prefix('\n'))
+        .unwrap_or(after_fence);
+    let end_index = after_newline.find("```")?;
+    Some(after_newline[..end_index].trim().to_string())
+}
+
+fn normalize_generated_script(content: String) -> Option<String> {
+    let trimmed = content.trim();
+    if trimmed.is_empty() {
+        None
+    } else {
+        Some(format!("{trimmed}\n"))
+    }
+}
+
+fn derive_script_name_from_description(description: &str) -> String {
+    let mut slug = String::new();
+    let mut last_was_dash = false;
+
+    for ch in description.chars() {
+        let lower = ch.to_ascii_lowercase();
+        if lower.is_ascii_alphanumeric() {
+            slug.push(lower);
+            last_was_dash = false;
+        } else if !last_was_dash && !slug.is_empty() {
+            slug.push('-');
+            last_was_dash = true;
+        }
+
+        if slug.len() >= 48 {
+            break;
+        }
+    }
+
+    while slug.ends_with('-') {
+        slug.pop();
+    }
+
+    if slug.is_empty() {
+        "ai-generated-script".to_string()
+    } else {
+        slug
+    }
+}
+
+fn generate_script_via_ai_backend(
+    registry: &crate::ai::ProviderRegistry,
+    model_id: &str,
+    prompt_description: &str,
+    config: &crate::config::Config,
+) -> Result<std::path::PathBuf> {
+    let provider = registry
+        .find_provider_for_model(model_id)
+        .cloned()
+        .ok_or_else(|| {
+            anyhow!(
+                "state={} attempted=find_provider_for_model model_id={} failure=provider_not_found",
+                AiScriptGenerationStage::ResolveProvider.as_str(),
+                model_id
+            )
+        })?;
+
+    let request_messages = vec![
+        crate::ai::ProviderMessage::system(AI_SCRIPT_GENERATION_SYSTEM_PROMPT),
+        crate::ai::ProviderMessage::user(build_ai_script_generation_user_prompt(prompt_description)),
+    ];
+
+    let ai_response = provider
+        .send_message(&request_messages, model_id)
+        .with_context(|| {
+            format!(
+                "state={} attempted=send_message model_id={} provider={} failure=provider_call_failed",
+                AiScriptGenerationStage::RequestCompletion.as_str(),
+                model_id,
+                provider.provider_id()
+            )
+        })?;
+
+    let generated_script = extract_generated_script_source(&ai_response).ok_or_else(|| {
+        anyhow!(
+            "state={} attempted=extract_generated_script model_id={} failure=empty_script_response",
+            AiScriptGenerationStage::ExtractScript.as_str(),
+            model_id
+        )
+    })?;
+
+    let script_name = derive_script_name_from_description(prompt_description);
+    let script_path = crate::script_creation::create_new_script(&script_name).with_context(|| {
+        format!(
+            "state={} attempted=create_new_script name={} failure=create_script_failed",
+            AiScriptGenerationStage::CreateScriptFile.as_str(),
+            script_name
+        )
+    })?;
+
+    std::fs::write(&script_path, generated_script).with_context(|| {
+        format!(
+            "state={} attempted=write_script path={} failure=write_failed",
+            AiScriptGenerationStage::WriteScriptFile.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    crate::script_creation::open_in_editor(&script_path, config).with_context(|| {
+        format!(
+            "state={} attempted=open_in_editor path={} failure=open_editor_failed",
+            AiScriptGenerationStage::OpenEditor.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
 
 impl ScriptListApp {
     pub(crate) fn is_in_prompt(&self) -> bool {
@@ -274,6 +461,146 @@
         cx.notify();
     }
 
+    /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
+    /// The generated script is saved to disk and opened in the configured editor.
+    pub fn generate_script_from_ai_prompt(
+        &mut self,
+        prompt_description: String,
+        cx: &mut Context<Self>,
+    ) {
+        let prompt_description = prompt_description.trim().to_string();
+        if prompt_description.is_empty() {
+            return;
+        }
+
+        let registry = self
+            .cached_provider_registry
+            .clone()
+            .unwrap_or_else(|| crate::ai::ProviderRegistry::from_environment_with_config(Some(&self.config)));
+
+        if !registry.has_any_provider() {
+            self.toast_manager.push(
+                components::toast::Toast::error("No AI providers configured for script generation", &self.theme)
+                    .duration_ms(Some(5000)),
+            );
+            cx.notify();
+            return;
+        }
+
+        let selected_model = match select_default_ai_script_model(&registry) {
+            Some(model) => model,
+            None => {
+                let stage = AiScriptGenerationStage::SelectModel.as_str();
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    &format!(
+                        "state={} attempted=select_default_model failure=no_available_models",
+                        stage
+                    ),
+                );
+                self.toast_manager.push(
+                    components::toast::Toast::error(
+                        "No AI models available for script generation",
+                        &self.theme,
+                    )
+                    .duration_ms(Some(5000)),
+                );
+                cx.notify();
+                return;
+            }
+        };
+
+        let model_id = selected_model.id.clone();
+        let provider = selected_model.provider.clone();
+        let config = self.config.clone();
+        let (tx, rx) = async_channel::bounded::<Result<std::path::PathBuf, String>>(1);
+
+        logging::log(
+            "AI_SCRIPT_GEN",
+            &format!(
+                "state=queued attempted=shift_tab_script_generation model_id={} provider={} prompt_len={}",
+                model_id,
+                provider,
+                prompt_description.len()
+            ),
+        );
+        self.show_hud("Generating script with AI...".to_string(), Some(1500), cx);
+
+        std::thread::spawn(move || {
+            logging::log(
+                "AI_SCRIPT_GEN",
+                &format!(
+                    "state=running attempted=generate_script model_id={} provider={}",
+                    model_id, provider
+                ),
+            );
+
+            let generation_result =
+                generate_script_via_ai_backend(&registry, &model_id, &prompt_description, &config)
+                    .map_err(|error| error.to_string());
+
+            if tx.send_blocking(generation_result).is_err() {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=send_result failure=result_channel_closed",
+                );
+            }
+        });
+
+        cx.spawn(async move |this, cx| {
+            let Ok(result) = rx.recv().await else {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=recv_result failure=result_channel_closed",
+                );
+                return;
+            };
+
+            let _ = cx.update(|cx| {
+                this.update(cx, |app, cx| {
+                    match result {
+                        Ok(script_path) => {
+                            let script_name = script_path
+                                .file_name()
+                                .and_then(|name| name.to_str())
+                                .unwrap_or("generated script");
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!(
+                                    "state=completed attempted=generate_script path={}",
+                                    script_path.display()
+                                ),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::success(
+                                    format!("Generated and opened {}", script_name),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(3500)),
+                            );
+                            app.close_and_reset_window(cx);
+                        }
+                        Err(error) => {
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!("state=failed attempted=generate_script error={}", error),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::error(
+                                    format!("Failed to generate script: {}", error),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(7000)),
+                            );
+                            cx.notify();
+                        }
+                    }
+                })
+            });
+        })
+        .detach();
+    }
+
     /// Rebuild the cached provider registry in a background thread.
     /// Called after config changes (API key saved, Claude Code enabled, etc.)
     pub fn rebuild_provider_registry_async(&mut self, cx: &mut Context<Self>) {
@@ -312,3 +639,42 @@
         .detach();
     }
 }
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_build_ai_script_generation_user_prompt_includes_description() {
+        let prompt = build_ai_script_generation_user_prompt("create a weather script");
+        assert!(prompt.contains("create a weather script"));
+        assert!(prompt.contains("Script Kit script"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_prefers_typescript_fence() {
+        let response = r#"
+Here's your script:
+```typescript
+// Name: Weather
+import "@scriptkit/sdk";
+```
+"#;
+        let extracted = extract_generated_script_source(response).expect("script should be extracted");
+        assert!(extracted.contains("// Name: Weather"));
+        assert!(!extracted.contains("```"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_falls_back_to_plain_text() {
+        let response = "// Name: Plain\nimport \"@scriptkit/sdk\";";
+        let extracted = extract_generated_script_source(response).expect("script should be extracted");
+        assert_eq!(extracted, "// Name: Plain\nimport \"@scriptkit/sdk\";\n");
+    }
+
+    #[test]
+    fn test_derive_script_name_from_description_uses_fallback_for_symbols() {
+        let name = derive_script_name_from_description("@@@ !!!");
+        assert_eq!(name, "ai-generated-script");
+    }
+}
diff --git a/src/app_impl/startup.rs b/src/app_impl/startup.rs
index 2e95070751083b8f4c24cd7eae8372d6bec3f6d6..441b4f23bca1b6294025c698eeb11d45815e85e0
--- a/src/app_impl/startup.rs
+++ b/src/app_impl/startup.rs
@@ -555,9 +555,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -720,20 +722,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();

file update:
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index 98a0c9cd1db5a933992c0f99cb20a3c2ca436b41..c734de610c7778bf1a236ddfec8b6cc41b4dc804
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -1,4 +1,191 @@
 use super::*;
+use anyhow::{anyhow, Context as AnyhowContext, Result};
+
+#[derive(Debug, Clone, Copy)]
+enum AiScriptGenerationStage {
+    SelectModel,
+    ResolveProvider,
+    RequestCompletion,
+    ExtractScript,
+    CreateScriptFile,
+    WriteScriptFile,
+    OpenEditor,
+}
+
+impl AiScriptGenerationStage {
+    fn as_str(self) -> &'static str {
+        match self {
+            Self::SelectModel => "select_model",
+            Self::ResolveProvider => "resolve_provider",
+            Self::RequestCompletion => "request_completion",
+            Self::ExtractScript => "extract_script",
+            Self::CreateScriptFile => "create_script_file",
+            Self::WriteScriptFile => "write_script_file",
+            Self::OpenEditor => "open_editor",
+        }
+    }
+}
+
+const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You are ScriptKitScriptGenerator.
+Generate a complete Script Kit TypeScript script and return code only.
+Requirements:
+- Include metadata comments at the top:
+  // Name: <script name>
+  // Description: <short summary>
+- Include: import "@scriptkit/sdk";
+- Use idiomatic Script Kit APIs (for example await arg(), await div(), await editor()) when useful.
+- Prefer clear async/await flow and practical defaults.
+- Return a full runnable script with no extra explanation."#;
+
+fn build_ai_script_generation_user_prompt(description: &str) -> String {
+    format!(
+        "Generate a complete Script Kit script for this request:\n\n{}\n\nReturn only the TypeScript script source.",
+        description.trim()
+    )
+}
+
+fn select_default_ai_script_model(registry: &crate::ai::ProviderRegistry) -> Option<crate::ai::ModelInfo> {
+    registry
+        .get_all_models()
+        .into_iter()
+        .find(|model| model.provider.eq_ignore_ascii_case("vercel"))
+        .or_else(|| registry.get_all_models().into_iter().next())
+}
+
+fn extract_generated_script_source(raw_response: &str) -> Option<String> {
+    let trimmed = raw_response.trim();
+    if trimmed.is_empty() {
+        return None;
+    }
+
+    // Prefer typed code fences first, then generic fences.
+    for fence in ["```typescript", "```ts", "```javascript", "```js", "```"] {
+        if let Some(content) = extract_first_fenced_block(trimmed, fence) {
+            return normalize_generated_script(content);
+        }
+    }
+
+    normalize_generated_script(trimmed.to_string())
+}
+
+fn extract_first_fenced_block(response: &str, fence_start: &str) -> Option<String> {
+    let start_index = response.find(fence_start)?;
+    let after_fence = &response[start_index + fence_start.len()..];
+    let after_newline = after_fence
+        .strip_prefix("\r\n")
+        .or_else(|| after_fence.strip_prefix('\n'))
+        .unwrap_or(after_fence);
+    let end_index = after_newline.find("```")?;
+    Some(after_newline[..end_index].trim().to_string())
+}
+
+fn normalize_generated_script(content: String) -> Option<String> {
+    let trimmed = content.trim();
+    if trimmed.is_empty() {
+        None
+    } else {
+        Some(format!("{trimmed}\n"))
+    }
+}
+
+fn derive_script_name_from_description(description: &str) -> String {
+    let mut slug = String::new();
+    let mut last_was_dash = false;
+
+    for ch in description.chars() {
+        let lower = ch.to_ascii_lowercase();
+        if lower.is_ascii_alphanumeric() {
+            slug.push(lower);
+            last_was_dash = false;
+        } else if !last_was_dash && !slug.is_empty() {
+            slug.push('-');
+            last_was_dash = true;
+        }
+
+        if slug.len() >= 48 {
+            break;
+        }
+    }
+
+    while slug.ends_with('-') {
+        slug.pop();
+    }
+
+    if slug.is_empty() {
+        "ai-generated-script".to_string()
+    } else {
+        slug
+    }
+}
+
+fn generate_script_via_ai_backend(
+    registry: &crate::ai::ProviderRegistry,
+    model_id: &str,
+    prompt_description: &str,
+    config: &crate::config::Config,
+) -> Result<std::path::PathBuf> {
+    let provider = registry
+        .find_provider_for_model(model_id)
+        .cloned()
+        .ok_or_else(|| {
+            anyhow!(
+                "state={} attempted=find_provider_for_model model_id={} failure=provider_not_found",
+                AiScriptGenerationStage::ResolveProvider.as_str(),
+                model_id
+            )
+        })?;
+
+    let request_messages = vec![
+        crate::ai::ProviderMessage::system(AI_SCRIPT_GENERATION_SYSTEM_PROMPT),
+        crate::ai::ProviderMessage::user(build_ai_script_generation_user_prompt(prompt_description)),
+    ];
+
+    let ai_response = provider
+        .send_message(&request_messages, model_id)
+        .with_context(|| {
+            format!(
+                "state={} attempted=send_message model_id={} provider={} failure=provider_call_failed",
+                AiScriptGenerationStage::RequestCompletion.as_str(),
+                model_id,
+                provider.provider_id()
+            )
+        })?;
+
+    let generated_script = extract_generated_script_source(&ai_response).ok_or_else(|| {
+        anyhow!(
+            "state={} attempted=extract_generated_script model_id={} failure=empty_script_response",
+            AiScriptGenerationStage::ExtractScript.as_str(),
+            model_id
+        )
+    })?;
+
+    let script_name = derive_script_name_from_description(prompt_description);
+    let script_path = crate::script_creation::create_new_script(&script_name).with_context(|| {
+        format!(
+            "state={} attempted=create_new_script name={} failure=create_script_failed",
+            AiScriptGenerationStage::CreateScriptFile.as_str(),
+            script_name
+        )
+    })?;
+
+    std::fs::write(&script_path, generated_script).with_context(|| {
+        format!(
+            "state={} attempted=write_script path={} failure=write_failed",
+            AiScriptGenerationStage::WriteScriptFile.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    crate::script_creation::open_in_editor(&script_path, config).with_context(|| {
+        format!(
+            "state={} attempted=open_in_editor path={} failure=open_editor_failed",
+            AiScriptGenerationStage::OpenEditor.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
 
 impl ScriptListApp {
     pub(crate) fn is_in_prompt(&self) -> bool {
@@ -274,6 +461,146 @@
         cx.notify();
     }
 
+    /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
+    /// The generated script is saved to disk and opened in the configured editor.
+    pub fn generate_script_from_ai_prompt(
+        &mut self,
+        prompt_description: String,
+        cx: &mut Context<Self>,
+    ) {
+        let prompt_description = prompt_description.trim().to_string();
+        if prompt_description.is_empty() {
+            return;
+        }
+
+        let registry = self
+            .cached_provider_registry
+            .clone()
+            .unwrap_or_else(|| crate::ai::ProviderRegistry::from_environment_with_config(Some(&self.config)));
+
+        if !registry.has_any_provider() {
+            self.toast_manager.push(
+                components::toast::Toast::error("No AI providers configured for script generation", &self.theme)
+                    .duration_ms(Some(5000)),
+            );
+            cx.notify();
+            return;
+        }
+
+        let selected_model = match select_default_ai_script_model(&registry) {
+            Some(model) => model,
+            None => {
+                let stage = AiScriptGenerationStage::SelectModel.as_str();
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    &format!(
+                        "state={} attempted=select_default_model failure=no_available_models",
+                        stage
+                    ),
+                );
+                self.toast_manager.push(
+                    components::toast::Toast::error(
+                        "No AI models available for script generation",
+                        &self.theme,
+                    )
+                    .duration_ms(Some(5000)),
+                );
+                cx.notify();
+                return;
+            }
+        };
+
+        let model_id = selected_model.id.clone();
+        let provider = selected_model.provider.clone();
+        let config = self.config.clone();
+        let (tx, rx) = async_channel::bounded::<Result<std::path::PathBuf, String>>(1);
+
+        logging::log(
+            "AI_SCRIPT_GEN",
+            &format!(
+                "state=queued attempted=shift_tab_script_generation model_id={} provider={} prompt_len={}",
+                model_id,
+                provider,
+                prompt_description.len()
+            ),
+        );
+        self.show_hud("Generating script with AI...".to_string(), Some(1500), cx);
+
+        std::thread::spawn(move || {
+            logging::log(
+                "AI_SCRIPT_GEN",
+                &format!(
+                    "state=running attempted=generate_script model_id={} provider={}",
+                    model_id, provider
+                ),
+            );
+
+            let generation_result =
+                generate_script_via_ai_backend(&registry, &model_id, &prompt_description, &config)
+                    .map_err(|error| error.to_string());
+
+            if tx.send_blocking(generation_result).is_err() {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=send_result failure=result_channel_closed",
+                );
+            }
+        });
+
+        cx.spawn(async move |this, cx| {
+            let Ok(result) = rx.recv().await else {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=recv_result failure=result_channel_closed",
+                );
+                return;
+            };
+
+            let _ = cx.update(|cx| {
+                this.update(cx, |app, cx| {
+                    match result {
+                        Ok(script_path) => {
+                            let script_name = script_path
+                                .file_name()
+                                .and_then(|name| name.to_str())
+                                .unwrap_or("generated script");
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!(
+                                    "state=completed attempted=generate_script path={}",
+                                    script_path.display()
+                                ),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::success(
+                                    format!("Generated and opened {}", script_name),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(3500)),
+                            );
+                            app.close_and_reset_window(cx);
+                        }
+                        Err(error) => {
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!("state=failed attempted=generate_script error={}", error),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::error(
+                                    format!("Failed to generate script: {}", error),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(7000)),
+                            );
+                            cx.notify();
+                        }
+                    }
+                })
+            });
+        })
+        .detach();
+    }
+
     /// Rebuild the cached provider registry in a background thread.
     /// Called after config changes (API key saved, Claude Code enabled, etc.)
     pub fn rebuild_provider_registry_async(&mut self, cx: &mut Context<Self>) {
@@ -312,3 +639,42 @@
         .detach();
     }
 }
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_build_ai_script_generation_user_prompt_includes_description() {
+        let prompt = build_ai_script_generation_user_prompt("create a weather script");
+        assert!(prompt.contains("create a weather script"));
+        assert!(prompt.contains("Script Kit script"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_prefers_typescript_fence() {
+        let response = r#"
+Here's your script:
+```typescript
+// Name: Weather
+import "@scriptkit/sdk";
+```
+"#;
+        let extracted = extract_generated_script_source(response).expect("script should be extracted");
+        assert!(extracted.contains("// Name: Weather"));
+        assert!(!extracted.contains("```"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_falls_back_to_plain_text() {
+        let response = "// Name: Plain\nimport \"@scriptkit/sdk\";";
+        let extracted = extract_generated_script_source(response).expect("script should be extracted");
+        assert_eq!(extracted, "// Name: Plain\nimport \"@scriptkit/sdk\";\n");
+    }
+
+    #[test]
+    fn test_derive_script_name_from_description_uses_fallback_for_symbols() {
+        let name = derive_script_name_from_description("@@@ !!!");
+        assert_eq!(name, "ai-generated-script");
+    }
+}
diff --git a/src/app_impl/startup.rs b/src/app_impl/startup.rs
index 2e95070751083b8f4c24cd7eae8372d6bec3f6d6..441b4f23bca1b6294025c698eeb11d45815e85e0
--- a/src/app_impl/startup.rs
+++ b/src/app_impl/startup.rs
@@ -555,9 +555,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -720,20 +722,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();


thinking
**Patching new tab startup**
file update
M /Users/johnlindquist/dev/script-kit-gpui/src/app_impl/startup_new_tab.rs
@@ -8,5 +8,7 @@
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
@@ -173,4 +175,4 @@
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
@@ -178,3 +180,2 @@
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
@@ -185,4 +186,8 @@
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
apply_patch(auto_approved=true) exited 0 in 22ms:
Success. Updated the following files:
M src/app_impl/startup_new_tab.rs
file update:
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index 98a0c9cd1db5a933992c0f99cb20a3c2ca436b41..c734de610c7778bf1a236ddfec8b6cc41b4dc804
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -1,4 +1,191 @@
 use super::*;
+use anyhow::{anyhow, Context as AnyhowContext, Result};
+
+#[derive(Debug, Clone, Copy)]
+enum AiScriptGenerationStage {
+    SelectModel,
+    ResolveProvider,
+    RequestCompletion,
+    ExtractScript,
+    CreateScriptFile,
+    WriteScriptFile,
+    OpenEditor,
+}
+
+impl AiScriptGenerationStage {
+    fn as_str(self) -> &'static str {
+        match self {
+            Self::SelectModel => "select_model",
+            Self::ResolveProvider => "resolve_provider",
+            Self::RequestCompletion => "request_completion",
+            Self::ExtractScript => "extract_script",
+            Self::CreateScriptFile => "create_script_file",
+            Self::WriteScriptFile => "write_script_file",
+            Self::OpenEditor => "open_editor",
+        }
+    }
+}
+
+const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You are ScriptKitScriptGenerator.
+Generate a complete Script Kit TypeScript script and return code only.
+Requirements:
+- Include metadata comments at the top:
+  // Name: <script name>
+  // Description: <short summary>
+- Include: import "@scriptkit/sdk";
+- Use idiomatic Script Kit APIs (for example await arg(), await div(), await editor()) when useful.
+- Prefer clear async/await flow and practical defaults.
+- Return a full runnable script with no extra explanation."#;
+
+fn build_ai_script_generation_user_prompt(description: &str) -> String {
+    format!(
+        "Generate a complete Script Kit script for this request:\n\n{}\n\nReturn only the TypeScript script source.",
+        description.trim()
+    )
+}
+
+fn select_default_ai_script_model(registry: &crate::ai::ProviderRegistry) -> Option<crate::ai::ModelInfo> {
+    registry
+        .get_all_models()
+        .into_iter()
+        .find(|model| model.provider.eq_ignore_ascii_case("vercel"))
+        .or_else(|| registry.get_all_models().into_iter().next())
+}
+
+fn extract_generated_script_source(raw_response: &str) -> Option<String> {
+    let trimmed = raw_response.trim();
+    if trimmed.is_empty() {
+        return None;
+    }
+
+    // Prefer typed code fences first, then generic fences.
+    for fence in ["```typescript", "```ts", "```javascript", "```js", "```"] {
+        if let Some(content) = extract_first_fenced_block(trimmed, fence) {
+            return normalize_generated_script(content);
+        }
+    }
+
+    normalize_generated_script(trimmed.to_string())
+}
+
+fn extract_first_fenced_block(response: &str, fence_start: &str) -> Option<String> {
+    let start_index = response.find(fence_start)?;
+    let after_fence = &response[start_index + fence_start.len()..];
+    let after_newline = after_fence
+        .strip_prefix("\r\n")
+        .or_else(|| after_fence.strip_prefix('\n'))
+        .unwrap_or(after_fence);
+    let end_index = after_newline.find("```")?;
+    Some(after_newline[..end_index].trim().to_string())
+}
+
+fn normalize_generated_script(content: String) -> Option<String> {
+    let trimmed = content.trim();
+    if trimmed.is_empty() {
+        None
+    } else {
+        Some(format!("{trimmed}\n"))
+    }
+}
+
+fn derive_script_name_from_description(description: &str) -> String {
+    let mut slug = String::new();
+    let mut last_was_dash = false;
+
+    for ch in description.chars() {
+        let lower = ch.to_ascii_lowercase();
+        if lower.is_ascii_alphanumeric() {
+            slug.push(lower);
+            last_was_dash = false;
+        } else if !last_was_dash && !slug.is_empty() {
+            slug.push('-');
+            last_was_dash = true;
+        }
+
+        if slug.len() >= 48 {
+            break;
+        }
+    }
+
+    while slug.ends_with('-') {
+        slug.pop();
+    }
+
+    if slug.is_empty() {
+        "ai-generated-script".to_string()
+    } else {
+        slug
+    }
+}
+
+fn generate_script_via_ai_backend(
+    registry: &crate::ai::ProviderRegistry,
+    model_id: &str,
+    prompt_description: &str,
+    config: &crate::config::Config,
+) -> Result<std::path::PathBuf> {
+    let provider = registry
+        .find_provider_for_model(model_id)
+        .cloned()
+        .ok_or_else(|| {
+            anyhow!(
+                "state={} attempted=find_provider_for_model model_id={} failure=provider_not_found",
+                AiScriptGenerationStage::ResolveProvider.as_str(),
+                model_id
+            )
+        })?;
+
+    let request_messages = vec![
+        crate::ai::ProviderMessage::system(AI_SCRIPT_GENERATION_SYSTEM_PROMPT),
+        crate::ai::ProviderMessage::user(build_ai_script_generation_user_prompt(prompt_description)),
+    ];
+
+    let ai_response = provider
+        .send_message(&request_messages, model_id)
+        .with_context(|| {
+            format!(
+                "state={} attempted=send_message model_id={} provider={} failure=provider_call_failed",
+                AiScriptGenerationStage::RequestCompletion.as_str(),
+                model_id,
+                provider.provider_id()
+            )
+        })?;
+
+    let generated_script = extract_generated_script_source(&ai_response).ok_or_else(|| {
+        anyhow!(
+            "state={} attempted=extract_generated_script model_id={} failure=empty_script_response",
+            AiScriptGenerationStage::ExtractScript.as_str(),
+            model_id
+        )
+    })?;
+
+    let script_name = derive_script_name_from_description(prompt_description);
+    let script_path = crate::script_creation::create_new_script(&script_name).with_context(|| {
+        format!(
+            "state={} attempted=create_new_script name={} failure=create_script_failed",
+            AiScriptGenerationStage::CreateScriptFile.as_str(),
+            script_name
+        )
+    })?;
+
+    std::fs::write(&script_path, generated_script).with_context(|| {
+        format!(
+            "state={} attempted=write_script path={} failure=write_failed",
+            AiScriptGenerationStage::WriteScriptFile.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    crate::script_creation::open_in_editor(&script_path, config).with_context(|| {
+        format!(
+            "state={} attempted=open_in_editor path={} failure=open_editor_failed",
+            AiScriptGenerationStage::OpenEditor.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
 
 impl ScriptListApp {
     pub(crate) fn is_in_prompt(&self) -> bool {
@@ -274,6 +461,146 @@
         cx.notify();
     }
 
+    /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
+    /// The generated script is saved to disk and opened in the configured editor.
+    pub fn generate_script_from_ai_prompt(
+        &mut self,
+        prompt_description: String,
+        cx: &mut Context<Self>,
+    ) {
+        let prompt_description = prompt_description.trim().to_string();
+        if prompt_description.is_empty() {
+            return;
+        }
+
+        let registry = self
+            .cached_provider_registry
+            .clone()
+            .unwrap_or_else(|| crate::ai::ProviderRegistry::from_environment_with_config(Some(&self.config)));
+
+        if !registry.has_any_provider() {
+            self.toast_manager.push(
+                components::toast::Toast::error("No AI providers configured for script generation", &self.theme)
+                    .duration_ms(Some(5000)),
+            );
+            cx.notify();
+            return;
+        }
+
+        let selected_model = match select_default_ai_script_model(&registry) {
+            Some(model) => model,
+            None => {
+                let stage = AiScriptGenerationStage::SelectModel.as_str();
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    &format!(
+                        "state={} attempted=select_default_model failure=no_available_models",
+                        stage
+                    ),
+                );
+                self.toast_manager.push(
+                    components::toast::Toast::error(
+                        "No AI models available for script generation",
+                        &self.theme,
+                    )
+                    .duration_ms(Some(5000)),
+                );
+                cx.notify();
+                return;
+            }
+        };
+
+        let model_id = selected_model.id.clone();
+        let provider = selected_model.provider.clone();
+        let config = self.config.clone();
+        let (tx, rx) = async_channel::bounded::<Result<std::path::PathBuf, String>>(1);
+
+        logging::log(
+            "AI_SCRIPT_GEN",
+            &format!(
+                "state=queued attempted=shift_tab_script_generation model_id={} provider={} prompt_len={}",
+                model_id,
+                provider,
+                prompt_description.len()
+            ),
+        );
+        self.show_hud("Generating script with AI...".to_string(), Some(1500), cx);
+
+        std::thread::spawn(move || {
+            logging::log(
+                "AI_SCRIPT_GEN",
+                &format!(
+                    "state=running attempted=generate_script model_id={} provider={}",
+                    model_id, provider
+                ),
+            );
+
+            let generation_result =
+                generate_script_via_ai_backend(&registry, &model_id, &prompt_description, &config)
+                    .map_err(|error| error.to_string());
+
+            if tx.send_blocking(generation_result).is_err() {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=send_result failure=result_channel_closed",
+                );
+            }
+        });
+
+        cx.spawn(async move |this, cx| {
+            let Ok(result) = rx.recv().await else {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=recv_result failure=result_channel_closed",
+                );
+                return;
+            };
+
+            let _ = cx.update(|cx| {
+                this.update(cx, |app, cx| {
+                    match result {
+                        Ok(script_path) => {
+                            let script_name = script_path
+                                .file_name()
+                                .and_then(|name| name.to_str())
+                                .unwrap_or("generated script");
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!(
+                                    "state=completed attempted=generate_script path={}",
+                                    script_path.display()
+                                ),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::success(
+                                    format!("Generated and opened {}", script_name),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(3500)),
+                            );
+                            app.close_and_reset_window(cx);
+                        }
+                        Err(error) => {
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!("state=failed attempted=generate_script error={}", error),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::error(
+                                    format!("Failed to generate script: {}", error),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(7000)),
+                            );
+                            cx.notify();
+                        }
+                    }
+                })
+            });
+        })
+        .detach();
+    }
+
     /// Rebuild the cached provider registry in a background thread.
     /// Called after config changes (API key saved, Claude Code enabled, etc.)
     pub fn rebuild_provider_registry_async(&mut self, cx: &mut Context<Self>) {
@@ -312,3 +639,42 @@
         .detach();
     }
 }
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_build_ai_script_generation_user_prompt_includes_description() {
+        let prompt = build_ai_script_generation_user_prompt("create a weather script");
+        assert!(prompt.contains("create a weather script"));
+        assert!(prompt.contains("Script Kit script"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_prefers_typescript_fence() {
+        let response = r#"
+Here's your script:
+```typescript
+// Name: Weather
+import "@scriptkit/sdk";
+```
+"#;
+        let extracted = extract_generated_script_source(response).expect("script should be extracted");
+        assert!(extracted.contains("// Name: Weather"));
+        assert!(!extracted.contains("```"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_falls_back_to_plain_text() {
+        let response = "// Name: Plain\nimport \"@scriptkit/sdk\";";
+        let extracted = extract_generated_script_source(response).expect("script should be extracted");
+        assert_eq!(extracted, "// Name: Plain\nimport \"@scriptkit/sdk\";\n");
+    }
+
+    #[test]
+    fn test_derive_script_name_from_description_uses_fallback_for_symbols() {
+        let name = derive_script_name_from_description("@@@ !!!");
+        assert_eq!(name, "ai-generated-script");
+    }
+}
diff --git a/src/app_impl/startup.rs b/src/app_impl/startup.rs
index 2e95070751083b8f4c24cd7eae8372d6bec3f6d6..441b4f23bca1b6294025c698eeb11d45815e85e0
--- a/src/app_impl/startup.rs
+++ b/src/app_impl/startup.rs
@@ -555,9 +555,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -720,20 +722,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/startup_new_tab.rs b/src/app_impl/startup_new_tab.rs
index 6c807a195553d9f168087c2ca6d771c76e555c11..7c84270cbe91c0b1ddd460aa29db8c788766d0cf
--- a/src/app_impl/startup_new_tab.rs
+++ b/src/app_impl/startup_new_tab.rs
@@ -6,9 +6,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -171,20 +173,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();

file update:
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index 98a0c9cd1db5a933992c0f99cb20a3c2ca436b41..c734de610c7778bf1a236ddfec8b6cc41b4dc804
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -1,4 +1,191 @@
 use super::*;
+use anyhow::{anyhow, Context as AnyhowContext, Result};
+
+#[derive(Debug, Clone, Copy)]
+enum AiScriptGenerationStage {
+    SelectModel,
+    ResolveProvider,
+    RequestCompletion,
+    ExtractScript,
+    CreateScriptFile,
+    WriteScriptFile,
+    OpenEditor,
+}
+
+impl AiScriptGenerationStage {
+    fn as_str(self) -> &'static str {
+        match self {
+            Self::SelectModel => "select_model",
+            Self::ResolveProvider => "resolve_provider",
+            Self::RequestCompletion => "request_completion",
+            Self::ExtractScript => "extract_script",
+            Self::CreateScriptFile => "create_script_file",
+            Self::WriteScriptFile => "write_script_file",
+            Self::OpenEditor => "open_editor",
+        }
+    }
+}
+
+const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You are ScriptKitScriptGenerator.
+Generate a complete Script Kit TypeScript script and return code only.
+Requirements:
+- Include metadata comments at the top:
+  // Name: <script name>
+  // Description: <short summary>
+- Include: import "@scriptkit/sdk";
+- Use idiomatic Script Kit APIs (for example await arg(), await div(), await editor()) when useful.
+- Prefer clear async/await flow and practical defaults.
+- Return a full runnable script with no extra explanation."#;
+
+fn build_ai_script_generation_user_prompt(description: &str) -> String {
+    format!(
+        "Generate a complete Script Kit script for this request:\n\n{}\n\nReturn only the TypeScript script source.",
+        description.trim()
+    )
+}
+
+fn select_default_ai_script_model(registry: &crate::ai::ProviderRegistry) -> Option<crate::ai::ModelInfo> {
+    registry
+        .get_all_models()
+        .into_iter()
+        .find(|model| model.provider.eq_ignore_ascii_case("vercel"))
+        .or_else(|| registry.get_all_models().into_iter().next())
+}
+
+fn extract_generated_script_source(raw_response: &str) -> Option<String> {
+    let trimmed = raw_response.trim();
+    if trimmed.is_empty() {
+        return None;
+    }
+
+    // Prefer typed code fences first, then generic fences.
+    for fence in ["```typescript", "```ts", "```javascript", "```js", "```"] {
+        if let Some(content) = extract_first_fenced_block(trimmed, fence) {
+            return normalize_generated_script(content);
+        }
+    }
+
+    normalize_generated_script(trimmed.to_string())
+}
+
+fn extract_first_fenced_block(response: &str, fence_start: &str) -> Option<String> {
+    let start_index = response.find(fence_start)?;
+    let after_fence = &response[start_index + fence_start.len()..];
+    let after_newline = after_fence
+        .strip_prefix("\r\n")
+        .or_else(|| after_fence.strip_prefix('\n'))
+        .unwrap_or(after_fence);
+    let end_index = after_newline.find("```")?;
+    Some(after_newline[..end_index].trim().to_string())
+}
+
+fn normalize_generated_script(content: String) -> Option<String> {
+    let trimmed = content.trim();
+    if trimmed.is_empty() {
+        None
+    } else {
+        Some(format!("{trimmed}\n"))
+    }
+}
+
+fn derive_script_name_from_description(description: &str) -> String {
+    let mut slug = String::new();
+    let mut last_was_dash = false;
+
+    for ch in description.chars() {
+        let lower = ch.to_ascii_lowercase();
+        if lower.is_ascii_alphanumeric() {
+            slug.push(lower);
+            last_was_dash = false;
+        } else if !last_was_dash && !slug.is_empty() {
+            slug.push('-');
+            last_was_dash = true;
+        }
+
+        if slug.len() >= 48 {
+            break;
+        }
+    }
+
+    while slug.ends_with('-') {
+        slug.pop();
+    }
+
+    if slug.is_empty() {
+        "ai-generated-script".to_string()
+    } else {
+        slug
+    }
+}
+
+fn generate_script_via_ai_backend(
+    registry: &crate::ai::ProviderRegistry,
+    model_id: &str,
+    prompt_description: &str,
+    config: &crate::config::Config,
+) -> Result<std::path::PathBuf> {
+    let provider = registry
+        .find_provider_for_model(model_id)
+        .cloned()
+        .ok_or_else(|| {
+            anyhow!(
+                "state={} attempted=find_provider_for_model model_id={} failure=provider_not_found",
+                AiScriptGenerationStage::ResolveProvider.as_str(),
+                model_id
+            )
+        })?;
+
+    let request_messages = vec![
+        crate::ai::ProviderMessage::system(AI_SCRIPT_GENERATION_SYSTEM_PROMPT),
+        crate::ai::ProviderMessage::user(build_ai_script_generation_user_prompt(prompt_description)),
+    ];
+
+    let ai_response = provider
+        .send_message(&request_messages, model_id)
+        .with_context(|| {
+            format!(
+                "state={} attempted=send_message model_id={} provider={} failure=provider_call_failed",
+                AiScriptGenerationStage::RequestCompletion.as_str(),
+                model_id,
+                provider.provider_id()
+            )
+        })?;
+
+    let generated_script = extract_generated_script_source(&ai_response).ok_or_else(|| {
+        anyhow!(
+            "state={} attempted=extract_generated_script model_id={} failure=empty_script_response",
+            AiScriptGenerationStage::ExtractScript.as_str(),
+            model_id
+        )
+    })?;
+
+    let script_name = derive_script_name_from_description(prompt_description);
+    let script_path = crate::script_creation::create_new_script(&script_name).with_context(|| {
+        format!(
+            "state={} attempted=create_new_script name={} failure=create_script_failed",
+            AiScriptGenerationStage::CreateScriptFile.as_str(),
+            script_name
+        )
+    })?;
+
+    std::fs::write(&script_path, generated_script).with_context(|| {
+        format!(
+            "state={} attempted=write_script path={} failure=write_failed",
+            AiScriptGenerationStage::WriteScriptFile.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    crate::script_creation::open_in_editor(&script_path, config).with_context(|| {
+        format!(
+            "state={} attempted=open_in_editor path={} failure=open_editor_failed",
+            AiScriptGenerationStage::OpenEditor.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
 
 impl ScriptListApp {
     pub(crate) fn is_in_prompt(&self) -> bool {
@@ -274,6 +461,146 @@
         cx.notify();
     }
 
+    /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
+    /// The generated script is saved to disk and opened in the configured editor.
+    pub fn generate_script_from_ai_prompt(
+        &mut self,
+        prompt_description: String,
+        cx: &mut Context<Self>,
+    ) {
+        let prompt_description = prompt_description.trim().to_string();
+        if prompt_description.is_empty() {
+            return;
+        }
+
+        let registry = self
+            .cached_provider_registry
+            .clone()
+            .unwrap_or_else(|| crate::ai::ProviderRegistry::from_environment_with_config(Some(&self.config)));
+
+        if !registry.has_any_provider() {
+            self.toast_manager.push(
+                components::toast::Toast::error("No AI providers configured for script generation", &self.theme)
+                    .duration_ms(Some(5000)),
+            );
+            cx.notify();
+            return;
+        }
+
+        let selected_model = match select_default_ai_script_model(&registry) {
+            Some(model) => model,
+            None => {
+                let stage = AiScriptGenerationStage::SelectModel.as_str();
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    &format!(
+                        "state={} attempted=select_default_model failure=no_available_models",
+                        stage
+                    ),
+                );
+                self.toast_manager.push(
+                    components::toast::Toast::error(
+                        "No AI models available for script generation",
+                        &self.theme,
+                    )
+                    .duration_ms(Some(5000)),
+                );
+                cx.notify();
+                return;
+            }
+        };
+
+        let model_id = selected_model.id.clone();
+        let provider = selected_model.provider.clone();
+        let config = self.config.clone();
+        let (tx, rx) = async_channel::bounded::<Result<std::path::PathBuf, String>>(1);
+
+        logging::log(
+            "AI_SCRIPT_GEN",
+            &format!(
+                "state=queued attempted=shift_tab_script_generation model_id={} provider={} prompt_len={}",
+                model_id,
+                provider,
+                prompt_description.len()
+            ),
+        );
+        self.show_hud("Generating script with AI...".to_string(), Some(1500), cx);
+
+        std::thread::spawn(move || {
+            logging::log(
+                "AI_SCRIPT_GEN",
+                &format!(
+                    "state=running attempted=generate_script model_id={} provider={}",
+                    model_id, provider
+                ),
+            );
+
+            let generation_result =
+                generate_script_via_ai_backend(&registry, &model_id, &prompt_description, &config)
+                    .map_err(|error| error.to_string());
+
+            if tx.send_blocking(generation_result).is_err() {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=send_result failure=result_channel_closed",
+                );
+            }
+        });
+
+        cx.spawn(async move |this, cx| {
+            let Ok(result) = rx.recv().await else {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=recv_result failure=result_channel_closed",
+                );
+                return;
+            };
+
+            let _ = cx.update(|cx| {
+                this.update(cx, |app, cx| {
+                    match result {
+                        Ok(script_path) => {
+                            let script_name = script_path
+                                .file_name()
+                                .and_then(|name| name.to_str())
+                                .unwrap_or("generated script");
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!(
+                                    "state=completed attempted=generate_script path={}",
+                                    script_path.display()
+                                ),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::success(
+                                    format!("Generated and opened {}", script_name),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(3500)),
+                            );
+                            app.close_and_reset_window(cx);
+                        }
+                        Err(error) => {
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!("state=failed attempted=generate_script error={}", error),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::error(
+                                    format!("Failed to generate script: {}", error),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(7000)),
+                            );
+                            cx.notify();
+                        }
+                    }
+                })
+            });
+        })
+        .detach();
+    }
+
     /// Rebuild the cached provider registry in a background thread.
     /// Called after config changes (API key saved, Claude Code enabled, etc.)
     pub fn rebuild_provider_registry_async(&mut self, cx: &mut Context<Self>) {
@@ -312,3 +639,42 @@
         .detach();
     }
 }
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_build_ai_script_generation_user_prompt_includes_description() {
+        let prompt = build_ai_script_generation_user_prompt("create a weather script");
+        assert!(prompt.contains("create a weather script"));
+        assert!(prompt.contains("Script Kit script"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_prefers_typescript_fence() {
+        let response = r#"
+Here's your script:
+```typescript
+// Name: Weather
+import "@scriptkit/sdk";
+```
+"#;
+        let extracted = extract_generated_script_source(response).expect("script should be extracted");
+        assert!(extracted.contains("// Name: Weather"));
+        assert!(!extracted.contains("```"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_falls_back_to_plain_text() {
+        let response = "// Name: Plain\nimport \"@scriptkit/sdk\";";
+        let extracted = extract_generated_script_source(response).expect("script should be extracted");
+        assert_eq!(extracted, "// Name: Plain\nimport \"@scriptkit/sdk\";\n");
+    }
+
+    #[test]
+    fn test_derive_script_name_from_description_uses_fallback_for_symbols() {
+        let name = derive_script_name_from_description("@@@ !!!");
+        assert_eq!(name, "ai-generated-script");
+    }
+}
diff --git a/src/app_impl/startup.rs b/src/app_impl/startup.rs
index 2e95070751083b8f4c24cd7eae8372d6bec3f6d6..441b4f23bca1b6294025c698eeb11d45815e85e0
--- a/src/app_impl/startup.rs
+++ b/src/app_impl/startup.rs
@@ -555,9 +555,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -720,20 +722,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/startup_new_tab.rs b/src/app_impl/startup_new_tab.rs
index 6c807a195553d9f168087c2ca6d771c76e555c11..7c84270cbe91c0b1ddd460aa29db8c788766d0cf
--- a/src/app_impl/startup_new_tab.rs
+++ b/src/app_impl/startup_new_tab.rs
@@ -6,9 +6,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -171,20 +173,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();


thinking
**Scoping rustfmt to changed files**
exec
/bin/zsh -lc "git diff -- src/app_impl/prompt_ai.rs | sed -n '1,320p'" in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc "git diff -- src/app_impl/startup.rs src/app_impl/startup_new_tab.rs | sed -n '1,260p'" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index 98a0c9c..c734de6 100644
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -1,4 +1,191 @@
 use super::*;
+use anyhow::{anyhow, Context as AnyhowContext, Result};
+
+#[derive(Debug, Clone, Copy)]
+enum AiScriptGenerationStage {
+    SelectModel,
+    ResolveProvider,
+    RequestCompletion,
+    ExtractScript,
+    CreateScriptFile,
+    WriteScriptFile,
+    OpenEditor,
+}
+
+impl AiScriptGenerationStage {
 succeeded in 51ms:
diff --git a/src/app_impl/startup.rs b/src/app_impl/startup.rs
index 2e95070..441b4f2 100644
--- a/src/app_impl/startup.rs
+++ b/src/app_impl/startup.rs
@@ -555,9 +555,11 @@ impl ScriptListApp {
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -720,20 +722,23 @@ impl ScriptListApp {
                                 }
                             }
file update:
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index 98a0c9cd1db5a933992c0f99cb20a3c2ca436b41..c734de610c7778bf1a236ddfec8b6cc41b4dc804
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -1,4 +1,191 @@
 use super::*;
+use anyhow::{anyhow, Context as AnyhowContext, Result};
+
+#[derive(Debug, Clone, Copy)]
+enum AiScriptGenerationStage {
+    SelectModel,
+    ResolveProvider,
+    RequestCompletion,
+    ExtractScript,
+    CreateScriptFile,
+    WriteScriptFile,
+    OpenEditor,
+}
+
+impl AiScriptGenerationStage {
+    fn as_str(self) -> &'static str {
+        match self {
+            Self::SelectModel => "select_model",
+            Self::ResolveProvider => "resolve_provider",
+            Self::RequestCompletion => "request_completion",
+            Self::ExtractScript => "extract_script",
+            Self::CreateScriptFile => "create_script_file",
+            Self::WriteScriptFile => "write_script_file",
+            Self::OpenEditor => "open_editor",
+        }
+    }
+}
+
+const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You are ScriptKitScriptGenerator.
+Generate a complete Script Kit TypeScript script and return code only.
+Requirements:
+- Include metadata comments at the top:
+  // Name: <script name>
+  // Description: <short summary>
+- Include: import "@scriptkit/sdk";
+- Use idiomatic Script Kit APIs (for example await arg(), await div(), await editor()) when useful.
+- Prefer clear async/await flow and practical defaults.
+- Return a full runnable script with no extra explanation."#;
+
+fn build_ai_script_generation_user_prompt(description: &str) -> String {
+    format!(
+        "Generate a complete Script Kit script for this request:\n\n{}\n\nReturn only the TypeScript script source.",
+        description.trim()
+    )
+}
+
+fn select_default_ai_script_model(registry: &crate::ai::ProviderRegistry) -> Option<crate::ai::ModelInfo> {
+    registry
+        .get_all_models()
+        .into_iter()
+        .find(|model| model.provider.eq_ignore_ascii_case("vercel"))
+        .or_else(|| registry.get_all_models().into_iter().next())
+}
+
+fn extract_generated_script_source(raw_response: &str) -> Option<String> {
+    let trimmed = raw_response.trim();
+    if trimmed.is_empty() {
+        return None;
+    }
+
+    // Prefer typed code fences first, then generic fences.
+    for fence in ["```typescript", "```ts", "```javascript", "```js", "```"] {
+        if let Some(content) = extract_first_fenced_block(trimmed, fence) {
+            return normalize_generated_script(content);
+        }
+    }
+
+    normalize_generated_script(trimmed.to_string())
+}
+
+fn extract_first_fenced_block(response: &str, fence_start: &str) -> Option<String> {
+    let start_index = response.find(fence_start)?;
+    let after_fence = &response[start_index + fence_start.len()..];
+    let after_newline = after_fence
+        .strip_prefix("\r\n")
+        .or_else(|| after_fence.strip_prefix('\n'))
+        .unwrap_or(after_fence);
+    let end_index = after_newline.find("```")?;
+    Some(after_newline[..end_index].trim().to_string())
+}
+
+fn normalize_generated_script(content: String) -> Option<String> {
+    let trimmed = content.trim();
+    if trimmed.is_empty() {
+        None
+    } else {
+        Some(format!("{trimmed}\n"))
+    }
+}
+
+fn derive_script_name_from_description(description: &str) -> String {
+    let mut slug = String::new();
+    let mut last_was_dash = false;
+
+    for ch in description.chars() {
+        let lower = ch.to_ascii_lowercase();
+        if lower.is_ascii_alphanumeric() {
+            slug.push(lower);
+            last_was_dash = false;
+        } else if !last_was_dash && !slug.is_empty() {
+            slug.push('-');
+            last_was_dash = true;
+        }
+
+        if slug.len() >= 48 {
+            break;
+        }
+    }
+
+    while slug.ends_with('-') {
+        slug.pop();
+    }
+
+    if slug.is_empty() {
+        "ai-generated-script".to_string()
+    } else {
+        slug
+    }
+}
+
+fn generate_script_via_ai_backend(
+    registry: &crate::ai::ProviderRegistry,
+    model_id: &str,
+    prompt_description: &str,
+    config: &crate::config::Config,
+) -> Result<std::path::PathBuf> {
+    let provider = registry
+        .find_provider_for_model(model_id)
+        .cloned()
+        .ok_or_else(|| {
+            anyhow!(
+                "state={} attempted=find_provider_for_model model_id={} failure=provider_not_found",
+                AiScriptGenerationStage::ResolveProvider.as_str(),
+                model_id
+            )
+        })?;
+
+    let request_messages = vec![
+        crate::ai::ProviderMessage::system(AI_SCRIPT_GENERATION_SYSTEM_PROMPT),
+        crate::ai::ProviderMessage::user(build_ai_script_generation_user_prompt(prompt_description)),
+    ];
+
+    let ai_response = provider
+        .send_message(&request_messages, model_id)
+        .with_context(|| {
+            format!(
+                "state={} attempted=send_message model_id={} provider={} failure=provider_call_failed",
+                AiScriptGenerationStage::RequestCompletion.as_str(),
+                model_id,
+                provider.provider_id()
+            )
+        })?;
+
+    let generated_script = extract_generated_script_source(&ai_response).ok_or_else(|| {
+        anyhow!(
+            "state={} attempted=extract_generated_script model_id={} failure=empty_script_response",
+            AiScriptGenerationStage::ExtractScript.as_str(),
+            model_id
+        )
+    })?;
+
+    let script_name = derive_script_name_from_description(prompt_description);
+    let script_path = crate::script_creation::create_new_script(&script_name).with_context(|| {
+        format!(
+            "state={} attempted=create_new_script name={} failure=create_script_failed",
+            AiScriptGenerationStage::CreateScriptFile.as_str(),
+            script_name
+        )
+    })?;
+
+    std::fs::write(&script_path, generated_script).with_context(|| {
+        format!(
+            "state={} attempted=write_script path={} failure=write_failed",
+            AiScriptGenerationStage::WriteScriptFile.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    crate::script_creation::open_in_editor(&script_path, config).with_context(|| {
+        format!(
+            "state={} attempted=open_in_editor path={} failure=open_editor_failed",
+            AiScriptGenerationStage::OpenEditor.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
 
 impl ScriptListApp {
     pub(crate) fn is_in_prompt(&self) -> bool {
@@ -274,6 +461,146 @@
         cx.notify();
     }
 
+    /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
+    /// The generated script is saved to disk and opened in the configured editor.
+    pub fn generate_script_from_ai_prompt(
+        &mut self,
+        prompt_description: String,
+        cx: &mut Context<Self>,
+    ) {
+        let prompt_description = prompt_description.trim().to_string();
+        if prompt_description.is_empty() {
+            return;
+        }
+
+        let registry = self
+            .cached_provider_registry
+            .clone()
+            .unwrap_or_else(|| crate::ai::ProviderRegistry::from_environment_with_config(Some(&self.config)));
+
+        if !registry.has_any_provider() {
+            self.toast_manager.push(
+                components::toast::Toast::error("No AI providers configured for script generation", &self.theme)
+                    .duration_ms(Some(5000)),
+            );
+            cx.notify();
+            return;
+        }
+
+        let selected_model = match select_default_ai_script_model(&registry) {
+            Some(model) => model,
+            None => {
+                let stage = AiScriptGenerationStage::SelectModel.as_str();
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    &format!(
+                        "state={} attempted=select_default_model failure=no_available_models",
+                        stage
+                    ),
+                );
+                self.toast_manager.push(
+                    components::toast::Toast::error(
+                        "No AI models available for script generation",
+                        &self.theme,
+                    )
+                    .duration_ms(Some(5000)),
+                );
+                cx.notify();
+                return;
+            }
+        };
+
+        let model_id = selected_model.id.clone();
+        let provider = selected_model.provider.clone();
+        let config = self.config.clone();
+        let (tx, rx) = async_channel::bounded::<Result<std::path::PathBuf, String>>(1);
+
+        logging::log(
+            "AI_SCRIPT_GEN",
+            &format!(
+                "state=queued attempted=shift_tab_script_generation model_id={} provider={} prompt_len={}",
+                model_id,
+                provider,
+                prompt_description.len()
+            ),
+        );
+        self.show_hud("Generating script with AI...".to_string(), Some(1500), cx);
+
+        std::thread::spawn(move || {
+            logging::log(
+                "AI_SCRIPT_GEN",
+                &format!(
+                    "state=running attempted=generate_script model_id={} provider={}",
+                    model_id, provider
+                ),
+            );
+
+            let generation_result =
+                generate_script_via_ai_backend(&registry, &model_id, &prompt_description, &config)
+                    .map_err(|error| error.to_string());
+
+            if tx.send_blocking(generation_result).is_err() {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=send_result failure=result_channel_closed",
+                );
+            }
+        });
+
+        cx.spawn(async move |this, cx| {
+            let Ok(result) = rx.recv().await else {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=recv_result failure=result_channel_closed",
+                );
+                return;
+            };
+
+            let _ = cx.update(|cx| {
+                this.update(cx, |app, cx| {
+                    match result {
+                        Ok(script_path) => {
+                            let script_name = script_path
+                                .file_name()
+                                .and_then(|name| name.to_str())
+                                .unwrap_or("generated script");
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!(
+                                    "state=completed attempted=generate_script path={}",
+                                    script_path.display()
+                                ),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::success(
+                                    format!("Generated and opened {}", script_name),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(3500)),
+                            );
+                            app.close_and_reset_window(cx);
+                        }
+                        Err(error) => {
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!("state=failed attempted=generate_script error={}", error),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::error(
+                                    format!("Failed to generate script: {}", error),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(7000)),
+                            );
+                            cx.notify();
+                        }
+                    }
+                })
+            });
+        })
+        .detach();
+    }
+
     /// Rebuild the cached provider registry in a background thread.
     /// Called after config changes (API key saved, Claude Code enabled, etc.)
     pub fn rebuild_provider_registry_async(&mut self, cx: &mut Context<Self>) {
@@ -312,3 +639,42 @@
         .detach();
     }
 }
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_build_ai_script_generation_user_prompt_includes_description() {
+        let prompt = build_ai_script_generation_user_prompt("create a weather script");
+        assert!(prompt.contains("create a weather script"));
+        assert!(prompt.contains("Script Kit script"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_prefers_typescript_fence() {
+        let response = r#"
+Here's your script:
+```typescript
+// Name: Weather
+import "@scriptkit/sdk";
+```
+"#;
+        let extracted = extract_generated_script_source(response).expect("script should be extracted");
+        assert!(extracted.contains("// Name: Weather"));
+        assert!(!extracted.contains("```"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_falls_back_to_plain_text() {
+        let response = "// Name: Plain\nimport \"@scriptkit/sdk\";";
+        let extracted = extract_generated_script_source(response).expect("script should be extracted");
+        assert_eq!(extracted, "// Name: Plain\nimport \"@scriptkit/sdk\";\n");
+    }
+
+    #[test]
+    fn test_derive_script_name_from_description_uses_fallback_for_symbols() {
+        let name = derive_script_name_from_description("@@@ !!!");
+        assert_eq!(name, "ai-generated-script");
+    }
+}
diff --git a/src/app_impl/startup.rs b/src/app_impl/startup.rs
index 2e95070751083b8f4c24cd7eae8372d6bec3f6d6..441b4f23bca1b6294025c698eeb11d45815e85e0
--- a/src/app_impl/startup.rs
+++ b/src/app_impl/startup.rs
@@ -555,9 +555,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -720,20 +722,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/startup_new_tab.rs b/src/app_impl/startup_new_tab.rs
index 6c807a195553d9f168087c2ca6d771c76e555c11..7c84270cbe91c0b1ddd460aa29db8c788766d0cf
--- a/src/app_impl/startup_new_tab.rs
+++ b/src/app_impl/startup_new_tab.rs
@@ -6,9 +6,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -171,20 +173,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();


thinking
**Reviewing type fix and lint considerations**
exec
/bin/zsh -lc "git diff -- src/app_impl/prompt_ai.rs | sed -n '320,640p'" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
+                        Err(error) => {
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!("state=failed attempted=generate_script error={}", error),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::error(
+                                    format!("Failed to generate script: {}", error),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(7000)),
+                            );
+                            cx.notify();
+                        }
+                    }
+                })
+            });
+        })
+        .detach();
+    }
file update:
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index 98a0c9cd1db5a933992c0f99cb20a3c2ca436b41..c734de610c7778bf1a236ddfec8b6cc41b4dc804
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -1,4 +1,191 @@
 use super::*;
+use anyhow::{anyhow, Context as AnyhowContext, Result};
+
+#[derive(Debug, Clone, Copy)]
+enum AiScriptGenerationStage {
+    SelectModel,
+    ResolveProvider,
+    RequestCompletion,
+    ExtractScript,
+    CreateScriptFile,
+    WriteScriptFile,
+    OpenEditor,
+}
+
+impl AiScriptGenerationStage {
+    fn as_str(self) -> &'static str {
+        match self {
+            Self::SelectModel => "select_model",
+            Self::ResolveProvider => "resolve_provider",
+            Self::RequestCompletion => "request_completion",
+            Self::ExtractScript => "extract_script",
+            Self::CreateScriptFile => "create_script_file",
+            Self::WriteScriptFile => "write_script_file",
+            Self::OpenEditor => "open_editor",
+        }
+    }
+}
+
+const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You are ScriptKitScriptGenerator.
+Generate a complete Script Kit TypeScript script and return code only.
+Requirements:
+- Include metadata comments at the top:
+  // Name: <script name>
+  // Description: <short summary>
+- Include: import "@scriptkit/sdk";
+- Use idiomatic Script Kit APIs (for example await arg(), await div(), await editor()) when useful.
+- Prefer clear async/await flow and practical defaults.
+- Return a full runnable script with no extra explanation."#;
+
+fn build_ai_script_generation_user_prompt(description: &str) -> String {
+    format!(
+        "Generate a complete Script Kit script for this request:\n\n{}\n\nReturn only the TypeScript script source.",
+        description.trim()
+    )
+}
+
+fn select_default_ai_script_model(registry: &crate::ai::ProviderRegistry) -> Option<crate::ai::ModelInfo> {
+    registry
+        .get_all_models()
+        .into_iter()
+        .find(|model| model.provider.eq_ignore_ascii_case("vercel"))
+        .or_else(|| registry.get_all_models().into_iter().next())
+}
+
+fn extract_generated_script_source(raw_response: &str) -> Option<String> {
+    let trimmed = raw_response.trim();
+    if trimmed.is_empty() {
+        return None;
+    }
+
+    // Prefer typed code fences first, then generic fences.
+    for fence in ["```typescript", "```ts", "```javascript", "```js", "```"] {
+        if let Some(content) = extract_first_fenced_block(trimmed, fence) {
+            return normalize_generated_script(content);
+        }
+    }
+
+    normalize_generated_script(trimmed.to_string())
+}
+
+fn extract_first_fenced_block(response: &str, fence_start: &str) -> Option<String> {
+    let start_index = response.find(fence_start)?;
+    let after_fence = &response[start_index + fence_start.len()..];
+    let after_newline = after_fence
+        .strip_prefix("\r\n")
+        .or_else(|| after_fence.strip_prefix('\n'))
+        .unwrap_or(after_fence);
+    let end_index = after_newline.find("```")?;
+    Some(after_newline[..end_index].trim().to_string())
+}
+
+fn normalize_generated_script(content: String) -> Option<String> {
+    let trimmed = content.trim();
+    if trimmed.is_empty() {
+        None
+    } else {
+        Some(format!("{trimmed}\n"))
+    }
+}
+
+fn derive_script_name_from_description(description: &str) -> String {
+    let mut slug = String::new();
+    let mut last_was_dash = false;
+
+    for ch in description.chars() {
+        let lower = ch.to_ascii_lowercase();
+        if lower.is_ascii_alphanumeric() {
+            slug.push(lower);
+            last_was_dash = false;
+        } else if !last_was_dash && !slug.is_empty() {
+            slug.push('-');
+            last_was_dash = true;
+        }
+
+        if slug.len() >= 48 {
+            break;
+        }
+    }
+
+    while slug.ends_with('-') {
+        slug.pop();
+    }
+
+    if slug.is_empty() {
+        "ai-generated-script".to_string()
+    } else {
+        slug
+    }
+}
+
+fn generate_script_via_ai_backend(
+    registry: &crate::ai::ProviderRegistry,
+    model_id: &str,
+    prompt_description: &str,
+    config: &crate::config::Config,
+) -> Result<std::path::PathBuf> {
+    let provider = registry
+        .find_provider_for_model(model_id)
+        .cloned()
+        .ok_or_else(|| {
+            anyhow!(
+                "state={} attempted=find_provider_for_model model_id={} failure=provider_not_found",
+                AiScriptGenerationStage::ResolveProvider.as_str(),
+                model_id
+            )
+        })?;
+
+    let request_messages = vec![
+        crate::ai::ProviderMessage::system(AI_SCRIPT_GENERATION_SYSTEM_PROMPT),
+        crate::ai::ProviderMessage::user(build_ai_script_generation_user_prompt(prompt_description)),
+    ];
+
+    let ai_response = provider
+        .send_message(&request_messages, model_id)
+        .with_context(|| {
+            format!(
+                "state={} attempted=send_message model_id={} provider={} failure=provider_call_failed",
+                AiScriptGenerationStage::RequestCompletion.as_str(),
+                model_id,
+                provider.provider_id()
+            )
+        })?;
+
+    let generated_script = extract_generated_script_source(&ai_response).ok_or_else(|| {
+        anyhow!(
+            "state={} attempted=extract_generated_script model_id={} failure=empty_script_response",
+            AiScriptGenerationStage::ExtractScript.as_str(),
+            model_id
+        )
+    })?;
+
+    let script_name = derive_script_name_from_description(prompt_description);
+    let script_path = crate::script_creation::create_new_script(&script_name).with_context(|| {
+        format!(
+            "state={} attempted=create_new_script name={} failure=create_script_failed",
+            AiScriptGenerationStage::CreateScriptFile.as_str(),
+            script_name
+        )
+    })?;
+
+    std::fs::write(&script_path, generated_script).with_context(|| {
+        format!(
+            "state={} attempted=write_script path={} failure=write_failed",
+            AiScriptGenerationStage::WriteScriptFile.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    crate::script_creation::open_in_editor(&script_path, config).with_context(|| {
+        format!(
+            "state={} attempted=open_in_editor path={} failure=open_editor_failed",
+            AiScriptGenerationStage::OpenEditor.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
 
 impl ScriptListApp {
     pub(crate) fn is_in_prompt(&self) -> bool {
@@ -274,6 +461,146 @@
         cx.notify();
     }
 
+    /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
+    /// The generated script is saved to disk and opened in the configured editor.
+    pub fn generate_script_from_ai_prompt(
+        &mut self,
+        prompt_description: String,
+        cx: &mut Context<Self>,
+    ) {
+        let prompt_description = prompt_description.trim().to_string();
+        if prompt_description.is_empty() {
+            return;
+        }
+
+        let registry = self
+            .cached_provider_registry
+            .clone()
+            .unwrap_or_else(|| crate::ai::ProviderRegistry::from_environment_with_config(Some(&self.config)));
+
+        if !registry.has_any_provider() {
+            self.toast_manager.push(
+                components::toast::Toast::error("No AI providers configured for script generation", &self.theme)
+                    .duration_ms(Some(5000)),
+            );
+            cx.notify();
+            return;
+        }
+
+        let selected_model = match select_default_ai_script_model(&registry) {
+            Some(model) => model,
+            None => {
+                let stage = AiScriptGenerationStage::SelectModel.as_str();
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    &format!(
+                        "state={} attempted=select_default_model failure=no_available_models",
+                        stage
+                    ),
+                );
+                self.toast_manager.push(
+                    components::toast::Toast::error(
+                        "No AI models available for script generation",
+                        &self.theme,
+                    )
+                    .duration_ms(Some(5000)),
+                );
+                cx.notify();
+                return;
+            }
+        };
+
+        let model_id = selected_model.id.clone();
+        let provider = selected_model.provider.clone();
+        let config = self.config.clone();
+        let (tx, rx) = async_channel::bounded::<Result<std::path::PathBuf, String>>(1);
+
+        logging::log(
+            "AI_SCRIPT_GEN",
+            &format!(
+                "state=queued attempted=shift_tab_script_generation model_id={} provider={} prompt_len={}",
+                model_id,
+                provider,
+                prompt_description.len()
+            ),
+        );
+        self.show_hud("Generating script with AI...".to_string(), Some(1500), cx);
+
+        std::thread::spawn(move || {
+            logging::log(
+                "AI_SCRIPT_GEN",
+                &format!(
+                    "state=running attempted=generate_script model_id={} provider={}",
+                    model_id, provider
+                ),
+            );
+
+            let generation_result =
+                generate_script_via_ai_backend(&registry, &model_id, &prompt_description, &config)
+                    .map_err(|error| error.to_string());
+
+            if tx.send_blocking(generation_result).is_err() {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=send_result failure=result_channel_closed",
+                );
+            }
+        });
+
+        cx.spawn(async move |this, cx| {
+            let Ok(result) = rx.recv().await else {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=recv_result failure=result_channel_closed",
+                );
+                return;
+            };
+
+            let _ = cx.update(|cx| {
+                this.update(cx, |app, cx| {
+                    match result {
+                        Ok(script_path) => {
+                            let script_name = script_path
+                                .file_name()
+                                .and_then(|name| name.to_str())
+                                .unwrap_or("generated script");
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!(
+                                    "state=completed attempted=generate_script path={}",
+                                    script_path.display()
+                                ),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::success(
+                                    format!("Generated and opened {}", script_name),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(3500)),
+                            );
+                            app.close_and_reset_window(cx);
+                        }
+                        Err(error) => {
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!("state=failed attempted=generate_script error={}", error),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::error(
+                                    format!("Failed to generate script: {}", error),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(7000)),
+                            );
+                            cx.notify();
+                        }
+                    }
+                })
+            });
+        })
+        .detach();
+    }
+
     /// Rebuild the cached provider registry in a background thread.
     /// Called after config changes (API key saved, Claude Code enabled, etc.)
     pub fn rebuild_provider_registry_async(&mut self, cx: &mut Context<Self>) {
@@ -312,3 +639,42 @@
         .detach();
     }
 }
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_build_ai_script_generation_user_prompt_includes_description() {
+        let prompt = build_ai_script_generation_user_prompt("create a weather script");
+        assert!(prompt.contains("create a weather script"));
+        assert!(prompt.contains("Script Kit script"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_prefers_typescript_fence() {
+        let response = r#"
+Here's your script:
+```typescript
+// Name: Weather
+import "@scriptkit/sdk";
+```
+"#;
+        let extracted = extract_generated_script_source(response).expect("script should be extracted");
+        assert!(extracted.contains("// Name: Weather"));
+        assert!(!extracted.contains("```"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_falls_back_to_plain_text() {
+        let response = "// Name: Plain\nimport \"@scriptkit/sdk\";";
+        let extracted = extract_generated_script_source(response).expect("script should be extracted");
+        assert_eq!(extracted, "// Name: Plain\nimport \"@scriptkit/sdk\";\n");
+    }
+
+    #[test]
+    fn test_derive_script_name_from_description_uses_fallback_for_symbols() {
+        let name = derive_script_name_from_description("@@@ !!!");
+        assert_eq!(name, "ai-generated-script");
+    }
+}
diff --git a/src/app_impl/startup.rs b/src/app_impl/startup.rs
index 2e95070751083b8f4c24cd7eae8372d6bec3f6d6..441b4f23bca1b6294025c698eeb11d45815e85e0
--- a/src/app_impl/startup.rs
+++ b/src/app_impl/startup.rs
@@ -555,9 +555,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -720,20 +722,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/startup_new_tab.rs b/src/app_impl/startup_new_tab.rs
index 6c807a195553d9f168087c2ca6d771c76e555c11..7c84270cbe91c0b1ddd460aa29db8c788766d0cf
--- a/src/app_impl/startup_new_tab.rs
+++ b/src/app_impl/startup_new_tab.rs
@@ -6,9 +6,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -171,20 +173,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();


thinking
**Identifying compile and test improvements**
codex
I still need a behavior-level test for the new key path, so I’m adding a lightweight source-audit test in `app_impl/tests.rs` that verifies `Shift+Tab` in ScriptList routes to `generate_script_from_ai_prompt` while plain Tab still routes to chat. I’ll claim that file first.
exec
/bin/zsh -lc '~/.claude/bin/swarm.ts claim --wait --id codex-shift-tab-keybind src/app_impl/tests.rs' in /Users/johnlindquist/dev/script-kit-gpui succeeded in 52ms:
Claimed for codex-shift-tab-keybind: src/app_impl/tests.rs
file update:
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index 98a0c9cd1db5a933992c0f99cb20a3c2ca436b41..c734de610c7778bf1a236ddfec8b6cc41b4dc804
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -1,4 +1,191 @@
 use super::*;
+use anyhow::{anyhow, Context as AnyhowContext, Result};
+
+#[derive(Debug, Clone, Copy)]
+enum AiScriptGenerationStage {
+    SelectModel,
+    ResolveProvider,
+    RequestCompletion,
+    ExtractScript,
+    CreateScriptFile,
+    WriteScriptFile,
+    OpenEditor,
+}
+
+impl AiScriptGenerationStage {
+    fn as_str(self) -> &'static str {
+        match self {
+            Self::SelectModel => "select_model",
+            Self::ResolveProvider => "resolve_provider",
+            Self::RequestCompletion => "request_completion",
+            Self::ExtractScript => "extract_script",
+            Self::CreateScriptFile => "create_script_file",
+            Self::WriteScriptFile => "write_script_file",
+            Self::OpenEditor => "open_editor",
+        }
+    }
+}
+
+const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You are ScriptKitScriptGenerator.
+Generate a complete Script Kit TypeScript script and return code only.
+Requirements:
+- Include metadata comments at the top:
+  // Name: <script name>
+  // Description: <short summary>
+- Include: import "@scriptkit/sdk";
+- Use idiomatic Script Kit APIs (for example await arg(), await div(), await editor()) when useful.
+- Prefer clear async/await flow and practical defaults.
+- Return a full runnable script with no extra explanation."#;
+
+fn build_ai_script_generation_user_prompt(description: &str) -> String {
+    format!(
+        "Generate a complete Script Kit script for this request:\n\n{}\n\nReturn only the TypeScript script source.",
+        description.trim()
+    )
+}
+
+fn select_default_ai_script_model(registry: &crate::ai::ProviderRegistry) -> Option<crate::ai::ModelInfo> {
+    registry
+        .get_all_models()
+        .into_iter()
+        .find(|model| model.provider.eq_ignore_ascii_case("vercel"))
+        .or_else(|| registry.get_all_models().into_iter().next())
+}
+
+fn extract_generated_script_source(raw_response: &str) -> Option<String> {
+    let trimmed = raw_response.trim();
+    if trimmed.is_empty() {
+        return None;
+    }
+
+    // Prefer typed code fences first, then generic fences.
+    for fence in ["```typescript", "```ts", "```javascript", "```js", "```"] {
+        if let Some(content) = extract_first_fenced_block(trimmed, fence) {
+            return normalize_generated_script(content);
+        }
+    }
+
+    normalize_generated_script(trimmed.to_string())
+}
+
+fn extract_first_fenced_block(response: &str, fence_start: &str) -> Option<String> {
+    let start_index = response.find(fence_start)?;
+    let after_fence = &response[start_index + fence_start.len()..];
+    let after_newline = after_fence
+        .strip_prefix("\r\n")
+        .or_else(|| after_fence.strip_prefix('\n'))
+        .unwrap_or(after_fence);
+    let end_index = after_newline.find("```")?;
+    Some(after_newline[..end_index].trim().to_string())
+}
+
+fn normalize_generated_script(content: String) -> Option<String> {
+    let trimmed = content.trim();
+    if trimmed.is_empty() {
+        None
+    } else {
+        Some(format!("{trimmed}\n"))
+    }
+}
+
+fn derive_script_name_from_description(description: &str) -> String {
+    let mut slug = String::new();
+    let mut last_was_dash = false;
+
+    for ch in description.chars() {
+        let lower = ch.to_ascii_lowercase();
+        if lower.is_ascii_alphanumeric() {
+            slug.push(lower);
+            last_was_dash = false;
+        } else if !last_was_dash && !slug.is_empty() {
+            slug.push('-');
+            last_was_dash = true;
+        }
+
+        if slug.len() >= 48 {
+            break;
+        }
+    }
+
+    while slug.ends_with('-') {
+        slug.pop();
+    }
+
+    if slug.is_empty() {
+        "ai-generated-script".to_string()
+    } else {
+        slug
+    }
+}
+
+fn generate_script_via_ai_backend(
+    registry: &crate::ai::ProviderRegistry,
+    model_id: &str,
+    prompt_description: &str,
+    config: &crate::config::Config,
+) -> Result<std::path::PathBuf> {
+    let provider = registry
+        .find_provider_for_model(model_id)
+        .cloned()
+        .ok_or_else(|| {
+            anyhow!(
+                "state={} attempted=find_provider_for_model model_id={} failure=provider_not_found",
+                AiScriptGenerationStage::ResolveProvider.as_str(),
+                model_id
+            )
+        })?;
+
+    let request_messages = vec![
+        crate::ai::ProviderMessage::system(AI_SCRIPT_GENERATION_SYSTEM_PROMPT),
+        crate::ai::ProviderMessage::user(build_ai_script_generation_user_prompt(prompt_description)),
+    ];
+
+    let ai_response = provider
+        .send_message(&request_messages, model_id)
+        .with_context(|| {
+            format!(
+                "state={} attempted=send_message model_id={} provider={} failure=provider_call_failed",
+                AiScriptGenerationStage::RequestCompletion.as_str(),
+                model_id,
+                provider.provider_id()
+            )
+        })?;
+
+    let generated_script = extract_generated_script_source(&ai_response).ok_or_else(|| {
+        anyhow!(
+            "state={} attempted=extract_generated_script model_id={} failure=empty_script_response",
+            AiScriptGenerationStage::ExtractScript.as_str(),
+            model_id
+        )
+    })?;
+
+    let script_name = derive_script_name_from_description(prompt_description);
+    let script_path = crate::script_creation::create_new_script(&script_name).with_context(|| {
+        format!(
+            "state={} attempted=create_new_script name={} failure=create_script_failed",
+            AiScriptGenerationStage::CreateScriptFile.as_str(),
+            script_name
+        )
+    })?;
+
+    std::fs::write(&script_path, generated_script).with_context(|| {
+        format!(
+            "state={} attempted=write_script path={} failure=write_failed",
+            AiScriptGenerationStage::WriteScriptFile.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    crate::script_creation::open_in_editor(&script_path, config).with_context(|| {
+        format!(
+            "state={} attempted=open_in_editor path={} failure=open_editor_failed",
+            AiScriptGenerationStage::OpenEditor.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
 
 impl ScriptListApp {
     pub(crate) fn is_in_prompt(&self) -> bool {
@@ -274,6 +461,146 @@
         cx.notify();
     }
 
+    /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
+    /// The generated script is saved to disk and opened in the configured editor.
+    pub fn generate_script_from_ai_prompt(
+        &mut self,
+        prompt_description: String,
+        cx: &mut Context<Self>,
+    ) {
+        let prompt_description = prompt_description.trim().to_string();
+        if prompt_description.is_empty() {
+            return;
+        }
+
+        let registry = self
+            .cached_provider_registry
+            .clone()
+            .unwrap_or_else(|| crate::ai::ProviderRegistry::from_environment_with_config(Some(&self.config)));
+
+        if !registry.has_any_provider() {
+            self.toast_manager.push(
+                components::toast::Toast::error("No AI providers configured for script generation", &self.theme)
+                    .duration_ms(Some(5000)),
+            );
+            cx.notify();
+            return;
+        }
+
+        let selected_model = match select_default_ai_script_model(&registry) {
+            Some(model) => model,
+            None => {
+                let stage = AiScriptGenerationStage::SelectModel.as_str();
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    &format!(
+                        "state={} attempted=select_default_model failure=no_available_models",
+                        stage
+                    ),
+                );
+                self.toast_manager.push(
+                    components::toast::Toast::error(
+                        "No AI models available for script generation",
+                        &self.theme,
+                    )
+                    .duration_ms(Some(5000)),
+                );
+                cx.notify();
+                return;
+            }
+        };
+
+        let model_id = selected_model.id.clone();
+        let provider = selected_model.provider.clone();
+        let config = self.config.clone();
+        let (tx, rx) = async_channel::bounded::<Result<std::path::PathBuf, String>>(1);
+
+        logging::log(
+            "AI_SCRIPT_GEN",
+            &format!(
+                "state=queued attempted=shift_tab_script_generation model_id={} provider={} prompt_len={}",
+                model_id,
+                provider,
+                prompt_description.len()
+            ),
+        );
+        self.show_hud("Generating script with AI...".to_string(), Some(1500), cx);
+
+        std::thread::spawn(move || {
+            logging::log(
+                "AI_SCRIPT_GEN",
+                &format!(
+                    "state=running attempted=generate_script model_id={} provider={}",
+                    model_id, provider
+                ),
+            );
+
+            let generation_result =
+                generate_script_via_ai_backend(&registry, &model_id, &prompt_description, &config)
+                    .map_err(|error| error.to_string());
+
+            if tx.send_blocking(generation_result).is_err() {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=send_result failure=result_channel_closed",
+                );
+            }
+        });
+
+        cx.spawn(async move |this, cx| {
+            let Ok(result) = rx.recv().await else {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=recv_result failure=result_channel_closed",
+                );
+                return;
+            };
+
+            let _ = cx.update(|cx| {
+                this.update(cx, |app, cx| {
+                    match result {
+                        Ok(script_path) => {
+                            let script_name = script_path
+                                .file_name()
+                                .and_then(|name| name.to_str())
+                                .unwrap_or("generated script");
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!(
+                                    "state=completed attempted=generate_script path={}",
+                                    script_path.display()
+                                ),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::success(
+                                    format!("Generated and opened {}", script_name),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(3500)),
+                            );
+                            app.close_and_reset_window(cx);
+                        }
+                        Err(error) => {
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!("state=failed attempted=generate_script error={}", error),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::error(
+                                    format!("Failed to generate script: {}", error),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(7000)),
+                            );
+                            cx.notify();
+                        }
+                    }
+                })
+            });
+        })
+        .detach();
+    }
+
     /// Rebuild the cached provider registry in a background thread.
     /// Called after config changes (API key saved, Claude Code enabled, etc.)
     pub fn rebuild_provider_registry_async(&mut self, cx: &mut Context<Self>) {
@@ -312,3 +639,42 @@
         .detach();
     }
 }
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_build_ai_script_generation_user_prompt_includes_description() {
+        let prompt = build_ai_script_generation_user_prompt("create a weather script");
+        assert!(prompt.contains("create a weather script"));
+        assert!(prompt.contains("Script Kit script"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_prefers_typescript_fence() {
+        let response = r#"
+Here's your script:
+```typescript
+// Name: Weather
+import "@scriptkit/sdk";
+```
+"#;
+        let extracted = extract_generated_script_source(response).expect("script should be extracted");
+        assert!(extracted.contains("// Name: Weather"));
+        assert!(!extracted.contains("```"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_falls_back_to_plain_text() {
+        let response = "// Name: Plain\nimport \"@scriptkit/sdk\";";
+        let extracted = extract_generated_script_source(response).expect("script should be extracted");
+        assert_eq!(extracted, "// Name: Plain\nimport \"@scriptkit/sdk\";\n");
+    }
+
+    #[test]
+    fn test_derive_script_name_from_description_uses_fallback_for_symbols() {
+        let name = derive_script_name_from_description("@@@ !!!");
+        assert_eq!(name, "ai-generated-script");
+    }
+}
diff --git a/src/app_impl/startup.rs b/src/app_impl/startup.rs
index 2e95070751083b8f4c24cd7eae8372d6bec3f6d6..441b4f23bca1b6294025c698eeb11d45815e85e0
--- a/src/app_impl/startup.rs
+++ b/src/app_impl/startup.rs
@@ -555,9 +555,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -720,20 +722,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/startup_new_tab.rs b/src/app_impl/startup_new_tab.rs
index 6c807a195553d9f168087c2ca6d771c76e555c11..7c84270cbe91c0b1ddd460aa29db8c788766d0cf
--- a/src/app_impl/startup_new_tab.rs
+++ b/src/app_impl/startup_new_tab.rs
@@ -6,9 +6,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -171,20 +173,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();


thinking
**Adding imports and tests**
file update
M /Users/johnlindquist/dev/script-kit-gpui/src/app_impl/tests.rs
@@ -3,2 +3,3 @@
     use super::{calculate_fallback_error_message, ScriptListApp};
+    use std::fs;
 
@@ -48,2 +49,26 @@
     }
+
+    #[test]
+    fn test_shift_tab_routes_to_ai_script_generation_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("if has_shift")
+                && startup_tab.contains("this.generate_script_from_ai_prompt(query, cx);"),
+            "Shift+Tab in ScriptList should route to generate_script_from_ai_prompt. \
+             Missing expected branch in startup_new_tab.rs"
+        );
+    }
+
+    #[test]
+    fn test_tab_still_routes_to_inline_ai_chat_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("this.show_inline_ai_chat(Some(query), cx);"),
+            "Tab in ScriptList should continue routing to show_inline_ai_chat"
+        );
+    }
 }
apply_patch(auto_approved=true) exited 0 in 26ms:
Success. Updated the following files:
M src/app_impl/tests.rs
file update:
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index 98a0c9cd1db5a933992c0f99cb20a3c2ca436b41..c734de610c7778bf1a236ddfec8b6cc41b4dc804
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -1,4 +1,191 @@
 use super::*;
+use anyhow::{anyhow, Context as AnyhowContext, Result};
+
+#[derive(Debug, Clone, Copy)]
+enum AiScriptGenerationStage {
+    SelectModel,
+    ResolveProvider,
+    RequestCompletion,
+    ExtractScript,
+    CreateScriptFile,
+    WriteScriptFile,
+    OpenEditor,
+}
+
+impl AiScriptGenerationStage {
+    fn as_str(self) -> &'static str {
+        match self {
+            Self::SelectModel => "select_model",
+            Self::ResolveProvider => "resolve_provider",
+            Self::RequestCompletion => "request_completion",
+            Self::ExtractScript => "extract_script",
+            Self::CreateScriptFile => "create_script_file",
+            Self::WriteScriptFile => "write_script_file",
+            Self::OpenEditor => "open_editor",
+        }
+    }
+}
+
+const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You are ScriptKitScriptGenerator.
+Generate a complete Script Kit TypeScript script and return code only.
+Requirements:
+- Include metadata comments at the top:
+  // Name: <script name>
+  // Description: <short summary>
+- Include: import "@scriptkit/sdk";
+- Use idiomatic Script Kit APIs (for example await arg(), await div(), await editor()) when useful.
+- Prefer clear async/await flow and practical defaults.
+- Return a full runnable script with no extra explanation."#;
+
+fn build_ai_script_generation_user_prompt(description: &str) -> String {
+    format!(
+        "Generate a complete Script Kit script for this request:\n\n{}\n\nReturn only the TypeScript script source.",
+        description.trim()
+    )
+}
+
+fn select_default_ai_script_model(registry: &crate::ai::ProviderRegistry) -> Option<crate::ai::ModelInfo> {
+    registry
+        .get_all_models()
+        .into_iter()
+        .find(|model| model.provider.eq_ignore_ascii_case("vercel"))
+        .or_else(|| registry.get_all_models().into_iter().next())
+}
+
+fn extract_generated_script_source(raw_response: &str) -> Option<String> {
+    let trimmed = raw_response.trim();
+    if trimmed.is_empty() {
+        return None;
+    }
+
+    // Prefer typed code fences first, then generic fences.
+    for fence in ["```typescript", "```ts", "```javascript", "```js", "```"] {
+        if let Some(content) = extract_first_fenced_block(trimmed, fence) {
+            return normalize_generated_script(content);
+        }
+    }
+
+    normalize_generated_script(trimmed.to_string())
+}
+
+fn extract_first_fenced_block(response: &str, fence_start: &str) -> Option<String> {
+    let start_index = response.find(fence_start)?;
+    let after_fence = &response[start_index + fence_start.len()..];
+    let after_newline = after_fence
+        .strip_prefix("\r\n")
+        .or_else(|| after_fence.strip_prefix('\n'))
+        .unwrap_or(after_fence);
+    let end_index = after_newline.find("```")?;
+    Some(after_newline[..end_index].trim().to_string())
+}
+
+fn normalize_generated_script(content: String) -> Option<String> {
+    let trimmed = content.trim();
+    if trimmed.is_empty() {
+        None
+    } else {
+        Some(format!("{trimmed}\n"))
+    }
+}
+
+fn derive_script_name_from_description(description: &str) -> String {
+    let mut slug = String::new();
+    let mut last_was_dash = false;
+
+    for ch in description.chars() {
+        let lower = ch.to_ascii_lowercase();
+        if lower.is_ascii_alphanumeric() {
+            slug.push(lower);
+            last_was_dash = false;
+        } else if !last_was_dash && !slug.is_empty() {
+            slug.push('-');
+            last_was_dash = true;
+        }
+
+        if slug.len() >= 48 {
+            break;
+        }
+    }
+
+    while slug.ends_with('-') {
+        slug.pop();
+    }
+
+    if slug.is_empty() {
+        "ai-generated-script".to_string()
+    } else {
+        slug
+    }
+}
+
+fn generate_script_via_ai_backend(
+    registry: &crate::ai::ProviderRegistry,
+    model_id: &str,
+    prompt_description: &str,
+    config: &crate::config::Config,
+) -> Result<std::path::PathBuf> {
+    let provider = registry
+        .find_provider_for_model(model_id)
+        .cloned()
+        .ok_or_else(|| {
+            anyhow!(
+                "state={} attempted=find_provider_for_model model_id={} failure=provider_not_found",
+                AiScriptGenerationStage::ResolveProvider.as_str(),
+                model_id
+            )
+        })?;
+
+    let request_messages = vec![
+        crate::ai::ProviderMessage::system(AI_SCRIPT_GENERATION_SYSTEM_PROMPT),
+        crate::ai::ProviderMessage::user(build_ai_script_generation_user_prompt(prompt_description)),
+    ];
+
+    let ai_response = provider
+        .send_message(&request_messages, model_id)
+        .with_context(|| {
+            format!(
+                "state={} attempted=send_message model_id={} provider={} failure=provider_call_failed",
+                AiScriptGenerationStage::RequestCompletion.as_str(),
+                model_id,
+                provider.provider_id()
+            )
+        })?;
+
+    let generated_script = extract_generated_script_source(&ai_response).ok_or_else(|| {
+        anyhow!(
+            "state={} attempted=extract_generated_script model_id={} failure=empty_script_response",
+            AiScriptGenerationStage::ExtractScript.as_str(),
+            model_id
+        )
+    })?;
+
+    let script_name = derive_script_name_from_description(prompt_description);
+    let script_path = crate::script_creation::create_new_script(&script_name).with_context(|| {
+        format!(
+            "state={} attempted=create_new_script name={} failure=create_script_failed",
+            AiScriptGenerationStage::CreateScriptFile.as_str(),
+            script_name
+        )
+    })?;
+
+    std::fs::write(&script_path, generated_script).with_context(|| {
+        format!(
+            "state={} attempted=write_script path={} failure=write_failed",
+            AiScriptGenerationStage::WriteScriptFile.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    crate::script_creation::open_in_editor(&script_path, config).with_context(|| {
+        format!(
+            "state={} attempted=open_in_editor path={} failure=open_editor_failed",
+            AiScriptGenerationStage::OpenEditor.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
 
 impl ScriptListApp {
     pub(crate) fn is_in_prompt(&self) -> bool {
@@ -274,6 +461,146 @@
         cx.notify();
     }
 
+    /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
+    /// The generated script is saved to disk and opened in the configured editor.
+    pub fn generate_script_from_ai_prompt(
+        &mut self,
+        prompt_description: String,
+        cx: &mut Context<Self>,
+    ) {
+        let prompt_description = prompt_description.trim().to_string();
+        if prompt_description.is_empty() {
+            return;
+        }
+
+        let registry = self
+            .cached_provider_registry
+            .clone()
+            .unwrap_or_else(|| crate::ai::ProviderRegistry::from_environment_with_config(Some(&self.config)));
+
+        if !registry.has_any_provider() {
+            self.toast_manager.push(
+                components::toast::Toast::error("No AI providers configured for script generation", &self.theme)
+                    .duration_ms(Some(5000)),
+            );
+            cx.notify();
+            return;
+        }
+
+        let selected_model = match select_default_ai_script_model(&registry) {
+            Some(model) => model,
+            None => {
+                let stage = AiScriptGenerationStage::SelectModel.as_str();
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    &format!(
+                        "state={} attempted=select_default_model failure=no_available_models",
+                        stage
+                    ),
+                );
+                self.toast_manager.push(
+                    components::toast::Toast::error(
+                        "No AI models available for script generation",
+                        &self.theme,
+                    )
+                    .duration_ms(Some(5000)),
+                );
+                cx.notify();
+                return;
+            }
+        };
+
+        let model_id = selected_model.id.clone();
+        let provider = selected_model.provider.clone();
+        let config = self.config.clone();
+        let (tx, rx) = async_channel::bounded::<Result<std::path::PathBuf, String>>(1);
+
+        logging::log(
+            "AI_SCRIPT_GEN",
+            &format!(
+                "state=queued attempted=shift_tab_script_generation model_id={} provider={} prompt_len={}",
+                model_id,
+                provider,
+                prompt_description.len()
+            ),
+        );
+        self.show_hud("Generating script with AI...".to_string(), Some(1500), cx);
+
+        std::thread::spawn(move || {
+            logging::log(
+                "AI_SCRIPT_GEN",
+                &format!(
+                    "state=running attempted=generate_script model_id={} provider={}",
+                    model_id, provider
+                ),
+            );
+
+            let generation_result =
+                generate_script_via_ai_backend(&registry, &model_id, &prompt_description, &config)
+                    .map_err(|error| error.to_string());
+
+            if tx.send_blocking(generation_result).is_err() {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=send_result failure=result_channel_closed",
+                );
+            }
+        });
+
+        cx.spawn(async move |this, cx| {
+            let Ok(result) = rx.recv().await else {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=recv_result failure=result_channel_closed",
+                );
+                return;
+            };
+
+            let _ = cx.update(|cx| {
+                this.update(cx, |app, cx| {
+                    match result {
+                        Ok(script_path) => {
+                            let script_name = script_path
+                                .file_name()
+                                .and_then(|name| name.to_str())
+                                .unwrap_or("generated script");
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!(
+                                    "state=completed attempted=generate_script path={}",
+                                    script_path.display()
+                                ),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::success(
+                                    format!("Generated and opened {}", script_name),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(3500)),
+                            );
+                            app.close_and_reset_window(cx);
+                        }
+                        Err(error) => {
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!("state=failed attempted=generate_script error={}", error),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::error(
+                                    format!("Failed to generate script: {}", error),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(7000)),
+                            );
+                            cx.notify();
+                        }
+                    }
+                })
+            });
+        })
+        .detach();
+    }
+
     /// Rebuild the cached provider registry in a background thread.
     /// Called after config changes (API key saved, Claude Code enabled, etc.)
     pub fn rebuild_provider_registry_async(&mut self, cx: &mut Context<Self>) {
@@ -312,3 +639,42 @@
         .detach();
     }
 }
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_build_ai_script_generation_user_prompt_includes_description() {
+        let prompt = build_ai_script_generation_user_prompt("create a weather script");
+        assert!(prompt.contains("create a weather script"));
+        assert!(prompt.contains("Script Kit script"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_prefers_typescript_fence() {
+        let response = r#"
+Here's your script:
+```typescript
+// Name: Weather
+import "@scriptkit/sdk";
+```
+"#;
+        let extracted = extract_generated_script_source(response).expect("script should be extracted");
+        assert!(extracted.contains("// Name: Weather"));
+        assert!(!extracted.contains("```"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_falls_back_to_plain_text() {
+        let response = "// Name: Plain\nimport \"@scriptkit/sdk\";";
+        let extracted = extract_generated_script_source(response).expect("script should be extracted");
+        assert_eq!(extracted, "// Name: Plain\nimport \"@scriptkit/sdk\";\n");
+    }
+
+    #[test]
+    fn test_derive_script_name_from_description_uses_fallback_for_symbols() {
+        let name = derive_script_name_from_description("@@@ !!!");
+        assert_eq!(name, "ai-generated-script");
+    }
+}
diff --git a/src/app_impl/startup.rs b/src/app_impl/startup.rs
index 2e95070751083b8f4c24cd7eae8372d6bec3f6d6..441b4f23bca1b6294025c698eeb11d45815e85e0
--- a/src/app_impl/startup.rs
+++ b/src/app_impl/startup.rs
@@ -555,9 +555,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -720,20 +722,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/startup_new_tab.rs b/src/app_impl/startup_new_tab.rs
index 6c807a195553d9f168087c2ca6d771c76e555c11..7c84270cbe91c0b1ddd460aa29db8c788766d0cf
--- a/src/app_impl/startup_new_tab.rs
+++ b/src/app_impl/startup_new_tab.rs
@@ -6,9 +6,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -171,20 +173,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/tests.rs b/src/app_impl/tests.rs
index d75ba121851d7233c5ae77bc474a64b27e7c31d2..bb78d84e5a8ec9ba80c0a2287ad0c1cba93f226d
--- a/src/app_impl/tests.rs
+++ b/src/app_impl/tests.rs
@@ -1,6 +1,7 @@
 #[cfg(test)]
 mod app_impl_state_sync_tests {
     use super::{calculate_fallback_error_message, ScriptListApp};
+    use std::fs;
 
     #[test]
     fn test_sync_builtin_query_state_updates_query_and_selection_when_changed() {
@@ -46,4 +47,28 @@
         assert!(message.contains("Could not evaluate expression"));
         assert!(message.contains("Check the syntax and try again"));
     }
+
+    #[test]
+    fn test_shift_tab_routes_to_ai_script_generation_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("if has_shift")
+                && startup_tab.contains("this.generate_script_from_ai_prompt(query, cx);"),
+            "Shift+Tab in ScriptList should route to generate_script_from_ai_prompt. \
+             Missing expected branch in startup_new_tab.rs"
+        );
+    }
+
+    #[test]
+    fn test_tab_still_routes_to_inline_ai_chat_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("this.show_inline_ai_chat(Some(query), cx);"),
+            "Tab in ScriptList should continue routing to show_inline_ai_chat"
+        );
+    }
 }

file update:
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index 98a0c9cd1db5a933992c0f99cb20a3c2ca436b41..c734de610c7778bf1a236ddfec8b6cc41b4dc804
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -1,4 +1,191 @@
 use super::*;
+use anyhow::{anyhow, Context as AnyhowContext, Result};
+
+#[derive(Debug, Clone, Copy)]
+enum AiScriptGenerationStage {
+    SelectModel,
+    ResolveProvider,
+    RequestCompletion,
+    ExtractScript,
+    CreateScriptFile,
+    WriteScriptFile,
+    OpenEditor,
+}
+
+impl AiScriptGenerationStage {
+    fn as_str(self) -> &'static str {
+        match self {
+            Self::SelectModel => "select_model",
+            Self::ResolveProvider => "resolve_provider",
+            Self::RequestCompletion => "request_completion",
+            Self::ExtractScript => "extract_script",
+            Self::CreateScriptFile => "create_script_file",
+            Self::WriteScriptFile => "write_script_file",
+            Self::OpenEditor => "open_editor",
+        }
+    }
+}
+
+const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You are ScriptKitScriptGenerator.
+Generate a complete Script Kit TypeScript script and return code only.
+Requirements:
+- Include metadata comments at the top:
+  // Name: <script name>
+  // Description: <short summary>
+- Include: import "@scriptkit/sdk";
+- Use idiomatic Script Kit APIs (for example await arg(), await div(), await editor()) when useful.
+- Prefer clear async/await flow and practical defaults.
+- Return a full runnable script with no extra explanation."#;
+
+fn build_ai_script_generation_user_prompt(description: &str) -> String {
+    format!(
+        "Generate a complete Script Kit script for this request:\n\n{}\n\nReturn only the TypeScript script source.",
+        description.trim()
+    )
+}
+
+fn select_default_ai_script_model(registry: &crate::ai::ProviderRegistry) -> Option<crate::ai::ModelInfo> {
+    registry
+        .get_all_models()
+        .into_iter()
+        .find(|model| model.provider.eq_ignore_ascii_case("vercel"))
+        .or_else(|| registry.get_all_models().into_iter().next())
+}
+
+fn extract_generated_script_source(raw_response: &str) -> Option<String> {
+    let trimmed = raw_response.trim();
+    if trimmed.is_empty() {
+        return None;
+    }
+
+    // Prefer typed code fences first, then generic fences.
+    for fence in ["```typescript", "```ts", "```javascript", "```js", "```"] {
+        if let Some(content) = extract_first_fenced_block(trimmed, fence) {
+            return normalize_generated_script(content);
+        }
+    }
+
+    normalize_generated_script(trimmed.to_string())
+}
+
+fn extract_first_fenced_block(response: &str, fence_start: &str) -> Option<String> {
+    let start_index = response.find(fence_start)?;
+    let after_fence = &response[start_index + fence_start.len()..];
+    let after_newline = after_fence
+        .strip_prefix("\r\n")
+        .or_else(|| after_fence.strip_prefix('\n'))
+        .unwrap_or(after_fence);
+    let end_index = after_newline.find("```")?;
+    Some(after_newline[..end_index].trim().to_string())
+}
+
+fn normalize_generated_script(content: String) -> Option<String> {
+    let trimmed = content.trim();
+    if trimmed.is_empty() {
+        None
+    } else {
+        Some(format!("{trimmed}\n"))
+    }
+}
+
+fn derive_script_name_from_description(description: &str) -> String {
+    let mut slug = String::new();
+    let mut last_was_dash = false;
+
+    for ch in description.chars() {
+        let lower = ch.to_ascii_lowercase();
+        if lower.is_ascii_alphanumeric() {
+            slug.push(lower);
+            last_was_dash = false;
+        } else if !last_was_dash && !slug.is_empty() {
+            slug.push('-');
+            last_was_dash = true;
+        }
+
+        if slug.len() >= 48 {
+            break;
+        }
+    }
+
+    while slug.ends_with('-') {
+        slug.pop();
+    }
+
+    if slug.is_empty() {
+        "ai-generated-script".to_string()
+    } else {
+        slug
+    }
+}
+
+fn generate_script_via_ai_backend(
+    registry: &crate::ai::ProviderRegistry,
+    model_id: &str,
+    prompt_description: &str,
+    config: &crate::config::Config,
+) -> Result<std::path::PathBuf> {
+    let provider = registry
+        .find_provider_for_model(model_id)
+        .cloned()
+        .ok_or_else(|| {
+            anyhow!(
+                "state={} attempted=find_provider_for_model model_id={} failure=provider_not_found",
+                AiScriptGenerationStage::ResolveProvider.as_str(),
+                model_id
+            )
+        })?;
+
+    let request_messages = vec![
+        crate::ai::ProviderMessage::system(AI_SCRIPT_GENERATION_SYSTEM_PROMPT),
+        crate::ai::ProviderMessage::user(build_ai_script_generation_user_prompt(prompt_description)),
+    ];
+
+    let ai_response = provider
+        .send_message(&request_messages, model_id)
+        .with_context(|| {
+            format!(
+                "state={} attempted=send_message model_id={} provider={} failure=provider_call_failed",
+                AiScriptGenerationStage::RequestCompletion.as_str(),
+                model_id,
+                provider.provider_id()
+            )
+        })?;
+
+    let generated_script = extract_generated_script_source(&ai_response).ok_or_else(|| {
+        anyhow!(
+            "state={} attempted=extract_generated_script model_id={} failure=empty_script_response",
+            AiScriptGenerationStage::ExtractScript.as_str(),
+            model_id
+        )
+    })?;
+
+    let script_name = derive_script_name_from_description(prompt_description);
+    let script_path = crate::script_creation::create_new_script(&script_name).with_context(|| {
+        format!(
+            "state={} attempted=create_new_script name={} failure=create_script_failed",
+            AiScriptGenerationStage::CreateScriptFile.as_str(),
+            script_name
+        )
+    })?;
+
+    std::fs::write(&script_path, generated_script).with_context(|| {
+        format!(
+            "state={} attempted=write_script path={} failure=write_failed",
+            AiScriptGenerationStage::WriteScriptFile.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    crate::script_creation::open_in_editor(&script_path, config).with_context(|| {
+        format!(
+            "state={} attempted=open_in_editor path={} failure=open_editor_failed",
+            AiScriptGenerationStage::OpenEditor.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
 
 impl ScriptListApp {
     pub(crate) fn is_in_prompt(&self) -> bool {
@@ -274,6 +461,146 @@
         cx.notify();
     }
 
+    /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
+    /// The generated script is saved to disk and opened in the configured editor.
+    pub fn generate_script_from_ai_prompt(
+        &mut self,
+        prompt_description: String,
+        cx: &mut Context<Self>,
+    ) {
+        let prompt_description = prompt_description.trim().to_string();
+        if prompt_description.is_empty() {
+            return;
+        }
+
+        let registry = self
+            .cached_provider_registry
+            .clone()
+            .unwrap_or_else(|| crate::ai::ProviderRegistry::from_environment_with_config(Some(&self.config)));
+
+        if !registry.has_any_provider() {
+            self.toast_manager.push(
+                components::toast::Toast::error("No AI providers configured for script generation", &self.theme)
+                    .duration_ms(Some(5000)),
+            );
+            cx.notify();
+            return;
+        }
+
+        let selected_model = match select_default_ai_script_model(&registry) {
+            Some(model) => model,
+            None => {
+                let stage = AiScriptGenerationStage::SelectModel.as_str();
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    &format!(
+                        "state={} attempted=select_default_model failure=no_available_models",
+                        stage
+                    ),
+                );
+                self.toast_manager.push(
+                    components::toast::Toast::error(
+                        "No AI models available for script generation",
+                        &self.theme,
+                    )
+                    .duration_ms(Some(5000)),
+                );
+                cx.notify();
+                return;
+            }
+        };
+
+        let model_id = selected_model.id.clone();
+        let provider = selected_model.provider.clone();
+        let config = self.config.clone();
+        let (tx, rx) = async_channel::bounded::<Result<std::path::PathBuf, String>>(1);
+
+        logging::log(
+            "AI_SCRIPT_GEN",
+            &format!(
+                "state=queued attempted=shift_tab_script_generation model_id={} provider={} prompt_len={}",
+                model_id,
+                provider,
+                prompt_description.len()
+            ),
+        );
+        self.show_hud("Generating script with AI...".to_string(), Some(1500), cx);
+
+        std::thread::spawn(move || {
+            logging::log(
+                "AI_SCRIPT_GEN",
+                &format!(
+                    "state=running attempted=generate_script model_id={} provider={}",
+                    model_id, provider
+                ),
+            );
+
+            let generation_result =
+                generate_script_via_ai_backend(&registry, &model_id, &prompt_description, &config)
+                    .map_err(|error| error.to_string());
+
+            if tx.send_blocking(generation_result).is_err() {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=send_result failure=result_channel_closed",
+                );
+            }
+        });
+
+        cx.spawn(async move |this, cx| {
+            let Ok(result) = rx.recv().await else {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=recv_result failure=result_channel_closed",
+                );
+                return;
+            };
+
+            let _ = cx.update(|cx| {
+                this.update(cx, |app, cx| {
+                    match result {
+                        Ok(script_path) => {
+                            let script_name = script_path
+                                .file_name()
+                                .and_then(|name| name.to_str())
+                                .unwrap_or("generated script");
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!(
+                                    "state=completed attempted=generate_script path={}",
+                                    script_path.display()
+                                ),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::success(
+                                    format!("Generated and opened {}", script_name),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(3500)),
+                            );
+                            app.close_and_reset_window(cx);
+                        }
+                        Err(error) => {
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!("state=failed attempted=generate_script error={}", error),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::error(
+                                    format!("Failed to generate script: {}", error),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(7000)),
+                            );
+                            cx.notify();
+                        }
+                    }
+                })
+            });
+        })
+        .detach();
+    }
+
     /// Rebuild the cached provider registry in a background thread.
     /// Called after config changes (API key saved, Claude Code enabled, etc.)
     pub fn rebuild_provider_registry_async(&mut self, cx: &mut Context<Self>) {
@@ -312,3 +639,42 @@
         .detach();
     }
 }
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_build_ai_script_generation_user_prompt_includes_description() {
+        let prompt = build_ai_script_generation_user_prompt("create a weather script");
+        assert!(prompt.contains("create a weather script"));
+        assert!(prompt.contains("Script Kit script"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_prefers_typescript_fence() {
+        let response = r#"
+Here's your script:
+```typescript
+// Name: Weather
+import "@scriptkit/sdk";
+```
+"#;
+        let extracted = extract_generated_script_source(response).expect("script should be extracted");
+        assert!(extracted.contains("// Name: Weather"));
+        assert!(!extracted.contains("```"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_falls_back_to_plain_text() {
+        let response = "// Name: Plain\nimport \"@scriptkit/sdk\";";
+        let extracted = extract_generated_script_source(response).expect("script should be extracted");
+        assert_eq!(extracted, "// Name: Plain\nimport \"@scriptkit/sdk\";\n");
+    }
+
+    #[test]
+    fn test_derive_script_name_from_description_uses_fallback_for_symbols() {
+        let name = derive_script_name_from_description("@@@ !!!");
+        assert_eq!(name, "ai-generated-script");
+    }
+}
diff --git a/src/app_impl/startup.rs b/src/app_impl/startup.rs
index 2e95070751083b8f4c24cd7eae8372d6bec3f6d6..441b4f23bca1b6294025c698eeb11d45815e85e0
--- a/src/app_impl/startup.rs
+++ b/src/app_impl/startup.rs
@@ -555,9 +555,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -720,20 +722,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/startup_new_tab.rs b/src/app_impl/startup_new_tab.rs
index 6c807a195553d9f168087c2ca6d771c76e555c11..7c84270cbe91c0b1ddd460aa29db8c788766d0cf
--- a/src/app_impl/startup_new_tab.rs
+++ b/src/app_impl/startup_new_tab.rs
@@ -6,9 +6,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -171,20 +173,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/tests.rs b/src/app_impl/tests.rs
index d75ba121851d7233c5ae77bc474a64b27e7c31d2..bb78d84e5a8ec9ba80c0a2287ad0c1cba93f226d
--- a/src/app_impl/tests.rs
+++ b/src/app_impl/tests.rs
@@ -1,6 +1,7 @@
 #[cfg(test)]
 mod app_impl_state_sync_tests {
     use super::{calculate_fallback_error_message, ScriptListApp};
+    use std::fs;
 
     #[test]
     fn test_sync_builtin_query_state_updates_query_and_selection_when_changed() {
@@ -46,4 +47,28 @@
         assert!(message.contains("Could not evaluate expression"));
         assert!(message.contains("Check the syntax and try again"));
     }
+
+    #[test]
+    fn test_shift_tab_routes_to_ai_script_generation_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("if has_shift")
+                && startup_tab.contains("this.generate_script_from_ai_prompt(query, cx);"),
+            "Shift+Tab in ScriptList should route to generate_script_from_ai_prompt. \
+             Missing expected branch in startup_new_tab.rs"
+        );
+    }
+
+    #[test]
+    fn test_tab_still_routes_to_inline_ai_chat_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("this.show_inline_ai_chat(Some(query), cx);"),
+            "Tab in ScriptList should continue routing to show_inline_ai_chat"
+        );
+    }
 }


thinking
**Optimizing channel type and model selection**
file update
M /Users/johnlindquist/dev/script-kit-gpui/src/app_impl/prompt_ai.rs
@@ -46,8 +46,11 @@
 
-fn select_default_ai_script_model(registry: &crate::ai::ProviderRegistry) -> Option<crate::ai::ModelInfo> {
-    registry
-        .get_all_models()
-        .into_iter()
+fn select_default_ai_script_model(
+    registry: &crate::ai::ProviderRegistry,
+) -> Option<crate::ai::ModelInfo> {
+    let all_models = registry.get_all_models();
+    all_models
+        .iter()
         .find(|model| model.provider.eq_ignore_ascii_case("vercel"))
-        .or_else(|| registry.get_all_models().into_iter().next())
+        .cloned()
+        .or_else(|| all_models.first().cloned())
 }
@@ -475,6 +478,5 @@
 
-        let registry = self
-            .cached_provider_registry
-            .clone()
-            .unwrap_or_else(|| crate::ai::ProviderRegistry::from_environment_with_config(Some(&self.config)));
+        let registry = self.cached_provider_registry.clone().unwrap_or_else(|| {
+            crate::ai::ProviderRegistry::from_environment_with_config(Some(&self.config))
+        });
 
@@ -515,3 +517,3 @@
         let config = self.config.clone();
-        let (tx, rx) = async_channel::bounded::<Result<std::path::PathBuf, String>>(1);
+        let (tx, rx) = async_channel::bounded::<std::result::Result<std::path::PathBuf, String>>(1);
 
@@ -662,3 +664,4 @@
 "#;
-        let extracted = extract_generated_script_source(response).expect("script should be extracted");
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
         assert!(extracted.contains("// Name: Weather"));
@@ -670,3 +673,4 @@
         let response = "// Name: Plain\nimport \"@scriptkit/sdk\";";
-        let extracted = extract_generated_script_source(response).expect("script should be extracted");
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
         assert_eq!(extracted, "// Name: Plain\nimport \"@scriptkit/sdk\";\n");
apply_patch(auto_approved=true) exited 0 in 27ms:
Success. Updated the following files:
M src/app_impl/prompt_ai.rs
file update:
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index 98a0c9cd1db5a933992c0f99cb20a3c2ca436b41..c985a5090574816267103019d1b272e1e3151a38
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -1,4 +1,194 @@
 use super::*;
+use anyhow::{anyhow, Context as AnyhowContext, Result};
+
+#[derive(Debug, Clone, Copy)]
+enum AiScriptGenerationStage {
+    SelectModel,
+    ResolveProvider,
+    RequestCompletion,
+    ExtractScript,
+    CreateScriptFile,
+    WriteScriptFile,
+    OpenEditor,
+}
+
+impl AiScriptGenerationStage {
+    fn as_str(self) -> &'static str {
+        match self {
+            Self::SelectModel => "select_model",
+            Self::ResolveProvider => "resolve_provider",
+            Self::RequestCompletion => "request_completion",
+            Self::ExtractScript => "extract_script",
+            Self::CreateScriptFile => "create_script_file",
+            Self::WriteScriptFile => "write_script_file",
+            Self::OpenEditor => "open_editor",
+        }
+    }
+}
+
+const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You are ScriptKitScriptGenerator.
+Generate a complete Script Kit TypeScript script and return code only.
+Requirements:
+- Include metadata comments at the top:
+  // Name: <script name>
+  // Description: <short summary>
+- Include: import "@scriptkit/sdk";
+- Use idiomatic Script Kit APIs (for example await arg(), await div(), await editor()) when useful.
+- Prefer clear async/await flow and practical defaults.
+- Return a full runnable script with no extra explanation."#;
+
+fn build_ai_script_generation_user_prompt(description: &str) -> String {
+    format!(
+        "Generate a complete Script Kit script for this request:\n\n{}\n\nReturn only the TypeScript script source.",
+        description.trim()
+    )
+}
+
+fn select_default_ai_script_model(
+    registry: &crate::ai::ProviderRegistry,
+) -> Option<crate::ai::ModelInfo> {
+    let all_models = registry.get_all_models();
+    all_models
+        .iter()
+        .find(|model| model.provider.eq_ignore_ascii_case("vercel"))
+        .cloned()
+        .or_else(|| all_models.first().cloned())
+}
+
+fn extract_generated_script_source(raw_response: &str) -> Option<String> {
+    let trimmed = raw_response.trim();
+    if trimmed.is_empty() {
+        return None;
+    }
+
+    // Prefer typed code fences first, then generic fences.
+    for fence in ["```typescript", "```ts", "```javascript", "```js", "```"] {
+        if let Some(content) = extract_first_fenced_block(trimmed, fence) {
+            return normalize_generated_script(content);
+        }
+    }
+
+    normalize_generated_script(trimmed.to_string())
+}
+
+fn extract_first_fenced_block(response: &str, fence_start: &str) -> Option<String> {
+    let start_index = response.find(fence_start)?;
+    let after_fence = &response[start_index + fence_start.len()..];
+    let after_newline = after_fence
+        .strip_prefix("\r\n")
+        .or_else(|| after_fence.strip_prefix('\n'))
+        .unwrap_or(after_fence);
+    let end_index = after_newline.find("```")?;
+    Some(after_newline[..end_index].trim().to_string())
+}
+
+fn normalize_generated_script(content: String) -> Option<String> {
+    let trimmed = content.trim();
+    if trimmed.is_empty() {
+        None
+    } else {
+        Some(format!("{trimmed}\n"))
+    }
+}
+
+fn derive_script_name_from_description(description: &str) -> String {
+    let mut slug = String::new();
+    let mut last_was_dash = false;
+
+    for ch in description.chars() {
+        let lower = ch.to_ascii_lowercase();
+        if lower.is_ascii_alphanumeric() {
+            slug.push(lower);
+            last_was_dash = false;
+        } else if !last_was_dash && !slug.is_empty() {
+            slug.push('-');
+            last_was_dash = true;
+        }
+
+        if slug.len() >= 48 {
+            break;
+        }
+    }
+
+    while slug.ends_with('-') {
+        slug.pop();
+    }
+
+    if slug.is_empty() {
+        "ai-generated-script".to_string()
+    } else {
+        slug
+    }
+}
+
+fn generate_script_via_ai_backend(
+    registry: &crate::ai::ProviderRegistry,
+    model_id: &str,
+    prompt_description: &str,
+    config: &crate::config::Config,
+) -> Result<std::path::PathBuf> {
+    let provider = registry
+        .find_provider_for_model(model_id)
+        .cloned()
+        .ok_or_else(|| {
+            anyhow!(
+                "state={} attempted=find_provider_for_model model_id={} failure=provider_not_found",
+                AiScriptGenerationStage::ResolveProvider.as_str(),
+                model_id
+            )
+        })?;
+
+    let request_messages = vec![
+        crate::ai::ProviderMessage::system(AI_SCRIPT_GENERATION_SYSTEM_PROMPT),
+        crate::ai::ProviderMessage::user(build_ai_script_generation_user_prompt(prompt_description)),
+    ];
+
+    let ai_response = provider
+        .send_message(&request_messages, model_id)
+        .with_context(|| {
+            format!(
+                "state={} attempted=send_message model_id={} provider={} failure=provider_call_failed",
+                AiScriptGenerationStage::RequestCompletion.as_str(),
+                model_id,
+                provider.provider_id()
+            )
+        })?;
+
+    let generated_script = extract_generated_script_source(&ai_response).ok_or_else(|| {
+        anyhow!(
+            "state={} attempted=extract_generated_script model_id={} failure=empty_script_response",
+            AiScriptGenerationStage::ExtractScript.as_str(),
+            model_id
+        )
+    })?;
+
+    let script_name = derive_script_name_from_description(prompt_description);
+    let script_path = crate::script_creation::create_new_script(&script_name).with_context(|| {
+        format!(
+            "state={} attempted=create_new_script name={} failure=create_script_failed",
+            AiScriptGenerationStage::CreateScriptFile.as_str(),
+            script_name
+        )
+    })?;
+
+    std::fs::write(&script_path, generated_script).with_context(|| {
+        format!(
+            "state={} attempted=write_script path={} failure=write_failed",
+            AiScriptGenerationStage::WriteScriptFile.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    crate::script_creation::open_in_editor(&script_path, config).with_context(|| {
+        format!(
+            "state={} attempted=open_in_editor path={} failure=open_editor_failed",
+            AiScriptGenerationStage::OpenEditor.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
 
 impl ScriptListApp {
     pub(crate) fn is_in_prompt(&self) -> bool {
@@ -274,6 +464,145 @@
         cx.notify();
     }
 
+    /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
+    /// The generated script is saved to disk and opened in the configured editor.
+    pub fn generate_script_from_ai_prompt(
+        &mut self,
+        prompt_description: String,
+        cx: &mut Context<Self>,
+    ) {
+        let prompt_description = prompt_description.trim().to_string();
+        if prompt_description.is_empty() {
+            return;
+        }
+
+        let registry = self.cached_provider_registry.clone().unwrap_or_else(|| {
+            crate::ai::ProviderRegistry::from_environment_with_config(Some(&self.config))
+        });
+
+        if !registry.has_any_provider() {
+            self.toast_manager.push(
+                components::toast::Toast::error("No AI providers configured for script generation", &self.theme)
+                    .duration_ms(Some(5000)),
+            );
+            cx.notify();
+            return;
+        }
+
+        let selected_model = match select_default_ai_script_model(&registry) {
+            Some(model) => model,
+            None => {
+                let stage = AiScriptGenerationStage::SelectModel.as_str();
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    &format!(
+                        "state={} attempted=select_default_model failure=no_available_models",
+                        stage
+                    ),
+                );
+                self.toast_manager.push(
+                    components::toast::Toast::error(
+                        "No AI models available for script generation",
+                        &self.theme,
+                    )
+                    .duration_ms(Some(5000)),
+                );
+                cx.notify();
+                return;
+            }
+        };
+
+        let model_id = selected_model.id.clone();
+        let provider = selected_model.provider.clone();
+        let config = self.config.clone();
+        let (tx, rx) = async_channel::bounded::<std::result::Result<std::path::PathBuf, String>>(1);
+
+        logging::log(
+            "AI_SCRIPT_GEN",
+            &format!(
+                "state=queued attempted=shift_tab_script_generation model_id={} provider={} prompt_len={}",
+                model_id,
+                provider,
+                prompt_description.len()
+            ),
+        );
+        self.show_hud("Generating script with AI...".to_string(), Some(1500), cx);
+
+        std::thread::spawn(move || {
+            logging::log(
+                "AI_SCRIPT_GEN",
+                &format!(
+                    "state=running attempted=generate_script model_id={} provider={}",
+                    model_id, provider
+                ),
+            );
+
+            let generation_result =
+                generate_script_via_ai_backend(&registry, &model_id, &prompt_description, &config)
+                    .map_err(|error| error.to_string());
+
+            if tx.send_blocking(generation_result).is_err() {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=send_result failure=result_channel_closed",
+                );
+            }
+        });
+
+        cx.spawn(async move |this, cx| {
+            let Ok(result) = rx.recv().await else {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=recv_result failure=result_channel_closed",
+                );
+                return;
+            };
+
+            let _ = cx.update(|cx| {
+                this.update(cx, |app, cx| {
+                    match result {
+                        Ok(script_path) => {
+                            let script_name = script_path
+                                .file_name()
+                                .and_then(|name| name.to_str())
+                                .unwrap_or("generated script");
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!(
+                                    "state=completed attempted=generate_script path={}",
+                                    script_path.display()
+                                ),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::success(
+                                    format!("Generated and opened {}", script_name),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(3500)),
+                            );
+                            app.close_and_reset_window(cx);
+                        }
+                        Err(error) => {
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!("state=failed attempted=generate_script error={}", error),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::error(
+                                    format!("Failed to generate script: {}", error),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(7000)),
+                            );
+                            cx.notify();
+                        }
+                    }
+                })
+            });
+        })
+        .detach();
+    }
+
     /// Rebuild the cached provider registry in a background thread.
     /// Called after config changes (API key saved, Claude Code enabled, etc.)
     pub fn rebuild_provider_registry_async(&mut self, cx: &mut Context<Self>) {
@@ -312,3 +641,44 @@
         .detach();
     }
 }
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_build_ai_script_generation_user_prompt_includes_description() {
+        let prompt = build_ai_script_generation_user_prompt("create a weather script");
+        assert!(prompt.contains("create a weather script"));
+        assert!(prompt.contains("Script Kit script"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_prefers_typescript_fence() {
+        let response = r#"
+Here's your script:
+```typescript
+// Name: Weather
+import "@scriptkit/sdk";
+```
+"#;
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert!(extracted.contains("// Name: Weather"));
+        assert!(!extracted.contains("```"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_falls_back_to_plain_text() {
+        let response = "// Name: Plain\nimport \"@scriptkit/sdk\";";
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert_eq!(extracted, "// Name: Plain\nimport \"@scriptkit/sdk\";\n");
+    }
+
+    #[test]
+    fn test_derive_script_name_from_description_uses_fallback_for_symbols() {
+        let name = derive_script_name_from_description("@@@ !!!");
+        assert_eq!(name, "ai-generated-script");
+    }
+}
diff --git a/src/app_impl/startup.rs b/src/app_impl/startup.rs
index 2e95070751083b8f4c24cd7eae8372d6bec3f6d6..441b4f23bca1b6294025c698eeb11d45815e85e0
--- a/src/app_impl/startup.rs
+++ b/src/app_impl/startup.rs
@@ -555,9 +555,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -720,20 +722,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/startup_new_tab.rs b/src/app_impl/startup_new_tab.rs
index 6c807a195553d9f168087c2ca6d771c76e555c11..7c84270cbe91c0b1ddd460aa29db8c788766d0cf
--- a/src/app_impl/startup_new_tab.rs
+++ b/src/app_impl/startup_new_tab.rs
@@ -6,9 +6,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -171,20 +173,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/tests.rs b/src/app_impl/tests.rs
index d75ba121851d7233c5ae77bc474a64b27e7c31d2..bb78d84e5a8ec9ba80c0a2287ad0c1cba93f226d
--- a/src/app_impl/tests.rs
+++ b/src/app_impl/tests.rs
@@ -1,6 +1,7 @@
 #[cfg(test)]
 mod app_impl_state_sync_tests {
     use super::{calculate_fallback_error_message, ScriptListApp};
+    use std::fs;
 
     #[test]
     fn test_sync_builtin_query_state_updates_query_and_selection_when_changed() {
@@ -46,4 +47,28 @@
         assert!(message.contains("Could not evaluate expression"));
         assert!(message.contains("Check the syntax and try again"));
     }
+
+    #[test]
+    fn test_shift_tab_routes_to_ai_script_generation_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("if has_shift")
+                && startup_tab.contains("this.generate_script_from_ai_prompt(query, cx);"),
+            "Shift+Tab in ScriptList should route to generate_script_from_ai_prompt. \
+             Missing expected branch in startup_new_tab.rs"
+        );
+    }
+
+    #[test]
+    fn test_tab_still_routes_to_inline_ai_chat_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("this.show_inline_ai_chat(Some(query), cx);"),
+            "Tab in ScriptList should continue routing to show_inline_ai_chat"
+        );
+    }
 }

file update:
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index 98a0c9cd1db5a933992c0f99cb20a3c2ca436b41..c985a5090574816267103019d1b272e1e3151a38
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -1,4 +1,194 @@
 use super::*;
+use anyhow::{anyhow, Context as AnyhowContext, Result};
+
+#[derive(Debug, Clone, Copy)]
+enum AiScriptGenerationStage {
+    SelectModel,
+    ResolveProvider,
+    RequestCompletion,
+    ExtractScript,
+    CreateScriptFile,
+    WriteScriptFile,
+    OpenEditor,
+}
+
+impl AiScriptGenerationStage {
+    fn as_str(self) -> &'static str {
+        match self {
+            Self::SelectModel => "select_model",
+            Self::ResolveProvider => "resolve_provider",
+            Self::RequestCompletion => "request_completion",
+            Self::ExtractScript => "extract_script",
+            Self::CreateScriptFile => "create_script_file",
+            Self::WriteScriptFile => "write_script_file",
+            Self::OpenEditor => "open_editor",
+        }
+    }
+}
+
+const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You are ScriptKitScriptGenerator.
+Generate a complete Script Kit TypeScript script and return code only.
+Requirements:
+- Include metadata comments at the top:
+  // Name: <script name>
+  // Description: <short summary>
+- Include: import "@scriptkit/sdk";
+- Use idiomatic Script Kit APIs (for example await arg(), await div(), await editor()) when useful.
+- Prefer clear async/await flow and practical defaults.
+- Return a full runnable script with no extra explanation."#;
+
+fn build_ai_script_generation_user_prompt(description: &str) -> String {
+    format!(
+        "Generate a complete Script Kit script for this request:\n\n{}\n\nReturn only the TypeScript script source.",
+        description.trim()
+    )
+}
+
+fn select_default_ai_script_model(
+    registry: &crate::ai::ProviderRegistry,
+) -> Option<crate::ai::ModelInfo> {
+    let all_models = registry.get_all_models();
+    all_models
+        .iter()
+        .find(|model| model.provider.eq_ignore_ascii_case("vercel"))
+        .cloned()
+        .or_else(|| all_models.first().cloned())
+}
+
+fn extract_generated_script_source(raw_response: &str) -> Option<String> {
+    let trimmed = raw_response.trim();
+    if trimmed.is_empty() {
+        return None;
+    }
+
+    // Prefer typed code fences first, then generic fences.
+    for fence in ["```typescript", "```ts", "```javascript", "```js", "```"] {
+        if let Some(content) = extract_first_fenced_block(trimmed, fence) {
+            return normalize_generated_script(content);
+        }
+    }
+
+    normalize_generated_script(trimmed.to_string())
+}
+
+fn extract_first_fenced_block(response: &str, fence_start: &str) -> Option<String> {
+    let start_index = response.find(fence_start)?;
+    let after_fence = &response[start_index + fence_start.len()..];
+    let after_newline = after_fence
+        .strip_prefix("\r\n")
+        .or_else(|| after_fence.strip_prefix('\n'))
+        .unwrap_or(after_fence);
+    let end_index = after_newline.find("```")?;
+    Some(after_newline[..end_index].trim().to_string())
+}
+
+fn normalize_generated_script(content: String) -> Option<String> {
+    let trimmed = content.trim();
+    if trimmed.is_empty() {
+        None
+    } else {
+        Some(format!("{trimmed}\n"))
+    }
+}
+
+fn derive_script_name_from_description(description: &str) -> String {
+    let mut slug = String::new();
+    let mut last_was_dash = false;
+
+    for ch in description.chars() {
+        let lower = ch.to_ascii_lowercase();
+        if lower.is_ascii_alphanumeric() {
+            slug.push(lower);
+            last_was_dash = false;
+        } else if !last_was_dash && !slug.is_empty() {
+            slug.push('-');
+            last_was_dash = true;
+        }
+
+        if slug.len() >= 48 {
+            break;
+        }
+    }
+
+    while slug.ends_with('-') {
+        slug.pop();
+    }
+
+    if slug.is_empty() {
+        "ai-generated-script".to_string()
+    } else {
+        slug
+    }
+}
+
+fn generate_script_via_ai_backend(
+    registry: &crate::ai::ProviderRegistry,
+    model_id: &str,
+    prompt_description: &str,
+    config: &crate::config::Config,
+) -> Result<std::path::PathBuf> {
+    let provider = registry
+        .find_provider_for_model(model_id)
+        .cloned()
+        .ok_or_else(|| {
+            anyhow!(
+                "state={} attempted=find_provider_for_model model_id={} failure=provider_not_found",
+                AiScriptGenerationStage::ResolveProvider.as_str(),
+                model_id
+            )
+        })?;
+
+    let request_messages = vec![
+        crate::ai::ProviderMessage::system(AI_SCRIPT_GENERATION_SYSTEM_PROMPT),
+        crate::ai::ProviderMessage::user(build_ai_script_generation_user_prompt(prompt_description)),
+    ];
+
+    let ai_response = provider
+        .send_message(&request_messages, model_id)
+        .with_context(|| {
+            format!(
+                "state={} attempted=send_message model_id={} provider={} failure=provider_call_failed",
+                AiScriptGenerationStage::RequestCompletion.as_str(),
+                model_id,
+                provider.provider_id()
+            )
+        })?;
+
+    let generated_script = extract_generated_script_source(&ai_response).ok_or_else(|| {
+        anyhow!(
+            "state={} attempted=extract_generated_script model_id={} failure=empty_script_response",
+            AiScriptGenerationStage::ExtractScript.as_str(),
+            model_id
+        )
+    })?;
+
+    let script_name = derive_script_name_from_description(prompt_description);
+    let script_path = crate::script_creation::create_new_script(&script_name).with_context(|| {
+        format!(
+            "state={} attempted=create_new_script name={} failure=create_script_failed",
+            AiScriptGenerationStage::CreateScriptFile.as_str(),
+            script_name
+        )
+    })?;
+
+    std::fs::write(&script_path, generated_script).with_context(|| {
+        format!(
+            "state={} attempted=write_script path={} failure=write_failed",
+            AiScriptGenerationStage::WriteScriptFile.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    crate::script_creation::open_in_editor(&script_path, config).with_context(|| {
+        format!(
+            "state={} attempted=open_in_editor path={} failure=open_editor_failed",
+            AiScriptGenerationStage::OpenEditor.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
 
 impl ScriptListApp {
     pub(crate) fn is_in_prompt(&self) -> bool {
@@ -274,6 +464,145 @@
         cx.notify();
     }
 
+    /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
+    /// The generated script is saved to disk and opened in the configured editor.
+    pub fn generate_script_from_ai_prompt(
+        &mut self,
+        prompt_description: String,
+        cx: &mut Context<Self>,
+    ) {
+        let prompt_description = prompt_description.trim().to_string();
+        if prompt_description.is_empty() {
+            return;
+        }
+
+        let registry = self.cached_provider_registry.clone().unwrap_or_else(|| {
+            crate::ai::ProviderRegistry::from_environment_with_config(Some(&self.config))
+        });
+
+        if !registry.has_any_provider() {
+            self.toast_manager.push(
+                components::toast::Toast::error("No AI providers configured for script generation", &self.theme)
+                    .duration_ms(Some(5000)),
+            );
+            cx.notify();
+            return;
+        }
+
+        let selected_model = match select_default_ai_script_model(&registry) {
+            Some(model) => model,
+            None => {
+                let stage = AiScriptGenerationStage::SelectModel.as_str();
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    &format!(
+                        "state={} attempted=select_default_model failure=no_available_models",
+                        stage
+                    ),
+                );
+                self.toast_manager.push(
+                    components::toast::Toast::error(
+                        "No AI models available for script generation",
+                        &self.theme,
+                    )
+                    .duration_ms(Some(5000)),
+                );
+                cx.notify();
+                return;
+            }
+        };
+
+        let model_id = selected_model.id.clone();
+        let provider = selected_model.provider.clone();
+        let config = self.config.clone();
+        let (tx, rx) = async_channel::bounded::<std::result::Result<std::path::PathBuf, String>>(1);
+
+        logging::log(
+            "AI_SCRIPT_GEN",
+            &format!(
+                "state=queued attempted=shift_tab_script_generation model_id={} provider={} prompt_len={}",
+                model_id,
+                provider,
+                prompt_description.len()
+            ),
+        );
+        self.show_hud("Generating script with AI...".to_string(), Some(1500), cx);
+
+        std::thread::spawn(move || {
+            logging::log(
+                "AI_SCRIPT_GEN",
+                &format!(
+                    "state=running attempted=generate_script model_id={} provider={}",
+                    model_id, provider
+                ),
+            );
+
+            let generation_result =
+                generate_script_via_ai_backend(&registry, &model_id, &prompt_description, &config)
+                    .map_err(|error| error.to_string());
+
+            if tx.send_blocking(generation_result).is_err() {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=send_result failure=result_channel_closed",
+                );
+            }
+        });
+
+        cx.spawn(async move |this, cx| {
+            let Ok(result) = rx.recv().await else {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=recv_result failure=result_channel_closed",
+                );
+                return;
+            };
+
+            let _ = cx.update(|cx| {
+                this.update(cx, |app, cx| {
+                    match result {
+                        Ok(script_path) => {
+                            let script_name = script_path
+                                .file_name()
+                                .and_then(|name| name.to_str())
+                                .unwrap_or("generated script");
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!(
+                                    "state=completed attempted=generate_script path={}",
+                                    script_path.display()
+                                ),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::success(
+                                    format!("Generated and opened {}", script_name),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(3500)),
+                            );
+                            app.close_and_reset_window(cx);
+                        }
+                        Err(error) => {
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!("state=failed attempted=generate_script error={}", error),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::error(
+                                    format!("Failed to generate script: {}", error),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(7000)),
+                            );
+                            cx.notify();
+                        }
+                    }
+                })
+            });
+        })
+        .detach();
+    }
+
     /// Rebuild the cached provider registry in a background thread.
     /// Called after config changes (API key saved, Claude Code enabled, etc.)
     pub fn rebuild_provider_registry_async(&mut self, cx: &mut Context<Self>) {
@@ -312,3 +641,44 @@
         .detach();
     }
 }
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_build_ai_script_generation_user_prompt_includes_description() {
+        let prompt = build_ai_script_generation_user_prompt("create a weather script");
+        assert!(prompt.contains("create a weather script"));
+        assert!(prompt.contains("Script Kit script"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_prefers_typescript_fence() {
+        let response = r#"
+Here's your script:
+```typescript
+// Name: Weather
+import "@scriptkit/sdk";
+```
+"#;
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert!(extracted.contains("// Name: Weather"));
+        assert!(!extracted.contains("```"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_falls_back_to_plain_text() {
+        let response = "// Name: Plain\nimport \"@scriptkit/sdk\";";
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert_eq!(extracted, "// Name: Plain\nimport \"@scriptkit/sdk\";\n");
+    }
+
+    #[test]
+    fn test_derive_script_name_from_description_uses_fallback_for_symbols() {
+        let name = derive_script_name_from_description("@@@ !!!");
+        assert_eq!(name, "ai-generated-script");
+    }
+}
diff --git a/src/app_impl/startup.rs b/src/app_impl/startup.rs
index 2e95070751083b8f4c24cd7eae8372d6bec3f6d6..441b4f23bca1b6294025c698eeb11d45815e85e0
--- a/src/app_impl/startup.rs
+++ b/src/app_impl/startup.rs
@@ -555,9 +555,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -720,20 +722,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/startup_new_tab.rs b/src/app_impl/startup_new_tab.rs
index 6c807a195553d9f168087c2ca6d771c76e555c11..7c84270cbe91c0b1ddd460aa29db8c788766d0cf
--- a/src/app_impl/startup_new_tab.rs
+++ b/src/app_impl/startup_new_tab.rs
@@ -6,9 +6,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -171,20 +173,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/tests.rs b/src/app_impl/tests.rs
index d75ba121851d7233c5ae77bc474a64b27e7c31d2..bb78d84e5a8ec9ba80c0a2287ad0c1cba93f226d
--- a/src/app_impl/tests.rs
+++ b/src/app_impl/tests.rs
@@ -1,6 +1,7 @@
 #[cfg(test)]
 mod app_impl_state_sync_tests {
     use super::{calculate_fallback_error_message, ScriptListApp};
+    use std::fs;
 
     #[test]
     fn test_sync_builtin_query_state_updates_query_and_selection_when_changed() {
@@ -46,4 +47,28 @@
         assert!(message.contains("Could not evaluate expression"));
         assert!(message.contains("Check the syntax and try again"));
     }
+
+    #[test]
+    fn test_shift_tab_routes_to_ai_script_generation_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("if has_shift")
+                && startup_tab.contains("this.generate_script_from_ai_prompt(query, cx);"),
+            "Shift+Tab in ScriptList should route to generate_script_from_ai_prompt. \
+             Missing expected branch in startup_new_tab.rs"
+        );
+    }
+
+    #[test]
+    fn test_tab_still_routes_to_inline_ai_chat_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("this.show_inline_ai_chat(Some(query), cx);"),
+            "Tab in ScriptList should continue routing to show_inline_ai_chat"
+        );
+    }
 }


thinking
**Running rustfmt on changed files**
exec
/bin/zsh -lc 'rustfmt src/app_impl/prompt_ai.rs src/app_impl/startup.rs src/app_impl/startup_new_tab.rs src/app_impl/tests.rs' in /Users/johnlindquist/dev/script-kit-gpui exited 1 in 51ms:
error: `async move` blocks are only allowed in Rust 2018 or later
   --> /Users/johnlindquist/dev/script-kit-gpui/src/app_impl/prompt_ai.rs:552:18
    |
552 |         cx.spawn(async move |this, cx| {
    |                  ^^^^^-^^^^
    |                       |
    |                       help: missing `,`

error: `async move` blocks are only allowed in Rust 2018 or later
   --> /Users/johnlindquist/dev/script-kit-gpui/src/app_impl/prompt_ai.rs:624:18
    |
624 |         cx.spawn(async move |this, cx| {
    |                  ^^^^^-^^^^
    |                       |
    |                       help: missing `,`

error: `async move` blocks are only allowed in Rust 2018 or later
   --> /Users/johnlindquist/dev/script-kit-gpui/src/app_impl/startup.rs:136:22
    |
136 |             cx.spawn(async move |this, cx| {
file update:
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index 98a0c9cd1db5a933992c0f99cb20a3c2ca436b41..c985a5090574816267103019d1b272e1e3151a38
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -1,4 +1,194 @@
 use super::*;
+use anyhow::{anyhow, Context as AnyhowContext, Result};
+
+#[derive(Debug, Clone, Copy)]
+enum AiScriptGenerationStage {
+    SelectModel,
+    ResolveProvider,
+    RequestCompletion,
+    ExtractScript,
+    CreateScriptFile,
+    WriteScriptFile,
+    OpenEditor,
+}
+
+impl AiScriptGenerationStage {
+    fn as_str(self) -> &'static str {
+        match self {
+            Self::SelectModel => "select_model",
+            Self::ResolveProvider => "resolve_provider",
+            Self::RequestCompletion => "request_completion",
+            Self::ExtractScript => "extract_script",
+            Self::CreateScriptFile => "create_script_file",
+            Self::WriteScriptFile => "write_script_file",
+            Self::OpenEditor => "open_editor",
+        }
+    }
+}
+
+const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You are ScriptKitScriptGenerator.
+Generate a complete Script Kit TypeScript script and return code only.
+Requirements:
+- Include metadata comments at the top:
+  // Name: <script name>
+  // Description: <short summary>
+- Include: import "@scriptkit/sdk";
+- Use idiomatic Script Kit APIs (for example await arg(), await div(), await editor()) when useful.
+- Prefer clear async/await flow and practical defaults.
+- Return a full runnable script with no extra explanation."#;
+
+fn build_ai_script_generation_user_prompt(description: &str) -> String {
+    format!(
+        "Generate a complete Script Kit script for this request:\n\n{}\n\nReturn only the TypeScript script source.",
+        description.trim()
+    )
+}
+
+fn select_default_ai_script_model(
+    registry: &crate::ai::ProviderRegistry,
+) -> Option<crate::ai::ModelInfo> {
+    let all_models = registry.get_all_models();
+    all_models
+        .iter()
+        .find(|model| model.provider.eq_ignore_ascii_case("vercel"))
+        .cloned()
+        .or_else(|| all_models.first().cloned())
+}
+
+fn extract_generated_script_source(raw_response: &str) -> Option<String> {
+    let trimmed = raw_response.trim();
+    if trimmed.is_empty() {
+        return None;
+    }
+
+    // Prefer typed code fences first, then generic fences.
+    for fence in ["```typescript", "```ts", "```javascript", "```js", "```"] {
+        if let Some(content) = extract_first_fenced_block(trimmed, fence) {
+            return normalize_generated_script(content);
+        }
+    }
+
+    normalize_generated_script(trimmed.to_string())
+}
+
+fn extract_first_fenced_block(response: &str, fence_start: &str) -> Option<String> {
+    let start_index = response.find(fence_start)?;
+    let after_fence = &response[start_index + fence_start.len()..];
+    let after_newline = after_fence
+        .strip_prefix("\r\n")
+        .or_else(|| after_fence.strip_prefix('\n'))
+        .unwrap_or(after_fence);
+    let end_index = after_newline.find("```")?;
+    Some(after_newline[..end_index].trim().to_string())
+}
+
+fn normalize_generated_script(content: String) -> Option<String> {
+    let trimmed = content.trim();
+    if trimmed.is_empty() {
+        None
+    } else {
+        Some(format!("{trimmed}\n"))
+    }
+}
+
+fn derive_script_name_from_description(description: &str) -> String {
+    let mut slug = String::new();
+    let mut last_was_dash = false;
+
+    for ch in description.chars() {
+        let lower = ch.to_ascii_lowercase();
+        if lower.is_ascii_alphanumeric() {
+            slug.push(lower);
+            last_was_dash = false;
+        } else if !last_was_dash && !slug.is_empty() {
+            slug.push('-');
+            last_was_dash = true;
+        }
+
+        if slug.len() >= 48 {
+            break;
+        }
+    }
+
+    while slug.ends_with('-') {
+        slug.pop();
+    }
+
+    if slug.is_empty() {
+        "ai-generated-script".to_string()
+    } else {
+        slug
+    }
+}
+
+fn generate_script_via_ai_backend(
+    registry: &crate::ai::ProviderRegistry,
+    model_id: &str,
+    prompt_description: &str,
+    config: &crate::config::Config,
+) -> Result<std::path::PathBuf> {
+    let provider = registry
+        .find_provider_for_model(model_id)
+        .cloned()
+        .ok_or_else(|| {
+            anyhow!(
+                "state={} attempted=find_provider_for_model model_id={} failure=provider_not_found",
+                AiScriptGenerationStage::ResolveProvider.as_str(),
+                model_id
+            )
+        })?;
+
+    let request_messages = vec![
+        crate::ai::ProviderMessage::system(AI_SCRIPT_GENERATION_SYSTEM_PROMPT),
+        crate::ai::ProviderMessage::user(build_ai_script_generation_user_prompt(prompt_description)),
+    ];
+
+    let ai_response = provider
+        .send_message(&request_messages, model_id)
+        .with_context(|| {
+            format!(
+                "state={} attempted=send_message model_id={} provider={} failure=provider_call_failed",
+                AiScriptGenerationStage::RequestCompletion.as_str(),
+                model_id,
+                provider.provider_id()
+            )
+        })?;
+
+    let generated_script = extract_generated_script_source(&ai_response).ok_or_else(|| {
+        anyhow!(
+            "state={} attempted=extract_generated_script model_id={} failure=empty_script_response",
+            AiScriptGenerationStage::ExtractScript.as_str(),
+            model_id
+        )
+    })?;
+
+    let script_name = derive_script_name_from_description(prompt_description);
+    let script_path = crate::script_creation::create_new_script(&script_name).with_context(|| {
+        format!(
+            "state={} attempted=create_new_script name={} failure=create_script_failed",
+            AiScriptGenerationStage::CreateScriptFile.as_str(),
+            script_name
+        )
+    })?;
+
+    std::fs::write(&script_path, generated_script).with_context(|| {
+        format!(
+            "state={} attempted=write_script path={} failure=write_failed",
+            AiScriptGenerationStage::WriteScriptFile.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    crate::script_creation::open_in_editor(&script_path, config).with_context(|| {
+        format!(
+            "state={} attempted=open_in_editor path={} failure=open_editor_failed",
+            AiScriptGenerationStage::OpenEditor.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
 
 impl ScriptListApp {
     pub(crate) fn is_in_prompt(&self) -> bool {
@@ -274,6 +464,145 @@
         cx.notify();
     }
 
+    /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
+    /// The generated script is saved to disk and opened in the configured editor.
+    pub fn generate_script_from_ai_prompt(
+        &mut self,
+        prompt_description: String,
+        cx: &mut Context<Self>,
+    ) {
+        let prompt_description = prompt_description.trim().to_string();
+        if prompt_description.is_empty() {
+            return;
+        }
+
+        let registry = self.cached_provider_registry.clone().unwrap_or_else(|| {
+            crate::ai::ProviderRegistry::from_environment_with_config(Some(&self.config))
+        });
+
+        if !registry.has_any_provider() {
+            self.toast_manager.push(
+                components::toast::Toast::error("No AI providers configured for script generation", &self.theme)
+                    .duration_ms(Some(5000)),
+            );
+            cx.notify();
+            return;
+        }
+
+        let selected_model = match select_default_ai_script_model(&registry) {
+            Some(model) => model,
+            None => {
+                let stage = AiScriptGenerationStage::SelectModel.as_str();
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    &format!(
+                        "state={} attempted=select_default_model failure=no_available_models",
+                        stage
+                    ),
+                );
+                self.toast_manager.push(
+                    components::toast::Toast::error(
+                        "No AI models available for script generation",
+                        &self.theme,
+                    )
+                    .duration_ms(Some(5000)),
+                );
+                cx.notify();
+                return;
+            }
+        };
+
+        let model_id = selected_model.id.clone();
+        let provider = selected_model.provider.clone();
+        let config = self.config.clone();
+        let (tx, rx) = async_channel::bounded::<std::result::Result<std::path::PathBuf, String>>(1);
+
+        logging::log(
+            "AI_SCRIPT_GEN",
+            &format!(
+                "state=queued attempted=shift_tab_script_generation model_id={} provider={} prompt_len={}",
+                model_id,
+                provider,
+                prompt_description.len()
+            ),
+        );
+        self.show_hud("Generating script with AI...".to_string(), Some(1500), cx);
+
+        std::thread::spawn(move || {
+            logging::log(
+                "AI_SCRIPT_GEN",
+                &format!(
+                    "state=running attempted=generate_script model_id={} provider={}",
+                    model_id, provider
+                ),
+            );
+
+            let generation_result =
+                generate_script_via_ai_backend(&registry, &model_id, &prompt_description, &config)
+                    .map_err(|error| error.to_string());
+
+            if tx.send_blocking(generation_result).is_err() {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=send_result failure=result_channel_closed",
+                );
+            }
+        });
+
+        cx.spawn(async move |this, cx| {
+            let Ok(result) = rx.recv().await else {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=recv_result failure=result_channel_closed",
+                );
+                return;
+            };
+
+            let _ = cx.update(|cx| {
+                this.update(cx, |app, cx| {
+                    match result {
+                        Ok(script_path) => {
+                            let script_name = script_path
+                                .file_name()
+                                .and_then(|name| name.to_str())
+                                .unwrap_or("generated script");
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!(
+                                    "state=completed attempted=generate_script path={}",
+                                    script_path.display()
+                                ),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::success(
+                                    format!("Generated and opened {}", script_name),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(3500)),
+                            );
+                            app.close_and_reset_window(cx);
+                        }
+                        Err(error) => {
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!("state=failed attempted=generate_script error={}", error),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::error(
+                                    format!("Failed to generate script: {}", error),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(7000)),
+                            );
+                            cx.notify();
+                        }
+                    }
+                })
+            });
+        })
+        .detach();
+    }
+
     /// Rebuild the cached provider registry in a background thread.
     /// Called after config changes (API key saved, Claude Code enabled, etc.)
     pub fn rebuild_provider_registry_async(&mut self, cx: &mut Context<Self>) {
@@ -312,3 +641,44 @@
         .detach();
     }
 }
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_build_ai_script_generation_user_prompt_includes_description() {
+        let prompt = build_ai_script_generation_user_prompt("create a weather script");
+        assert!(prompt.contains("create a weather script"));
+        assert!(prompt.contains("Script Kit script"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_prefers_typescript_fence() {
+        let response = r#"
+Here's your script:
+```typescript
+// Name: Weather
+import "@scriptkit/sdk";
+```
+"#;
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert!(extracted.contains("// Name: Weather"));
+        assert!(!extracted.contains("```"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_falls_back_to_plain_text() {
+        let response = "// Name: Plain\nimport \"@scriptkit/sdk\";";
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert_eq!(extracted, "// Name: Plain\nimport \"@scriptkit/sdk\";\n");
+    }
+
+    #[test]
+    fn test_derive_script_name_from_description_uses_fallback_for_symbols() {
+        let name = derive_script_name_from_description("@@@ !!!");
+        assert_eq!(name, "ai-generated-script");
+    }
+}
diff --git a/src/app_impl/startup.rs b/src/app_impl/startup.rs
index 2e95070751083b8f4c24cd7eae8372d6bec3f6d6..441b4f23bca1b6294025c698eeb11d45815e85e0
--- a/src/app_impl/startup.rs
+++ b/src/app_impl/startup.rs
@@ -555,9 +555,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -720,20 +722,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/startup_new_tab.rs b/src/app_impl/startup_new_tab.rs
index 6c807a195553d9f168087c2ca6d771c76e555c11..7c84270cbe91c0b1ddd460aa29db8c788766d0cf
--- a/src/app_impl/startup_new_tab.rs
+++ b/src/app_impl/startup_new_tab.rs
@@ -6,9 +6,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -171,20 +173,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/tests.rs b/src/app_impl/tests.rs
index d75ba121851d7233c5ae77bc474a64b27e7c31d2..bb78d84e5a8ec9ba80c0a2287ad0c1cba93f226d
--- a/src/app_impl/tests.rs
+++ b/src/app_impl/tests.rs
@@ -1,6 +1,7 @@
 #[cfg(test)]
 mod app_impl_state_sync_tests {
     use super::{calculate_fallback_error_message, ScriptListApp};
+    use std::fs;
 
     #[test]
     fn test_sync_builtin_query_state_updates_query_and_selection_when_changed() {
@@ -46,4 +47,28 @@
         assert!(message.contains("Could not evaluate expression"));
         assert!(message.contains("Check the syntax and try again"));
     }
+
+    #[test]
+    fn test_shift_tab_routes_to_ai_script_generation_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("if has_shift")
+                && startup_tab.contains("this.generate_script_from_ai_prompt(query, cx);"),
+            "Shift+Tab in ScriptList should route to generate_script_from_ai_prompt. \
+             Missing expected branch in startup_new_tab.rs"
+        );
+    }
+
+    #[test]
+    fn test_tab_still_routes_to_inline_ai_chat_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("this.show_inline_ai_chat(Some(query), cx);"),
+            "Tab in ScriptList should continue routing to show_inline_ai_chat"
+        );
+    }
 }


thinking
**Planning targeted Rust tests**
codex
The code changes are in place; I’m now running targeted Rust tests for the new script-generation helpers and the new `Shift+Tab` routing assertion to verify behavior without running the full workspace suite.
exec
/bin/zsh -lc 'cargo test test_build_ai_script_generation_user_prompt_includes_description -- --nocapture' in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc 'cargo test test_shift_tab_routes_to_ai_script_generation_in_script_list_tab_interceptor -- --nocapture' in /Users/johnlindquist/dev/script-kit-gpui exited 101 in 20.41s:
    Blocking waiting for file lock on package cache
    Blocking waiting for file lock on package cache
    Blocking waiting for file lock on package cache
   Compiling script-kit-gpui v0.1.0 (/Users/johnlindquist/dev/script-kit-gpui)
error[E0761]: file for module `tests` found at both "src/protocol/message/tests.rs" and "src/protocol/message/tests/mod.rs"
  --> src/protocol/message/mod.rs:87:1
   |
87 | mod tests;
   | ^^^^^^^^^^
   |
   = help: delete or rename one of them to remove the ambiguity

error[E0583]: file not found for module `script_generation`
  --> src/ai/mod.rs:40:1
   |
40 | pub(crate) mod script_generation;
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   |
   = help: to create the module `script_generation`, create file "src/ai/script_generation.rs" or "src/ai/script_generation/mod.rs"
   = note: if there is a `mod script_generation` elsewhere in the crate already, import it with `use crate::...` instead
file update:
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index 98a0c9cd1db5a933992c0f99cb20a3c2ca436b41..c985a5090574816267103019d1b272e1e3151a38
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -1,4 +1,194 @@
 use super::*;
+use anyhow::{anyhow, Context as AnyhowContext, Result};
+
+#[derive(Debug, Clone, Copy)]
+enum AiScriptGenerationStage {
+    SelectModel,
+    ResolveProvider,
+    RequestCompletion,
+    ExtractScript,
+    CreateScriptFile,
+    WriteScriptFile,
+    OpenEditor,
+}
+
+impl AiScriptGenerationStage {
+    fn as_str(self) -> &'static str {
+        match self {
+            Self::SelectModel => "select_model",
+            Self::ResolveProvider => "resolve_provider",
+            Self::RequestCompletion => "request_completion",
+            Self::ExtractScript => "extract_script",
+            Self::CreateScriptFile => "create_script_file",
+            Self::WriteScriptFile => "write_script_file",
+            Self::OpenEditor => "open_editor",
+        }
+    }
+}
+
+const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You are ScriptKitScriptGenerator.
+Generate a complete Script Kit TypeScript script and return code only.
+Requirements:
+- Include metadata comments at the top:
+  // Name: <script name>
+  // Description: <short summary>
+- Include: import "@scriptkit/sdk";
+- Use idiomatic Script Kit APIs (for example await arg(), await div(), await editor()) when useful.
+- Prefer clear async/await flow and practical defaults.
+- Return a full runnable script with no extra explanation."#;
+
+fn build_ai_script_generation_user_prompt(description: &str) -> String {
+    format!(
+        "Generate a complete Script Kit script for this request:\n\n{}\n\nReturn only the TypeScript script source.",
+        description.trim()
+    )
+}
+
+fn select_default_ai_script_model(
+    registry: &crate::ai::ProviderRegistry,
+) -> Option<crate::ai::ModelInfo> {
+    let all_models = registry.get_all_models();
+    all_models
+        .iter()
+        .find(|model| model.provider.eq_ignore_ascii_case("vercel"))
+        .cloned()
+        .or_else(|| all_models.first().cloned())
+}
+
+fn extract_generated_script_source(raw_response: &str) -> Option<String> {
+    let trimmed = raw_response.trim();
+    if trimmed.is_empty() {
+        return None;
+    }
+
+    // Prefer typed code fences first, then generic fences.
+    for fence in ["```typescript", "```ts", "```javascript", "```js", "```"] {
+        if let Some(content) = extract_first_fenced_block(trimmed, fence) {
+            return normalize_generated_script(content);
+        }
+    }
+
+    normalize_generated_script(trimmed.to_string())
+}
+
+fn extract_first_fenced_block(response: &str, fence_start: &str) -> Option<String> {
+    let start_index = response.find(fence_start)?;
+    let after_fence = &response[start_index + fence_start.len()..];
+    let after_newline = after_fence
+        .strip_prefix("\r\n")
+        .or_else(|| after_fence.strip_prefix('\n'))
+        .unwrap_or(after_fence);
+    let end_index = after_newline.find("```")?;
+    Some(after_newline[..end_index].trim().to_string())
+}
+
+fn normalize_generated_script(content: String) -> Option<String> {
+    let trimmed = content.trim();
+    if trimmed.is_empty() {
+        None
+    } else {
+        Some(format!("{trimmed}\n"))
+    }
+}
+
+fn derive_script_name_from_description(description: &str) -> String {
+    let mut slug = String::new();
+    let mut last_was_dash = false;
+
+    for ch in description.chars() {
+        let lower = ch.to_ascii_lowercase();
+        if lower.is_ascii_alphanumeric() {
+            slug.push(lower);
+            last_was_dash = false;
+        } else if !last_was_dash && !slug.is_empty() {
+            slug.push('-');
+            last_was_dash = true;
+        }
+
+        if slug.len() >= 48 {
+            break;
+        }
+    }
+
+    while slug.ends_with('-') {
+        slug.pop();
+    }
+
+    if slug.is_empty() {
+        "ai-generated-script".to_string()
+    } else {
+        slug
+    }
+}
+
+fn generate_script_via_ai_backend(
+    registry: &crate::ai::ProviderRegistry,
+    model_id: &str,
+    prompt_description: &str,
+    config: &crate::config::Config,
+) -> Result<std::path::PathBuf> {
+    let provider = registry
+        .find_provider_for_model(model_id)
+        .cloned()
+        .ok_or_else(|| {
+            anyhow!(
+                "state={} attempted=find_provider_for_model model_id={} failure=provider_not_found",
+                AiScriptGenerationStage::ResolveProvider.as_str(),
+                model_id
+            )
+        })?;
+
+    let request_messages = vec![
+        crate::ai::ProviderMessage::system(AI_SCRIPT_GENERATION_SYSTEM_PROMPT),
+        crate::ai::ProviderMessage::user(build_ai_script_generation_user_prompt(prompt_description)),
+    ];
+
+    let ai_response = provider
+        .send_message(&request_messages, model_id)
+        .with_context(|| {
+            format!(
+                "state={} attempted=send_message model_id={} provider={} failure=provider_call_failed",
+                AiScriptGenerationStage::RequestCompletion.as_str(),
+                model_id,
+                provider.provider_id()
+            )
+        })?;
+
+    let generated_script = extract_generated_script_source(&ai_response).ok_or_else(|| {
+        anyhow!(
+            "state={} attempted=extract_generated_script model_id={} failure=empty_script_response",
+            AiScriptGenerationStage::ExtractScript.as_str(),
+            model_id
+        )
+    })?;
+
+    let script_name = derive_script_name_from_description(prompt_description);
+    let script_path = crate::script_creation::create_new_script(&script_name).with_context(|| {
+        format!(
+            "state={} attempted=create_new_script name={} failure=create_script_failed",
+            AiScriptGenerationStage::CreateScriptFile.as_str(),
+            script_name
+        )
+    })?;
+
+    std::fs::write(&script_path, generated_script).with_context(|| {
+        format!(
+            "state={} attempted=write_script path={} failure=write_failed",
+            AiScriptGenerationStage::WriteScriptFile.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    crate::script_creation::open_in_editor(&script_path, config).with_context(|| {
+        format!(
+            "state={} attempted=open_in_editor path={} failure=open_editor_failed",
+            AiScriptGenerationStage::OpenEditor.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
 
 impl ScriptListApp {
     pub(crate) fn is_in_prompt(&self) -> bool {
@@ -274,6 +464,145 @@
         cx.notify();
     }
 
+    /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
+    /// The generated script is saved to disk and opened in the configured editor.
+    pub fn generate_script_from_ai_prompt(
+        &mut self,
+        prompt_description: String,
+        cx: &mut Context<Self>,
+    ) {
+        let prompt_description = prompt_description.trim().to_string();
+        if prompt_description.is_empty() {
+            return;
+        }
+
+        let registry = self.cached_provider_registry.clone().unwrap_or_else(|| {
+            crate::ai::ProviderRegistry::from_environment_with_config(Some(&self.config))
+        });
+
+        if !registry.has_any_provider() {
+            self.toast_manager.push(
+                components::toast::Toast::error("No AI providers configured for script generation", &self.theme)
+                    .duration_ms(Some(5000)),
+            );
+            cx.notify();
+            return;
+        }
+
+        let selected_model = match select_default_ai_script_model(&registry) {
+            Some(model) => model,
+            None => {
+                let stage = AiScriptGenerationStage::SelectModel.as_str();
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    &format!(
+                        "state={} attempted=select_default_model failure=no_available_models",
+                        stage
+                    ),
+                );
+                self.toast_manager.push(
+                    components::toast::Toast::error(
+                        "No AI models available for script generation",
+                        &self.theme,
+                    )
+                    .duration_ms(Some(5000)),
+                );
+                cx.notify();
+                return;
+            }
+        };
+
+        let model_id = selected_model.id.clone();
+        let provider = selected_model.provider.clone();
+        let config = self.config.clone();
+        let (tx, rx) = async_channel::bounded::<std::result::Result<std::path::PathBuf, String>>(1);
+
+        logging::log(
+            "AI_SCRIPT_GEN",
+            &format!(
+                "state=queued attempted=shift_tab_script_generation model_id={} provider={} prompt_len={}",
+                model_id,
+                provider,
+                prompt_description.len()
+            ),
+        );
+        self.show_hud("Generating script with AI...".to_string(), Some(1500), cx);
+
+        std::thread::spawn(move || {
+            logging::log(
+                "AI_SCRIPT_GEN",
+                &format!(
+                    "state=running attempted=generate_script model_id={} provider={}",
+                    model_id, provider
+                ),
+            );
+
+            let generation_result =
+                generate_script_via_ai_backend(&registry, &model_id, &prompt_description, &config)
+                    .map_err(|error| error.to_string());
+
+            if tx.send_blocking(generation_result).is_err() {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=send_result failure=result_channel_closed",
+                );
+            }
+        });
+
+        cx.spawn(async move |this, cx| {
+            let Ok(result) = rx.recv().await else {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=recv_result failure=result_channel_closed",
+                );
+                return;
+            };
+
+            let _ = cx.update(|cx| {
+                this.update(cx, |app, cx| {
+                    match result {
+                        Ok(script_path) => {
+                            let script_name = script_path
+                                .file_name()
+                                .and_then(|name| name.to_str())
+                                .unwrap_or("generated script");
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!(
+                                    "state=completed attempted=generate_script path={}",
+                                    script_path.display()
+                                ),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::success(
+                                    format!("Generated and opened {}", script_name),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(3500)),
+                            );
+                            app.close_and_reset_window(cx);
+                        }
+                        Err(error) => {
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!("state=failed attempted=generate_script error={}", error),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::error(
+                                    format!("Failed to generate script: {}", error),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(7000)),
+                            );
+                            cx.notify();
+                        }
+                    }
+                })
+            });
+        })
+        .detach();
+    }
+
     /// Rebuild the cached provider registry in a background thread.
     /// Called after config changes (API key saved, Claude Code enabled, etc.)
     pub fn rebuild_provider_registry_async(&mut self, cx: &mut Context<Self>) {
@@ -312,3 +641,44 @@
         .detach();
     }
 }
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_build_ai_script_generation_user_prompt_includes_description() {
+        let prompt = build_ai_script_generation_user_prompt("create a weather script");
+        assert!(prompt.contains("create a weather script"));
+        assert!(prompt.contains("Script Kit script"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_prefers_typescript_fence() {
+        let response = r#"
+Here's your script:
+```typescript
+// Name: Weather
+import "@scriptkit/sdk";
+```
+"#;
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert!(extracted.contains("// Name: Weather"));
+        assert!(!extracted.contains("```"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_falls_back_to_plain_text() {
+        let response = "// Name: Plain\nimport \"@scriptkit/sdk\";";
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert_eq!(extracted, "// Name: Plain\nimport \"@scriptkit/sdk\";\n");
+    }
+
+    #[test]
+    fn test_derive_script_name_from_description_uses_fallback_for_symbols() {
+        let name = derive_script_name_from_description("@@@ !!!");
+        assert_eq!(name, "ai-generated-script");
+    }
+}
diff --git a/src/app_impl/startup.rs b/src/app_impl/startup.rs
index 2e95070751083b8f4c24cd7eae8372d6bec3f6d6..441b4f23bca1b6294025c698eeb11d45815e85e0
--- a/src/app_impl/startup.rs
+++ b/src/app_impl/startup.rs
@@ -555,9 +555,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -720,20 +722,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/startup_new_tab.rs b/src/app_impl/startup_new_tab.rs
index 6c807a195553d9f168087c2ca6d771c76e555c11..7c84270cbe91c0b1ddd460aa29db8c788766d0cf
--- a/src/app_impl/startup_new_tab.rs
+++ b/src/app_impl/startup_new_tab.rs
@@ -6,9 +6,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -171,20 +173,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/tests.rs b/src/app_impl/tests.rs
index d75ba121851d7233c5ae77bc474a64b27e7c31d2..bb78d84e5a8ec9ba80c0a2287ad0c1cba93f226d
--- a/src/app_impl/tests.rs
+++ b/src/app_impl/tests.rs
@@ -1,6 +1,7 @@
 #[cfg(test)]
 mod app_impl_state_sync_tests {
     use super::{calculate_fallback_error_message, ScriptListApp};
+    use std::fs;
 
     #[test]
     fn test_sync_builtin_query_state_updates_query_and_selection_when_changed() {
@@ -46,4 +47,28 @@
         assert!(message.contains("Could not evaluate expression"));
         assert!(message.contains("Check the syntax and try again"));
     }
+
+    #[test]
+    fn test_shift_tab_routes_to_ai_script_generation_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("if has_shift")
+                && startup_tab.contains("this.generate_script_from_ai_prompt(query, cx);"),
+            "Shift+Tab in ScriptList should route to generate_script_from_ai_prompt. \
+             Missing expected branch in startup_new_tab.rs"
+        );
+    }
+
+    #[test]
+    fn test_tab_still_routes_to_inline_ai_chat_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("this.show_inline_ai_chat(Some(query), cx);"),
+            "Tab in ScriptList should continue routing to show_inline_ai_chat"
+        );
+    }
 }

 exited 101 in 39.47s:
    Blocking waiting for file lock on package cache
    Blocking waiting for file lock on package cache
    Blocking waiting for file lock on build directory
   Compiling script-kit-gpui v0.1.0 (/Users/johnlindquist/dev/script-kit-gpui)
error[E0761]: file for module `tests` found at both "src/protocol/message/tests.rs" and "src/protocol/message/tests/mod.rs"
  --> src/protocol/message/mod.rs:87:1
   |
87 | mod tests;
   | ^^^^^^^^^^
   |
   = help: delete or rename one of them to remove the ambiguity

error[E0583]: file not found for module `script_generation`
  --> src/ai/mod.rs:40:1
   |
40 | pub(crate) mod script_generation;
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   |
   = help: to create the module `script_generation`, create file "src/ai/script_generation.rs" or "src/ai/script_generation/mod.rs"
   = note: if there is a `mod script_generation` elsewhere in the crate already, import it with `use crate::...` instead
context compacted
Reconnecting... 1/5 (stream disconnected before completion: websocket closed by server before response.completed)

thinking
**Planning scoped verification steps**
codex
I’m picking up from the current branch state and will first verify the claimed files and diff, then run scoped verification for the new `Shift+Tab` path and adjust anything needed before finalizing.
exec
/bin/zsh -lc '~/.claude/bin/swarm.ts status && git status --short' in /Users/johnlindquist/dev/script-kit-gpui succeeded in 50ms:
Repo: /Users/johnlindquist/dev/script-kit-gpui
Updated: 2026-02-08T02:52:55.298Z

Active agents (2):
codex-script-gen-builtin [in_progress] codex/gpt-5.3-codex/exec
  task: In Script Kit GPUI, there needs to be a built-in feature or action for AI script generation triggered by Shift+Tab. Loo…
  scope: src/builtins/**,src/app_execute/**,src/ai/**,src/kit_store/**
  claims(6): src/builtins/part_001_entries/entries_002.rs, src/builtins/part_000.rs, src/builtins/tests/part_000.rs, src/app_execute/builtin_execution.rs, src/ai/script_gen…
  notes: Claimed builtin/app_execute/ai files; implementing AI script generation command + tests

codex-shift-tab-keybind [in_progress] codex/gpt-5.3-codex/exec
  task: In Script Kit GPUI, pressing Tab in the main prompt triggers an AI chat scenario. Add a Shift+Tab keyboard shortcut tha…
  scope: src/actions/dialog/**,src/app_impl/**,src/ai/**,src/execute_script/**
  claims(4): src/app_impl/startup.rs, src/app_impl/prompt_ai.rs, src/app_impl/startup_new_tab.rs, src/app_impl/tests.rs
  notes: Implementing Shift+Tab AI script generation in startup tab interceptor + prompt_ai backend action.

Inactive agents (253):
- codex-accessibility [done] done. exit=0. final=.ai/logs/codex-accessibility.final.md. See final message file.
- codex-action-labels [failed] failed. exit=1. final=.ai/logs/codex-action-labels.final.md.
- codex-actions-audit [failed] failed. exit=1. final=.ai/logs/codex-actions-audit.final.md. See final message file.
file update:
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index 98a0c9cd1db5a933992c0f99cb20a3c2ca436b41..c985a5090574816267103019d1b272e1e3151a38
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -1,4 +1,194 @@
 use super::*;
+use anyhow::{anyhow, Context as AnyhowContext, Result};
+
+#[derive(Debug, Clone, Copy)]
+enum AiScriptGenerationStage {
+    SelectModel,
+    ResolveProvider,
+    RequestCompletion,
+    ExtractScript,
+    CreateScriptFile,
+    WriteScriptFile,
+    OpenEditor,
+}
+
+impl AiScriptGenerationStage {
+    fn as_str(self) -> &'static str {
+        match self {
+            Self::SelectModel => "select_model",
+            Self::ResolveProvider => "resolve_provider",
+            Self::RequestCompletion => "request_completion",
+            Self::ExtractScript => "extract_script",
+            Self::CreateScriptFile => "create_script_file",
+            Self::WriteScriptFile => "write_script_file",
+            Self::OpenEditor => "open_editor",
+        }
+    }
+}
+
+const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You are ScriptKitScriptGenerator.
+Generate a complete Script Kit TypeScript script and return code only.
+Requirements:
+- Include metadata comments at the top:
+  // Name: <script name>
+  // Description: <short summary>
+- Include: import "@scriptkit/sdk";
+- Use idiomatic Script Kit APIs (for example await arg(), await div(), await editor()) when useful.
+- Prefer clear async/await flow and practical defaults.
+- Return a full runnable script with no extra explanation."#;
+
+fn build_ai_script_generation_user_prompt(description: &str) -> String {
+    format!(
+        "Generate a complete Script Kit script for this request:\n\n{}\n\nReturn only the TypeScript script source.",
+        description.trim()
+    )
+}
+
+fn select_default_ai_script_model(
+    registry: &crate::ai::ProviderRegistry,
+) -> Option<crate::ai::ModelInfo> {
+    let all_models = registry.get_all_models();
+    all_models
+        .iter()
+        .find(|model| model.provider.eq_ignore_ascii_case("vercel"))
+        .cloned()
+        .or_else(|| all_models.first().cloned())
+}
+
+fn extract_generated_script_source(raw_response: &str) -> Option<String> {
+    let trimmed = raw_response.trim();
+    if trimmed.is_empty() {
+        return None;
+    }
+
+    // Prefer typed code fences first, then generic fences.
+    for fence in ["```typescript", "```ts", "```javascript", "```js", "```"] {
+        if let Some(content) = extract_first_fenced_block(trimmed, fence) {
+            return normalize_generated_script(content);
+        }
+    }
+
+    normalize_generated_script(trimmed.to_string())
+}
+
+fn extract_first_fenced_block(response: &str, fence_start: &str) -> Option<String> {
+    let start_index = response.find(fence_start)?;
+    let after_fence = &response[start_index + fence_start.len()..];
+    let after_newline = after_fence
+        .strip_prefix("\r\n")
+        .or_else(|| after_fence.strip_prefix('\n'))
+        .unwrap_or(after_fence);
+    let end_index = after_newline.find("```")?;
+    Some(after_newline[..end_index].trim().to_string())
+}
+
+fn normalize_generated_script(content: String) -> Option<String> {
+    let trimmed = content.trim();
+    if trimmed.is_empty() {
+        None
+    } else {
+        Some(format!("{trimmed}\n"))
+    }
+}
+
+fn derive_script_name_from_description(description: &str) -> String {
+    let mut slug = String::new();
+    let mut last_was_dash = false;
+
+    for ch in description.chars() {
+        let lower = ch.to_ascii_lowercase();
+        if lower.is_ascii_alphanumeric() {
+            slug.push(lower);
+            last_was_dash = false;
+        } else if !last_was_dash && !slug.is_empty() {
+            slug.push('-');
+            last_was_dash = true;
+        }
+
+        if slug.len() >= 48 {
+            break;
+        }
+    }
+
+    while slug.ends_with('-') {
+        slug.pop();
+    }
+
+    if slug.is_empty() {
+        "ai-generated-script".to_string()
+    } else {
+        slug
+    }
+}
+
+fn generate_script_via_ai_backend(
+    registry: &crate::ai::ProviderRegistry,
+    model_id: &str,
+    prompt_description: &str,
+    config: &crate::config::Config,
+) -> Result<std::path::PathBuf> {
+    let provider = registry
+        .find_provider_for_model(model_id)
+        .cloned()
+        .ok_or_else(|| {
+            anyhow!(
+                "state={} attempted=find_provider_for_model model_id={} failure=provider_not_found",
+                AiScriptGenerationStage::ResolveProvider.as_str(),
+                model_id
+            )
+        })?;
+
+    let request_messages = vec![
+        crate::ai::ProviderMessage::system(AI_SCRIPT_GENERATION_SYSTEM_PROMPT),
+        crate::ai::ProviderMessage::user(build_ai_script_generation_user_prompt(prompt_description)),
+    ];
+
+    let ai_response = provider
+        .send_message(&request_messages, model_id)
+        .with_context(|| {
+            format!(
+                "state={} attempted=send_message model_id={} provider={} failure=provider_call_failed",
+                AiScriptGenerationStage::RequestCompletion.as_str(),
+                model_id,
+                provider.provider_id()
+            )
+        })?;
+
+    let generated_script = extract_generated_script_source(&ai_response).ok_or_else(|| {
+        anyhow!(
+            "state={} attempted=extract_generated_script model_id={} failure=empty_script_response",
+            AiScriptGenerationStage::ExtractScript.as_str(),
+            model_id
+        )
+    })?;
+
+    let script_name = derive_script_name_from_description(prompt_description);
+    let script_path = crate::script_creation::create_new_script(&script_name).with_context(|| {
+        format!(
+            "state={} attempted=create_new_script name={} failure=create_script_failed",
+            AiScriptGenerationStage::CreateScriptFile.as_str(),
+            script_name
+        )
+    })?;
+
+    std::fs::write(&script_path, generated_script).with_context(|| {
+        format!(
+            "state={} attempted=write_script path={} failure=write_failed",
+            AiScriptGenerationStage::WriteScriptFile.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    crate::script_creation::open_in_editor(&script_path, config).with_context(|| {
+        format!(
+            "state={} attempted=open_in_editor path={} failure=open_editor_failed",
+            AiScriptGenerationStage::OpenEditor.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
 
 impl ScriptListApp {
     pub(crate) fn is_in_prompt(&self) -> bool {
@@ -274,6 +464,145 @@
         cx.notify();
     }
 
+    /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
+    /// The generated script is saved to disk and opened in the configured editor.
+    pub fn generate_script_from_ai_prompt(
+        &mut self,
+        prompt_description: String,
+        cx: &mut Context<Self>,
+    ) {
+        let prompt_description = prompt_description.trim().to_string();
+        if prompt_description.is_empty() {
+            return;
+        }
+
+        let registry = self.cached_provider_registry.clone().unwrap_or_else(|| {
+            crate::ai::ProviderRegistry::from_environment_with_config(Some(&self.config))
+        });
+
+        if !registry.has_any_provider() {
+            self.toast_manager.push(
+                components::toast::Toast::error("No AI providers configured for script generation", &self.theme)
+                    .duration_ms(Some(5000)),
+            );
+            cx.notify();
+            return;
+        }
+
+        let selected_model = match select_default_ai_script_model(&registry) {
+            Some(model) => model,
+            None => {
+                let stage = AiScriptGenerationStage::SelectModel.as_str();
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    &format!(
+                        "state={} attempted=select_default_model failure=no_available_models",
+                        stage
+                    ),
+                );
+                self.toast_manager.push(
+                    components::toast::Toast::error(
+                        "No AI models available for script generation",
+                        &self.theme,
+                    )
+                    .duration_ms(Some(5000)),
+                );
+                cx.notify();
+                return;
+            }
+        };
+
+        let model_id = selected_model.id.clone();
+        let provider = selected_model.provider.clone();
+        let config = self.config.clone();
+        let (tx, rx) = async_channel::bounded::<std::result::Result<std::path::PathBuf, String>>(1);
+
+        logging::log(
+            "AI_SCRIPT_GEN",
+            &format!(
+                "state=queued attempted=shift_tab_script_generation model_id={} provider={} prompt_len={}",
+                model_id,
+                provider,
+                prompt_description.len()
+            ),
+        );
+        self.show_hud("Generating script with AI...".to_string(), Some(1500), cx);
+
+        std::thread::spawn(move || {
+            logging::log(
+                "AI_SCRIPT_GEN",
+                &format!(
+                    "state=running attempted=generate_script model_id={} provider={}",
+                    model_id, provider
+                ),
+            );
+
+            let generation_result =
+                generate_script_via_ai_backend(&registry, &model_id, &prompt_description, &config)
+                    .map_err(|error| error.to_string());
+
+            if tx.send_blocking(generation_result).is_err() {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=send_result failure=result_channel_closed",
+                );
+            }
+        });
+
+        cx.spawn(async move |this, cx| {
+            let Ok(result) = rx.recv().await else {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=recv_result failure=result_channel_closed",
+                );
+                return;
+            };
+
+            let _ = cx.update(|cx| {
+                this.update(cx, |app, cx| {
+                    match result {
+                        Ok(script_path) => {
+                            let script_name = script_path
+                                .file_name()
+                                .and_then(|name| name.to_str())
+                                .unwrap_or("generated script");
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!(
+                                    "state=completed attempted=generate_script path={}",
+                                    script_path.display()
+                                ),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::success(
+                                    format!("Generated and opened {}", script_name),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(3500)),
+                            );
+                            app.close_and_reset_window(cx);
+                        }
+                        Err(error) => {
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!("state=failed attempted=generate_script error={}", error),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::error(
+                                    format!("Failed to generate script: {}", error),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(7000)),
+                            );
+                            cx.notify();
+                        }
+                    }
+                })
+            });
+        })
+        .detach();
+    }
+
     /// Rebuild the cached provider registry in a background thread.
     /// Called after config changes (API key saved, Claude Code enabled, etc.)
     pub fn rebuild_provider_registry_async(&mut self, cx: &mut Context<Self>) {
@@ -312,3 +641,44 @@
         .detach();
     }
 }
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_build_ai_script_generation_user_prompt_includes_description() {
+        let prompt = build_ai_script_generation_user_prompt("create a weather script");
+        assert!(prompt.contains("create a weather script"));
+        assert!(prompt.contains("Script Kit script"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_prefers_typescript_fence() {
+        let response = r#"
+Here's your script:
+```typescript
+// Name: Weather
+import "@scriptkit/sdk";
+```
+"#;
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert!(extracted.contains("// Name: Weather"));
+        assert!(!extracted.contains("```"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_falls_back_to_plain_text() {
+        let response = "// Name: Plain\nimport \"@scriptkit/sdk\";";
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert_eq!(extracted, "// Name: Plain\nimport \"@scriptkit/sdk\";\n");
+    }
+
+    #[test]
+    fn test_derive_script_name_from_description_uses_fallback_for_symbols() {
+        let name = derive_script_name_from_description("@@@ !!!");
+        assert_eq!(name, "ai-generated-script");
+    }
+}
diff --git a/src/app_impl/startup.rs b/src/app_impl/startup.rs
index 2e95070751083b8f4c24cd7eae8372d6bec3f6d6..441b4f23bca1b6294025c698eeb11d45815e85e0
--- a/src/app_impl/startup.rs
+++ b/src/app_impl/startup.rs
@@ -555,9 +555,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -720,20 +722,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/startup_new_tab.rs b/src/app_impl/startup_new_tab.rs
index 6c807a195553d9f168087c2ca6d771c76e555c11..7c84270cbe91c0b1ddd460aa29db8c788766d0cf
--- a/src/app_impl/startup_new_tab.rs
+++ b/src/app_impl/startup_new_tab.rs
@@ -6,9 +6,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -171,20 +173,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/tests.rs b/src/app_impl/tests.rs
index d75ba121851d7233c5ae77bc474a64b27e7c31d2..bb78d84e5a8ec9ba80c0a2287ad0c1cba93f226d
--- a/src/app_impl/tests.rs
+++ b/src/app_impl/tests.rs
@@ -1,6 +1,7 @@
 #[cfg(test)]
 mod app_impl_state_sync_tests {
     use super::{calculate_fallback_error_message, ScriptListApp};
+    use std::fs;
 
     #[test]
     fn test_sync_builtin_query_state_updates_query_and_selection_when_changed() {
@@ -46,4 +47,28 @@
         assert!(message.contains("Could not evaluate expression"));
         assert!(message.contains("Check the syntax and try again"));
     }
+
+    #[test]
+    fn test_shift_tab_routes_to_ai_script_generation_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("if has_shift")
+                && startup_tab.contains("this.generate_script_from_ai_prompt(query, cx);"),
+            "Shift+Tab in ScriptList should route to generate_script_from_ai_prompt. \
+             Missing expected branch in startup_new_tab.rs"
+        );
+    }
+
+    #[test]
+    fn test_tab_still_routes_to_inline_ai_chat_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("this.show_inline_ai_chat(Some(query), cx);"),
+            "Tab in ScriptList should continue routing to show_inline_ai_chat"
+        );
+    }
 }


thinking
**Verifying claimed file changes**
codex
I confirmed your assigned files are still claimed and there are many unrelated workspace changes from other agents, so I’ll only validate and adjust the four files in your scope and ignore unrelated failures.
exec
/bin/zsh -lc 'git diff -- src/app_impl/startup.rs' in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc 'git diff -- src/app_impl/prompt_ai.rs' in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc 'git diff -- src/app_impl/tests.rs' in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc 'git diff -- src/app_impl/startup_new_tab.rs' in /Users/johnlindquist/dev/script-kit-gpui succeeded in 52ms:
diff --git a/src/app_impl/startup.rs b/src/app_impl/startup.rs
index 2e95070..441b4f2 100644
--- a/src/app_impl/startup.rs
+++ b/src/app_impl/startup.rs
@@ -555,9 +555,11 @@ impl ScriptListApp {
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -720,20 +722,23 @@ impl ScriptListApp {
                                 }
                             }
 succeeded in 51ms:
diff --git a/src/app_impl/tests.rs b/src/app_impl/tests.rs
index d75ba12..bb78d84 100644
--- a/src/app_impl/tests.rs
+++ b/src/app_impl/tests.rs
@@ -1,6 +1,7 @@
 #[cfg(test)]
 mod app_impl_state_sync_tests {
     use super::{calculate_fallback_error_message, ScriptListApp};
+    use std::fs;
 
     #[test]
     fn test_sync_builtin_query_state_updates_query_and_selection_when_changed() {
@@ -46,4 +47,28 @@ mod app_impl_state_sync_tests {
         assert!(message.contains("Could not evaluate expression"));
         assert!(message.contains("Check the syntax and try again"));
     }
+
+    #[test]
+    fn test_shift_tab_routes_to_ai_script_generation_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
 succeeded in 51ms:
diff --git a/src/app_impl/startup_new_tab.rs b/src/app_impl/startup_new_tab.rs
index 6c807a1..7c84270 100644
--- a/src/app_impl/startup_new_tab.rs
+++ b/src/app_impl/startup_new_tab.rs
@@ -6,9 +6,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -171,20 +173,23 @@
                                 }
                             }
 succeeded in 51ms:
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index 98a0c9c..c985a50 100644
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -1,4 +1,194 @@
 use super::*;
+use anyhow::{anyhow, Context as AnyhowContext, Result};
+
+#[derive(Debug, Clone, Copy)]
+enum AiScriptGenerationStage {
+    SelectModel,
+    ResolveProvider,
+    RequestCompletion,
+    ExtractScript,
+    CreateScriptFile,
+    WriteScriptFile,
+    OpenEditor,
+}
+
+impl AiScriptGenerationStage {
file update:
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index 98a0c9cd1db5a933992c0f99cb20a3c2ca436b41..c985a5090574816267103019d1b272e1e3151a38
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -1,4 +1,194 @@
 use super::*;
+use anyhow::{anyhow, Context as AnyhowContext, Result};
+
+#[derive(Debug, Clone, Copy)]
+enum AiScriptGenerationStage {
+    SelectModel,
+    ResolveProvider,
+    RequestCompletion,
+    ExtractScript,
+    CreateScriptFile,
+    WriteScriptFile,
+    OpenEditor,
+}
+
+impl AiScriptGenerationStage {
+    fn as_str(self) -> &'static str {
+        match self {
+            Self::SelectModel => "select_model",
+            Self::ResolveProvider => "resolve_provider",
+            Self::RequestCompletion => "request_completion",
+            Self::ExtractScript => "extract_script",
+            Self::CreateScriptFile => "create_script_file",
+            Self::WriteScriptFile => "write_script_file",
+            Self::OpenEditor => "open_editor",
+        }
+    }
+}
+
+const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You are ScriptKitScriptGenerator.
+Generate a complete Script Kit TypeScript script and return code only.
+Requirements:
+- Include metadata comments at the top:
+  // Name: <script name>
+  // Description: <short summary>
+- Include: import "@scriptkit/sdk";
+- Use idiomatic Script Kit APIs (for example await arg(), await div(), await editor()) when useful.
+- Prefer clear async/await flow and practical defaults.
+- Return a full runnable script with no extra explanation."#;
+
+fn build_ai_script_generation_user_prompt(description: &str) -> String {
+    format!(
+        "Generate a complete Script Kit script for this request:\n\n{}\n\nReturn only the TypeScript script source.",
+        description.trim()
+    )
+}
+
+fn select_default_ai_script_model(
+    registry: &crate::ai::ProviderRegistry,
+) -> Option<crate::ai::ModelInfo> {
+    let all_models = registry.get_all_models();
+    all_models
+        .iter()
+        .find(|model| model.provider.eq_ignore_ascii_case("vercel"))
+        .cloned()
+        .or_else(|| all_models.first().cloned())
+}
+
+fn extract_generated_script_source(raw_response: &str) -> Option<String> {
+    let trimmed = raw_response.trim();
+    if trimmed.is_empty() {
+        return None;
+    }
+
+    // Prefer typed code fences first, then generic fences.
+    for fence in ["```typescript", "```ts", "```javascript", "```js", "```"] {
+        if let Some(content) = extract_first_fenced_block(trimmed, fence) {
+            return normalize_generated_script(content);
+        }
+    }
+
+    normalize_generated_script(trimmed.to_string())
+}
+
+fn extract_first_fenced_block(response: &str, fence_start: &str) -> Option<String> {
+    let start_index = response.find(fence_start)?;
+    let after_fence = &response[start_index + fence_start.len()..];
+    let after_newline = after_fence
+        .strip_prefix("\r\n")
+        .or_else(|| after_fence.strip_prefix('\n'))
+        .unwrap_or(after_fence);
+    let end_index = after_newline.find("```")?;
+    Some(after_newline[..end_index].trim().to_string())
+}
+
+fn normalize_generated_script(content: String) -> Option<String> {
+    let trimmed = content.trim();
+    if trimmed.is_empty() {
+        None
+    } else {
+        Some(format!("{trimmed}\n"))
+    }
+}
+
+fn derive_script_name_from_description(description: &str) -> String {
+    let mut slug = String::new();
+    let mut last_was_dash = false;
+
+    for ch in description.chars() {
+        let lower = ch.to_ascii_lowercase();
+        if lower.is_ascii_alphanumeric() {
+            slug.push(lower);
+            last_was_dash = false;
+        } else if !last_was_dash && !slug.is_empty() {
+            slug.push('-');
+            last_was_dash = true;
+        }
+
+        if slug.len() >= 48 {
+            break;
+        }
+    }
+
+    while slug.ends_with('-') {
+        slug.pop();
+    }
+
+    if slug.is_empty() {
+        "ai-generated-script".to_string()
+    } else {
+        slug
+    }
+}
+
+fn generate_script_via_ai_backend(
+    registry: &crate::ai::ProviderRegistry,
+    model_id: &str,
+    prompt_description: &str,
+    config: &crate::config::Config,
+) -> Result<std::path::PathBuf> {
+    let provider = registry
+        .find_provider_for_model(model_id)
+        .cloned()
+        .ok_or_else(|| {
+            anyhow!(
+                "state={} attempted=find_provider_for_model model_id={} failure=provider_not_found",
+                AiScriptGenerationStage::ResolveProvider.as_str(),
+                model_id
+            )
+        })?;
+
+    let request_messages = vec![
+        crate::ai::ProviderMessage::system(AI_SCRIPT_GENERATION_SYSTEM_PROMPT),
+        crate::ai::ProviderMessage::user(build_ai_script_generation_user_prompt(prompt_description)),
+    ];
+
+    let ai_response = provider
+        .send_message(&request_messages, model_id)
+        .with_context(|| {
+            format!(
+                "state={} attempted=send_message model_id={} provider={} failure=provider_call_failed",
+                AiScriptGenerationStage::RequestCompletion.as_str(),
+                model_id,
+                provider.provider_id()
+            )
+        })?;
+
+    let generated_script = extract_generated_script_source(&ai_response).ok_or_else(|| {
+        anyhow!(
+            "state={} attempted=extract_generated_script model_id={} failure=empty_script_response",
+            AiScriptGenerationStage::ExtractScript.as_str(),
+            model_id
+        )
+    })?;
+
+    let script_name = derive_script_name_from_description(prompt_description);
+    let script_path = crate::script_creation::create_new_script(&script_name).with_context(|| {
+        format!(
+            "state={} attempted=create_new_script name={} failure=create_script_failed",
+            AiScriptGenerationStage::CreateScriptFile.as_str(),
+            script_name
+        )
+    })?;
+
+    std::fs::write(&script_path, generated_script).with_context(|| {
+        format!(
+            "state={} attempted=write_script path={} failure=write_failed",
+            AiScriptGenerationStage::WriteScriptFile.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    crate::script_creation::open_in_editor(&script_path, config).with_context(|| {
+        format!(
+            "state={} attempted=open_in_editor path={} failure=open_editor_failed",
+            AiScriptGenerationStage::OpenEditor.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
 
 impl ScriptListApp {
     pub(crate) fn is_in_prompt(&self) -> bool {
@@ -274,6 +464,145 @@
         cx.notify();
     }
 
+    /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
+    /// The generated script is saved to disk and opened in the configured editor.
+    pub fn generate_script_from_ai_prompt(
+        &mut self,
+        prompt_description: String,
+        cx: &mut Context<Self>,
+    ) {
+        let prompt_description = prompt_description.trim().to_string();
+        if prompt_description.is_empty() {
+            return;
+        }
+
+        let registry = self.cached_provider_registry.clone().unwrap_or_else(|| {
+            crate::ai::ProviderRegistry::from_environment_with_config(Some(&self.config))
+        });
+
+        if !registry.has_any_provider() {
+            self.toast_manager.push(
+                components::toast::Toast::error("No AI providers configured for script generation", &self.theme)
+                    .duration_ms(Some(5000)),
+            );
+            cx.notify();
+            return;
+        }
+
+        let selected_model = match select_default_ai_script_model(&registry) {
+            Some(model) => model,
+            None => {
+                let stage = AiScriptGenerationStage::SelectModel.as_str();
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    &format!(
+                        "state={} attempted=select_default_model failure=no_available_models",
+                        stage
+                    ),
+                );
+                self.toast_manager.push(
+                    components::toast::Toast::error(
+                        "No AI models available for script generation",
+                        &self.theme,
+                    )
+                    .duration_ms(Some(5000)),
+                );
+                cx.notify();
+                return;
+            }
+        };
+
+        let model_id = selected_model.id.clone();
+        let provider = selected_model.provider.clone();
+        let config = self.config.clone();
+        let (tx, rx) = async_channel::bounded::<std::result::Result<std::path::PathBuf, String>>(1);
+
+        logging::log(
+            "AI_SCRIPT_GEN",
+            &format!(
+                "state=queued attempted=shift_tab_script_generation model_id={} provider={} prompt_len={}",
+                model_id,
+                provider,
+                prompt_description.len()
+            ),
+        );
+        self.show_hud("Generating script with AI...".to_string(), Some(1500), cx);
+
+        std::thread::spawn(move || {
+            logging::log(
+                "AI_SCRIPT_GEN",
+                &format!(
+                    "state=running attempted=generate_script model_id={} provider={}",
+                    model_id, provider
+                ),
+            );
+
+            let generation_result =
+                generate_script_via_ai_backend(&registry, &model_id, &prompt_description, &config)
+                    .map_err(|error| error.to_string());
+
+            if tx.send_blocking(generation_result).is_err() {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=send_result failure=result_channel_closed",
+                );
+            }
+        });
+
+        cx.spawn(async move |this, cx| {
+            let Ok(result) = rx.recv().await else {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=recv_result failure=result_channel_closed",
+                );
+                return;
+            };
+
+            let _ = cx.update(|cx| {
+                this.update(cx, |app, cx| {
+                    match result {
+                        Ok(script_path) => {
+                            let script_name = script_path
+                                .file_name()
+                                .and_then(|name| name.to_str())
+                                .unwrap_or("generated script");
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!(
+                                    "state=completed attempted=generate_script path={}",
+                                    script_path.display()
+                                ),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::success(
+                                    format!("Generated and opened {}", script_name),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(3500)),
+                            );
+                            app.close_and_reset_window(cx);
+                        }
+                        Err(error) => {
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!("state=failed attempted=generate_script error={}", error),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::error(
+                                    format!("Failed to generate script: {}", error),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(7000)),
+                            );
+                            cx.notify();
+                        }
+                    }
+                })
+            });
+        })
+        .detach();
+    }
+
     /// Rebuild the cached provider registry in a background thread.
     /// Called after config changes (API key saved, Claude Code enabled, etc.)
     pub fn rebuild_provider_registry_async(&mut self, cx: &mut Context<Self>) {
@@ -312,3 +641,44 @@
         .detach();
     }
 }
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_build_ai_script_generation_user_prompt_includes_description() {
+        let prompt = build_ai_script_generation_user_prompt("create a weather script");
+        assert!(prompt.contains("create a weather script"));
+        assert!(prompt.contains("Script Kit script"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_prefers_typescript_fence() {
+        let response = r#"
+Here's your script:
+```typescript
+// Name: Weather
+import "@scriptkit/sdk";
+```
+"#;
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert!(extracted.contains("// Name: Weather"));
+        assert!(!extracted.contains("```"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_falls_back_to_plain_text() {
+        let response = "// Name: Plain\nimport \"@scriptkit/sdk\";";
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert_eq!(extracted, "// Name: Plain\nimport \"@scriptkit/sdk\";\n");
+    }
+
+    #[test]
+    fn test_derive_script_name_from_description_uses_fallback_for_symbols() {
+        let name = derive_script_name_from_description("@@@ !!!");
+        assert_eq!(name, "ai-generated-script");
+    }
+}
diff --git a/src/app_impl/startup.rs b/src/app_impl/startup.rs
index 2e95070751083b8f4c24cd7eae8372d6bec3f6d6..441b4f23bca1b6294025c698eeb11d45815e85e0
--- a/src/app_impl/startup.rs
+++ b/src/app_impl/startup.rs
@@ -555,9 +555,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -720,20 +722,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/startup_new_tab.rs b/src/app_impl/startup_new_tab.rs
index 6c807a195553d9f168087c2ca6d771c76e555c11..7c84270cbe91c0b1ddd460aa29db8c788766d0cf
--- a/src/app_impl/startup_new_tab.rs
+++ b/src/app_impl/startup_new_tab.rs
@@ -6,9 +6,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -171,20 +173,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/tests.rs b/src/app_impl/tests.rs
index d75ba121851d7233c5ae77bc474a64b27e7c31d2..bb78d84e5a8ec9ba80c0a2287ad0c1cba93f226d
--- a/src/app_impl/tests.rs
+++ b/src/app_impl/tests.rs
@@ -1,6 +1,7 @@
 #[cfg(test)]
 mod app_impl_state_sync_tests {
     use super::{calculate_fallback_error_message, ScriptListApp};
+    use std::fs;
 
     #[test]
     fn test_sync_builtin_query_state_updates_query_and_selection_when_changed() {
@@ -46,4 +47,28 @@
         assert!(message.contains("Could not evaluate expression"));
         assert!(message.contains("Check the syntax and try again"));
     }
+
+    #[test]
+    fn test_shift_tab_routes_to_ai_script_generation_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("if has_shift")
+                && startup_tab.contains("this.generate_script_from_ai_prompt(query, cx);"),
+            "Shift+Tab in ScriptList should route to generate_script_from_ai_prompt. \
+             Missing expected branch in startup_new_tab.rs"
+        );
+    }
+
+    #[test]
+    fn test_tab_still_routes_to_inline_ai_chat_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("this.show_inline_ai_chat(Some(query), cx);"),
+            "Tab in ScriptList should continue routing to show_inline_ai_chat"
+        );
+    }
 }


thinking
**Inspecting import conflicts**
codex
I’ve reviewed the diffs and next I’m checking file context for compile safety (imports, existing helpers, and test module structure) before running scoped tests.
exec
/bin/zsh -lc "sed -n '1,220p' src/app_impl/prompt_ai.rs" in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc "sed -n '520,760p' src/app_impl/prompt_ai.rs" in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc "sed -n '220,520p' src/app_impl/prompt_ai.rs" in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc 'rg -n "''^use|fn generate_script_from_ai_prompt|mod tests|show_inline_ai_chat|cached_provider_registry|ProviderRegistry::from_environment_with_config" src/app_impl/prompt_ai.rs src/app_impl/startup.rs src/app_impl/startup_new_tab.rs src/app_impl/tests.rs' in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
use super::*;
use anyhow::{anyhow, Context as AnyhowContext, Result};

#[derive(Debug, Clone, Copy)]
enum AiScriptGenerationStage {
    SelectModel,
    ResolveProvider,
    RequestCompletion,
    ExtractScript,
    CreateScriptFile,
    WriteScriptFile,
    OpenEditor,
}

impl AiScriptGenerationStage {
    fn as_str(self) -> &'static str {
        match self {
            Self::SelectModel => "select_model",
            Self::ResolveProvider => "resolve_provider",
            Self::RequestCompletion => "request_completion",
 succeeded in 51ms:
        logging::log(
            "AI_SCRIPT_GEN",
            &format!(
                "state=queued attempted=shift_tab_script_generation model_id={} provider={} prompt_len={}",
                model_id,
                provider,
                prompt_description.len()
            ),
        );
        self.show_hud("Generating script with AI...".to_string(), Some(1500), cx);

        std::thread::spawn(move || {
            logging::log(
                "AI_SCRIPT_GEN",
                &format!(
                    "state=running attempted=generate_script model_id={} provider={}",
                    model_id, provider
                ),
            );

 succeeded in 51ms:
    ) {
        logging::log(
            "UI",
            &format!("Submitting response for {}: {:?}", id, value),
        );

        let response = Message::Submit { id, value };

        if let Some(ref sender) = self.response_sender {
            // Use try_send to avoid blocking UI thread
            // If channel is full, the script isn't reading - log warning but don't freeze UI
            match sender.try_send(response) {
                Ok(()) => {
                    logging::log("UI", "Response queued for script");
                }
                Err(std::sync::mpsc::TrySendError::Full(_)) => {
                    // Channel is full - script isn't reading stdin fast enough
                    // This shouldn't happen in normal operation, log as warning
                    logging::log(
                        "WARN",
 succeeded in 51ms:
src/app_impl/prompt_ai.rs:1:use super::*;
src/app_impl/prompt_ai.rs:2:use anyhow::{anyhow, Context as AnyhowContext, Result};
src/app_impl/prompt_ai.rs:348:    pub fn show_inline_ai_chat(&mut self, initial_query: Option<String>, cx: &mut Context<Self>) {
src/app_impl/prompt_ai.rs:363:            .cached_provider_registry
src/app_impl/prompt_ai.rs:365:            .unwrap_or_else(|| ProviderRegistry::from_environment_with_config(Some(&self.config)));
src/app_impl/prompt_ai.rs:469:    pub fn generate_script_from_ai_prompt(
src/app_impl/prompt_ai.rs:479:        let registry = self.cached_provider_registry.clone().unwrap_or_else(|| {
src/app_impl/prompt_ai.rs:480:            crate::ai::ProviderRegistry::from_environment_with_config(Some(&self.config))
src/app_impl/prompt_ai.rs:609:        self.cached_provider_registry = None;
src/app_impl/prompt_ai.rs:615:                crate::ai::ProviderRegistry::from_environment_with_config(Some(&config_clone));
src/app_impl/prompt_ai.rs:633:                    app.cached_provider_registry = Some(registry);
src/app_impl/prompt_ai.rs:646:mod tests {
src/app_impl/startup_new_tab.rs:191:                                    this.show_inline_ai_chat(Some(query), cx);
src/app_impl/startup.rs:1:use super::*;
src/app_impl/startup.rs:494:            cached_provider_registry: None,
src/app_impl/startup.rs:515:                    crate::ai::ProviderRegistry::from_environment_with_config(Some(&config_clone));
src/app_impl/startup.rs:536:                        app.cached_provider_registry = Some(registry);
src/app_impl/startup.rs:740:                                    this.show_inline_ai_chat(Some(query), cx);
src/app_impl/tests.rs:70:            startup_tab.contains("this.show_inline_ai_chat(Some(query), cx);"),
src/app_impl/tests.rs:71:            "Tab in ScriptList should continue routing to show_inline_ai_chat"
file update:
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index 98a0c9cd1db5a933992c0f99cb20a3c2ca436b41..c985a5090574816267103019d1b272e1e3151a38
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -1,4 +1,194 @@
 use super::*;
+use anyhow::{anyhow, Context as AnyhowContext, Result};
+
+#[derive(Debug, Clone, Copy)]
+enum AiScriptGenerationStage {
+    SelectModel,
+    ResolveProvider,
+    RequestCompletion,
+    ExtractScript,
+    CreateScriptFile,
+    WriteScriptFile,
+    OpenEditor,
+}
+
+impl AiScriptGenerationStage {
+    fn as_str(self) -> &'static str {
+        match self {
+            Self::SelectModel => "select_model",
+            Self::ResolveProvider => "resolve_provider",
+            Self::RequestCompletion => "request_completion",
+            Self::ExtractScript => "extract_script",
+            Self::CreateScriptFile => "create_script_file",
+            Self::WriteScriptFile => "write_script_file",
+            Self::OpenEditor => "open_editor",
+        }
+    }
+}
+
+const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You are ScriptKitScriptGenerator.
+Generate a complete Script Kit TypeScript script and return code only.
+Requirements:
+- Include metadata comments at the top:
+  // Name: <script name>
+  // Description: <short summary>
+- Include: import "@scriptkit/sdk";
+- Use idiomatic Script Kit APIs (for example await arg(), await div(), await editor()) when useful.
+- Prefer clear async/await flow and practical defaults.
+- Return a full runnable script with no extra explanation."#;
+
+fn build_ai_script_generation_user_prompt(description: &str) -> String {
+    format!(
+        "Generate a complete Script Kit script for this request:\n\n{}\n\nReturn only the TypeScript script source.",
+        description.trim()
+    )
+}
+
+fn select_default_ai_script_model(
+    registry: &crate::ai::ProviderRegistry,
+) -> Option<crate::ai::ModelInfo> {
+    let all_models = registry.get_all_models();
+    all_models
+        .iter()
+        .find(|model| model.provider.eq_ignore_ascii_case("vercel"))
+        .cloned()
+        .or_else(|| all_models.first().cloned())
+}
+
+fn extract_generated_script_source(raw_response: &str) -> Option<String> {
+    let trimmed = raw_response.trim();
+    if trimmed.is_empty() {
+        return None;
+    }
+
+    // Prefer typed code fences first, then generic fences.
+    for fence in ["```typescript", "```ts", "```javascript", "```js", "```"] {
+        if let Some(content) = extract_first_fenced_block(trimmed, fence) {
+            return normalize_generated_script(content);
+        }
+    }
+
+    normalize_generated_script(trimmed.to_string())
+}
+
+fn extract_first_fenced_block(response: &str, fence_start: &str) -> Option<String> {
+    let start_index = response.find(fence_start)?;
+    let after_fence = &response[start_index + fence_start.len()..];
+    let after_newline = after_fence
+        .strip_prefix("\r\n")
+        .or_else(|| after_fence.strip_prefix('\n'))
+        .unwrap_or(after_fence);
+    let end_index = after_newline.find("```")?;
+    Some(after_newline[..end_index].trim().to_string())
+}
+
+fn normalize_generated_script(content: String) -> Option<String> {
+    let trimmed = content.trim();
+    if trimmed.is_empty() {
+        None
+    } else {
+        Some(format!("{trimmed}\n"))
+    }
+}
+
+fn derive_script_name_from_description(description: &str) -> String {
+    let mut slug = String::new();
+    let mut last_was_dash = false;
+
+    for ch in description.chars() {
+        let lower = ch.to_ascii_lowercase();
+        if lower.is_ascii_alphanumeric() {
+            slug.push(lower);
+            last_was_dash = false;
+        } else if !last_was_dash && !slug.is_empty() {
+            slug.push('-');
+            last_was_dash = true;
+        }
+
+        if slug.len() >= 48 {
+            break;
+        }
+    }
+
+    while slug.ends_with('-') {
+        slug.pop();
+    }
+
+    if slug.is_empty() {
+        "ai-generated-script".to_string()
+    } else {
+        slug
+    }
+}
+
+fn generate_script_via_ai_backend(
+    registry: &crate::ai::ProviderRegistry,
+    model_id: &str,
+    prompt_description: &str,
+    config: &crate::config::Config,
+) -> Result<std::path::PathBuf> {
+    let provider = registry
+        .find_provider_for_model(model_id)
+        .cloned()
+        .ok_or_else(|| {
+            anyhow!(
+                "state={} attempted=find_provider_for_model model_id={} failure=provider_not_found",
+                AiScriptGenerationStage::ResolveProvider.as_str(),
+                model_id
+            )
+        })?;
+
+    let request_messages = vec![
+        crate::ai::ProviderMessage::system(AI_SCRIPT_GENERATION_SYSTEM_PROMPT),
+        crate::ai::ProviderMessage::user(build_ai_script_generation_user_prompt(prompt_description)),
+    ];
+
+    let ai_response = provider
+        .send_message(&request_messages, model_id)
+        .with_context(|| {
+            format!(
+                "state={} attempted=send_message model_id={} provider={} failure=provider_call_failed",
+                AiScriptGenerationStage::RequestCompletion.as_str(),
+                model_id,
+                provider.provider_id()
+            )
+        })?;
+
+    let generated_script = extract_generated_script_source(&ai_response).ok_or_else(|| {
+        anyhow!(
+            "state={} attempted=extract_generated_script model_id={} failure=empty_script_response",
+            AiScriptGenerationStage::ExtractScript.as_str(),
+            model_id
+        )
+    })?;
+
+    let script_name = derive_script_name_from_description(prompt_description);
+    let script_path = crate::script_creation::create_new_script(&script_name).with_context(|| {
+        format!(
+            "state={} attempted=create_new_script name={} failure=create_script_failed",
+            AiScriptGenerationStage::CreateScriptFile.as_str(),
+            script_name
+        )
+    })?;
+
+    std::fs::write(&script_path, generated_script).with_context(|| {
+        format!(
+            "state={} attempted=write_script path={} failure=write_failed",
+            AiScriptGenerationStage::WriteScriptFile.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    crate::script_creation::open_in_editor(&script_path, config).with_context(|| {
+        format!(
+            "state={} attempted=open_in_editor path={} failure=open_editor_failed",
+            AiScriptGenerationStage::OpenEditor.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
 
 impl ScriptListApp {
     pub(crate) fn is_in_prompt(&self) -> bool {
@@ -274,6 +464,145 @@
         cx.notify();
     }
 
+    /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
+    /// The generated script is saved to disk and opened in the configured editor.
+    pub fn generate_script_from_ai_prompt(
+        &mut self,
+        prompt_description: String,
+        cx: &mut Context<Self>,
+    ) {
+        let prompt_description = prompt_description.trim().to_string();
+        if prompt_description.is_empty() {
+            return;
+        }
+
+        let registry = self.cached_provider_registry.clone().unwrap_or_else(|| {
+            crate::ai::ProviderRegistry::from_environment_with_config(Some(&self.config))
+        });
+
+        if !registry.has_any_provider() {
+            self.toast_manager.push(
+                components::toast::Toast::error("No AI providers configured for script generation", &self.theme)
+                    .duration_ms(Some(5000)),
+            );
+            cx.notify();
+            return;
+        }
+
+        let selected_model = match select_default_ai_script_model(&registry) {
+            Some(model) => model,
+            None => {
+                let stage = AiScriptGenerationStage::SelectModel.as_str();
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    &format!(
+                        "state={} attempted=select_default_model failure=no_available_models",
+                        stage
+                    ),
+                );
+                self.toast_manager.push(
+                    components::toast::Toast::error(
+                        "No AI models available for script generation",
+                        &self.theme,
+                    )
+                    .duration_ms(Some(5000)),
+                );
+                cx.notify();
+                return;
+            }
+        };
+
+        let model_id = selected_model.id.clone();
+        let provider = selected_model.provider.clone();
+        let config = self.config.clone();
+        let (tx, rx) = async_channel::bounded::<std::result::Result<std::path::PathBuf, String>>(1);
+
+        logging::log(
+            "AI_SCRIPT_GEN",
+            &format!(
+                "state=queued attempted=shift_tab_script_generation model_id={} provider={} prompt_len={}",
+                model_id,
+                provider,
+                prompt_description.len()
+            ),
+        );
+        self.show_hud("Generating script with AI...".to_string(), Some(1500), cx);
+
+        std::thread::spawn(move || {
+            logging::log(
+                "AI_SCRIPT_GEN",
+                &format!(
+                    "state=running attempted=generate_script model_id={} provider={}",
+                    model_id, provider
+                ),
+            );
+
+            let generation_result =
+                generate_script_via_ai_backend(&registry, &model_id, &prompt_description, &config)
+                    .map_err(|error| error.to_string());
+
+            if tx.send_blocking(generation_result).is_err() {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=send_result failure=result_channel_closed",
+                );
+            }
+        });
+
+        cx.spawn(async move |this, cx| {
+            let Ok(result) = rx.recv().await else {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=recv_result failure=result_channel_closed",
+                );
+                return;
+            };
+
+            let _ = cx.update(|cx| {
+                this.update(cx, |app, cx| {
+                    match result {
+                        Ok(script_path) => {
+                            let script_name = script_path
+                                .file_name()
+                                .and_then(|name| name.to_str())
+                                .unwrap_or("generated script");
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!(
+                                    "state=completed attempted=generate_script path={}",
+                                    script_path.display()
+                                ),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::success(
+                                    format!("Generated and opened {}", script_name),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(3500)),
+                            );
+                            app.close_and_reset_window(cx);
+                        }
+                        Err(error) => {
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!("state=failed attempted=generate_script error={}", error),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::error(
+                                    format!("Failed to generate script: {}", error),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(7000)),
+                            );
+                            cx.notify();
+                        }
+                    }
+                })
+            });
+        })
+        .detach();
+    }
+
     /// Rebuild the cached provider registry in a background thread.
     /// Called after config changes (API key saved, Claude Code enabled, etc.)
     pub fn rebuild_provider_registry_async(&mut self, cx: &mut Context<Self>) {
@@ -312,3 +641,44 @@
         .detach();
     }
 }
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_build_ai_script_generation_user_prompt_includes_description() {
+        let prompt = build_ai_script_generation_user_prompt("create a weather script");
+        assert!(prompt.contains("create a weather script"));
+        assert!(prompt.contains("Script Kit script"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_prefers_typescript_fence() {
+        let response = r#"
+Here's your script:
+```typescript
+// Name: Weather
+import "@scriptkit/sdk";
+```
+"#;
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert!(extracted.contains("// Name: Weather"));
+        assert!(!extracted.contains("```"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_falls_back_to_plain_text() {
+        let response = "// Name: Plain\nimport \"@scriptkit/sdk\";";
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert_eq!(extracted, "// Name: Plain\nimport \"@scriptkit/sdk\";\n");
+    }
+
+    #[test]
+    fn test_derive_script_name_from_description_uses_fallback_for_symbols() {
+        let name = derive_script_name_from_description("@@@ !!!");
+        assert_eq!(name, "ai-generated-script");
+    }
+}
diff --git a/src/app_impl/startup.rs b/src/app_impl/startup.rs
index 2e95070751083b8f4c24cd7eae8372d6bec3f6d6..441b4f23bca1b6294025c698eeb11d45815e85e0
--- a/src/app_impl/startup.rs
+++ b/src/app_impl/startup.rs
@@ -555,9 +555,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -720,20 +722,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/startup_new_tab.rs b/src/app_impl/startup_new_tab.rs
index 6c807a195553d9f168087c2ca6d771c76e555c11..7c84270cbe91c0b1ddd460aa29db8c788766d0cf
--- a/src/app_impl/startup_new_tab.rs
+++ b/src/app_impl/startup_new_tab.rs
@@ -6,9 +6,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -171,20 +173,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/tests.rs b/src/app_impl/tests.rs
index d75ba121851d7233c5ae77bc474a64b27e7c31d2..bb78d84e5a8ec9ba80c0a2287ad0c1cba93f226d
--- a/src/app_impl/tests.rs
+++ b/src/app_impl/tests.rs
@@ -1,6 +1,7 @@
 #[cfg(test)]
 mod app_impl_state_sync_tests {
     use super::{calculate_fallback_error_message, ScriptListApp};
+    use std::fs;
 
     #[test]
     fn test_sync_builtin_query_state_updates_query_and_selection_when_changed() {
@@ -46,4 +47,28 @@
         assert!(message.contains("Could not evaluate expression"));
         assert!(message.contains("Check the syntax and try again"));
     }
+
+    #[test]
+    fn test_shift_tab_routes_to_ai_script_generation_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("if has_shift")
+                && startup_tab.contains("this.generate_script_from_ai_prompt(query, cx);"),
+            "Shift+Tab in ScriptList should route to generate_script_from_ai_prompt. \
+             Missing expected branch in startup_new_tab.rs"
+        );
+    }
+
+    #[test]
+    fn test_tab_still_routes_to_inline_ai_chat_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("this.show_inline_ai_chat(Some(query), cx);"),
+            "Tab in ScriptList should continue routing to show_inline_ai_chat"
+        );
+    }
 }


thinking
**Inspecting AI model and provider types**
codex
I’m now validating type/API compatibility against the existing AI provider interfaces to ensure the new generation path uses real methods and structs.
exec
/bin/zsh -lc "sed -n '1,260p' src/ai/mod.rs" in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc "rg -n \"pub struct ModelInfo|struct ModelInfo|fn get_all_models|fn find_provider_for_model|trait.*send_message|ProviderMessage::system|ProviderMessage::user|provider_id\\(\" src/ai -g '*.rs'" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
//! AI Chat Module
//!
//! This module provides the data layer for the AI chat window feature.
//! It includes data models, SQLite storage with FTS5 search support,
//! and provider abstraction for BYOK (Bring Your Own Key) AI integration.
//!
//! # Architecture
//!
//! ```text
//! src/ai/
//! ├── mod.rs       - Module exports and documentation
//! ├── model.rs     - Data models (Chat, Message, ChatId, MessageRole)
//! ├── storage.rs   - SQLite persistence layer
//! ├── config.rs    - Environment variable detection and model configuration
//! └── providers.rs - Provider trait and implementations (OpenAI, Anthropic, etc.)
//! ```
//!
//! # Database Location
//!
//! The AI chats database is stored at `~/.scriptkit/ai-chats.db`.
 succeeded in 52ms:
src/ai/providers.rs:382:    fn provider_id(&self) -> &str;
src/ai/providers.rs:550:    fn provider_id(&self) -> &str {
src/ai/providers.rs:815:    fn provider_id(&self) -> &str {
src/ai/providers.rs:946:    fn provider_id(&self) -> &str {
src/ai/providers.rs:1006:    fn provider_id(&self) -> &str {
src/ai/providers.rs:1156:    fn provider_id(&self) -> &str {
src/ai/providers.rs:1835:    fn provider_id(&self) -> &str {
src/ai/providers.rs:2074:            .insert(provider.provider_id().to_string(), provider);
src/ai/providers.rs:2093:    pub fn get_all_models(&self) -> Vec<ModelInfo> {
src/ai/providers.rs:2110:    pub fn find_provider_for_model(&self, model_id: &str) -> Option<&Arc<dyn AiProvider>> {
src/ai/providers.rs:2129:        let user = ProviderMessage::user("Hello");
src/ai/providers.rs:2137:        let system = ProviderMessage::system("You are helpful");
src/ai/providers.rs:2145:        assert_eq!(provider.provider_id(), "openai");
src/ai/providers.rs:2156:        assert_eq!(provider.provider_id(), "anthropic");
src/ai/providers.rs:2172:            ProviderMessage::system("You are helpful"),
src/ai/providers.rs:2173:            ProviderMessage::user("Say hello"),
src/ai/providers.rs:2188:        let messages = vec![ProviderMessage::user("Say hello")];
src/ai/providers.rs:2215:            ProviderMessage::system("You are helpful"),
src/ai/providers.rs:2216:            ProviderMessage::user("Hello"),
src/ai/providers.rs:2231:            ProviderMessage::system("You are helpful"),
file update:
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index 98a0c9cd1db5a933992c0f99cb20a3c2ca436b41..c985a5090574816267103019d1b272e1e3151a38
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -1,4 +1,194 @@
 use super::*;
+use anyhow::{anyhow, Context as AnyhowContext, Result};
+
+#[derive(Debug, Clone, Copy)]
+enum AiScriptGenerationStage {
+    SelectModel,
+    ResolveProvider,
+    RequestCompletion,
+    ExtractScript,
+    CreateScriptFile,
+    WriteScriptFile,
+    OpenEditor,
+}
+
+impl AiScriptGenerationStage {
+    fn as_str(self) -> &'static str {
+        match self {
+            Self::SelectModel => "select_model",
+            Self::ResolveProvider => "resolve_provider",
+            Self::RequestCompletion => "request_completion",
+            Self::ExtractScript => "extract_script",
+            Self::CreateScriptFile => "create_script_file",
+            Self::WriteScriptFile => "write_script_file",
+            Self::OpenEditor => "open_editor",
+        }
+    }
+}
+
+const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You are ScriptKitScriptGenerator.
+Generate a complete Script Kit TypeScript script and return code only.
+Requirements:
+- Include metadata comments at the top:
+  // Name: <script name>
+  // Description: <short summary>
+- Include: import "@scriptkit/sdk";
+- Use idiomatic Script Kit APIs (for example await arg(), await div(), await editor()) when useful.
+- Prefer clear async/await flow and practical defaults.
+- Return a full runnable script with no extra explanation."#;
+
+fn build_ai_script_generation_user_prompt(description: &str) -> String {
+    format!(
+        "Generate a complete Script Kit script for this request:\n\n{}\n\nReturn only the TypeScript script source.",
+        description.trim()
+    )
+}
+
+fn select_default_ai_script_model(
+    registry: &crate::ai::ProviderRegistry,
+) -> Option<crate::ai::ModelInfo> {
+    let all_models = registry.get_all_models();
+    all_models
+        .iter()
+        .find(|model| model.provider.eq_ignore_ascii_case("vercel"))
+        .cloned()
+        .or_else(|| all_models.first().cloned())
+}
+
+fn extract_generated_script_source(raw_response: &str) -> Option<String> {
+    let trimmed = raw_response.trim();
+    if trimmed.is_empty() {
+        return None;
+    }
+
+    // Prefer typed code fences first, then generic fences.
+    for fence in ["```typescript", "```ts", "```javascript", "```js", "```"] {
+        if let Some(content) = extract_first_fenced_block(trimmed, fence) {
+            return normalize_generated_script(content);
+        }
+    }
+
+    normalize_generated_script(trimmed.to_string())
+}
+
+fn extract_first_fenced_block(response: &str, fence_start: &str) -> Option<String> {
+    let start_index = response.find(fence_start)?;
+    let after_fence = &response[start_index + fence_start.len()..];
+    let after_newline = after_fence
+        .strip_prefix("\r\n")
+        .or_else(|| after_fence.strip_prefix('\n'))
+        .unwrap_or(after_fence);
+    let end_index = after_newline.find("```")?;
+    Some(after_newline[..end_index].trim().to_string())
+}
+
+fn normalize_generated_script(content: String) -> Option<String> {
+    let trimmed = content.trim();
+    if trimmed.is_empty() {
+        None
+    } else {
+        Some(format!("{trimmed}\n"))
+    }
+}
+
+fn derive_script_name_from_description(description: &str) -> String {
+    let mut slug = String::new();
+    let mut last_was_dash = false;
+
+    for ch in description.chars() {
+        let lower = ch.to_ascii_lowercase();
+        if lower.is_ascii_alphanumeric() {
+            slug.push(lower);
+            last_was_dash = false;
+        } else if !last_was_dash && !slug.is_empty() {
+            slug.push('-');
+            last_was_dash = true;
+        }
+
+        if slug.len() >= 48 {
+            break;
+        }
+    }
+
+    while slug.ends_with('-') {
+        slug.pop();
+    }
+
+    if slug.is_empty() {
+        "ai-generated-script".to_string()
+    } else {
+        slug
+    }
+}
+
+fn generate_script_via_ai_backend(
+    registry: &crate::ai::ProviderRegistry,
+    model_id: &str,
+    prompt_description: &str,
+    config: &crate::config::Config,
+) -> Result<std::path::PathBuf> {
+    let provider = registry
+        .find_provider_for_model(model_id)
+        .cloned()
+        .ok_or_else(|| {
+            anyhow!(
+                "state={} attempted=find_provider_for_model model_id={} failure=provider_not_found",
+                AiScriptGenerationStage::ResolveProvider.as_str(),
+                model_id
+            )
+        })?;
+
+    let request_messages = vec![
+        crate::ai::ProviderMessage::system(AI_SCRIPT_GENERATION_SYSTEM_PROMPT),
+        crate::ai::ProviderMessage::user(build_ai_script_generation_user_prompt(prompt_description)),
+    ];
+
+    let ai_response = provider
+        .send_message(&request_messages, model_id)
+        .with_context(|| {
+            format!(
+                "state={} attempted=send_message model_id={} provider={} failure=provider_call_failed",
+                AiScriptGenerationStage::RequestCompletion.as_str(),
+                model_id,
+                provider.provider_id()
+            )
+        })?;
+
+    let generated_script = extract_generated_script_source(&ai_response).ok_or_else(|| {
+        anyhow!(
+            "state={} attempted=extract_generated_script model_id={} failure=empty_script_response",
+            AiScriptGenerationStage::ExtractScript.as_str(),
+            model_id
+        )
+    })?;
+
+    let script_name = derive_script_name_from_description(prompt_description);
+    let script_path = crate::script_creation::create_new_script(&script_name).with_context(|| {
+        format!(
+            "state={} attempted=create_new_script name={} failure=create_script_failed",
+            AiScriptGenerationStage::CreateScriptFile.as_str(),
+            script_name
+        )
+    })?;
+
+    std::fs::write(&script_path, generated_script).with_context(|| {
+        format!(
+            "state={} attempted=write_script path={} failure=write_failed",
+            AiScriptGenerationStage::WriteScriptFile.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    crate::script_creation::open_in_editor(&script_path, config).with_context(|| {
+        format!(
+            "state={} attempted=open_in_editor path={} failure=open_editor_failed",
+            AiScriptGenerationStage::OpenEditor.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
 
 impl ScriptListApp {
     pub(crate) fn is_in_prompt(&self) -> bool {
@@ -274,6 +464,145 @@
         cx.notify();
     }
 
+    /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
+    /// The generated script is saved to disk and opened in the configured editor.
+    pub fn generate_script_from_ai_prompt(
+        &mut self,
+        prompt_description: String,
+        cx: &mut Context<Self>,
+    ) {
+        let prompt_description = prompt_description.trim().to_string();
+        if prompt_description.is_empty() {
+            return;
+        }
+
+        let registry = self.cached_provider_registry.clone().unwrap_or_else(|| {
+            crate::ai::ProviderRegistry::from_environment_with_config(Some(&self.config))
+        });
+
+        if !registry.has_any_provider() {
+            self.toast_manager.push(
+                components::toast::Toast::error("No AI providers configured for script generation", &self.theme)
+                    .duration_ms(Some(5000)),
+            );
+            cx.notify();
+            return;
+        }
+
+        let selected_model = match select_default_ai_script_model(&registry) {
+            Some(model) => model,
+            None => {
+                let stage = AiScriptGenerationStage::SelectModel.as_str();
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    &format!(
+                        "state={} attempted=select_default_model failure=no_available_models",
+                        stage
+                    ),
+                );
+                self.toast_manager.push(
+                    components::toast::Toast::error(
+                        "No AI models available for script generation",
+                        &self.theme,
+                    )
+                    .duration_ms(Some(5000)),
+                );
+                cx.notify();
+                return;
+            }
+        };
+
+        let model_id = selected_model.id.clone();
+        let provider = selected_model.provider.clone();
+        let config = self.config.clone();
+        let (tx, rx) = async_channel::bounded::<std::result::Result<std::path::PathBuf, String>>(1);
+
+        logging::log(
+            "AI_SCRIPT_GEN",
+            &format!(
+                "state=queued attempted=shift_tab_script_generation model_id={} provider={} prompt_len={}",
+                model_id,
+                provider,
+                prompt_description.len()
+            ),
+        );
+        self.show_hud("Generating script with AI...".to_string(), Some(1500), cx);
+
+        std::thread::spawn(move || {
+            logging::log(
+                "AI_SCRIPT_GEN",
+                &format!(
+                    "state=running attempted=generate_script model_id={} provider={}",
+                    model_id, provider
+                ),
+            );
+
+            let generation_result =
+                generate_script_via_ai_backend(&registry, &model_id, &prompt_description, &config)
+                    .map_err(|error| error.to_string());
+
+            if tx.send_blocking(generation_result).is_err() {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=send_result failure=result_channel_closed",
+                );
+            }
+        });
+
+        cx.spawn(async move |this, cx| {
+            let Ok(result) = rx.recv().await else {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=recv_result failure=result_channel_closed",
+                );
+                return;
+            };
+
+            let _ = cx.update(|cx| {
+                this.update(cx, |app, cx| {
+                    match result {
+                        Ok(script_path) => {
+                            let script_name = script_path
+                                .file_name()
+                                .and_then(|name| name.to_str())
+                                .unwrap_or("generated script");
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!(
+                                    "state=completed attempted=generate_script path={}",
+                                    script_path.display()
+                                ),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::success(
+                                    format!("Generated and opened {}", script_name),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(3500)),
+                            );
+                            app.close_and_reset_window(cx);
+                        }
+                        Err(error) => {
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!("state=failed attempted=generate_script error={}", error),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::error(
+                                    format!("Failed to generate script: {}", error),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(7000)),
+                            );
+                            cx.notify();
+                        }
+                    }
+                })
+            });
+        })
+        .detach();
+    }
+
     /// Rebuild the cached provider registry in a background thread.
     /// Called after config changes (API key saved, Claude Code enabled, etc.)
     pub fn rebuild_provider_registry_async(&mut self, cx: &mut Context<Self>) {
@@ -312,3 +641,44 @@
         .detach();
     }
 }
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_build_ai_script_generation_user_prompt_includes_description() {
+        let prompt = build_ai_script_generation_user_prompt("create a weather script");
+        assert!(prompt.contains("create a weather script"));
+        assert!(prompt.contains("Script Kit script"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_prefers_typescript_fence() {
+        let response = r#"
+Here's your script:
+```typescript
+// Name: Weather
+import "@scriptkit/sdk";
+```
+"#;
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert!(extracted.contains("// Name: Weather"));
+        assert!(!extracted.contains("```"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_falls_back_to_plain_text() {
+        let response = "// Name: Plain\nimport \"@scriptkit/sdk\";";
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert_eq!(extracted, "// Name: Plain\nimport \"@scriptkit/sdk\";\n");
+    }
+
+    #[test]
+    fn test_derive_script_name_from_description_uses_fallback_for_symbols() {
+        let name = derive_script_name_from_description("@@@ !!!");
+        assert_eq!(name, "ai-generated-script");
+    }
+}
diff --git a/src/app_impl/startup.rs b/src/app_impl/startup.rs
index 2e95070751083b8f4c24cd7eae8372d6bec3f6d6..441b4f23bca1b6294025c698eeb11d45815e85e0
--- a/src/app_impl/startup.rs
+++ b/src/app_impl/startup.rs
@@ -555,9 +555,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -720,20 +722,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/startup_new_tab.rs b/src/app_impl/startup_new_tab.rs
index 6c807a195553d9f168087c2ca6d771c76e555c11..7c84270cbe91c0b1ddd460aa29db8c788766d0cf
--- a/src/app_impl/startup_new_tab.rs
+++ b/src/app_impl/startup_new_tab.rs
@@ -6,9 +6,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -171,20 +173,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/tests.rs b/src/app_impl/tests.rs
index d75ba121851d7233c5ae77bc474a64b27e7c31d2..bb78d84e5a8ec9ba80c0a2287ad0c1cba93f226d
--- a/src/app_impl/tests.rs
+++ b/src/app_impl/tests.rs
@@ -1,6 +1,7 @@
 #[cfg(test)]
 mod app_impl_state_sync_tests {
     use super::{calculate_fallback_error_message, ScriptListApp};
+    use std::fs;
 
     #[test]
     fn test_sync_builtin_query_state_updates_query_and_selection_when_changed() {
@@ -46,4 +47,28 @@
         assert!(message.contains("Could not evaluate expression"));
         assert!(message.contains("Check the syntax and try again"));
     }
+
+    #[test]
+    fn test_shift_tab_routes_to_ai_script_generation_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("if has_shift")
+                && startup_tab.contains("this.generate_script_from_ai_prompt(query, cx);"),
+            "Shift+Tab in ScriptList should route to generate_script_from_ai_prompt. \
+             Missing expected branch in startup_new_tab.rs"
+        );
+    }
+
+    #[test]
+    fn test_tab_still_routes_to_inline_ai_chat_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("this.show_inline_ai_chat(Some(query), cx);"),
+            "Tab in ScriptList should continue routing to show_inline_ai_chat"
+        );
+    }
 }


thinking
**Planning targeted test runs**
codex
I’m running only scoped Rust tests tied to the files I changed (`app_impl` key routing + prompt parsing helpers). If they fail, I’ll separate real regressions from unrelated workspace breakage.
exec
/bin/zsh -lc 'cargo test --lib app_impl::tests::app_impl_state_sync_tests::test_shift_tab_routes_to_ai_script_generation_in_script_list_tab_interceptor' in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc 'cargo test --lib app_impl::prompt_ai::tests::test_build_ai_script_generation_user_prompt_includes_description' in /Users/johnlindquist/dev/script-kit-gpui exited 101 in 18.55s:
    Blocking waiting for file lock on package cache
    Blocking waiting for file lock on package cache
    Blocking waiting for file lock on package cache
   Compiling script-kit-gpui v0.1.0 (/Users/johnlindquist/dev/script-kit-gpui)
error[E0761]: file for module `tests` found at both "src/protocol/message/tests.rs" and "src/protocol/message/tests/mod.rs"
  --> src/protocol/message/mod.rs:87:1
   |
87 | mod tests;
   | ^^^^^^^^^^
   |
   = help: delete or rename one of them to remove the ambiguity

error[E0761]: file for module `tests` found at both "src/designs/core/tests.rs" and "src/designs/core/tests/mod.rs"
 --> src/designs/core/part_04.rs:2:1
  |
2 | mod tests;
  | ^^^^^^^^^^
  |
  = help: delete or rename one of them to remove the ambiguity

file update:
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index 98a0c9cd1db5a933992c0f99cb20a3c2ca436b41..c985a5090574816267103019d1b272e1e3151a38
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -1,4 +1,194 @@
 use super::*;
+use anyhow::{anyhow, Context as AnyhowContext, Result};
+
+#[derive(Debug, Clone, Copy)]
+enum AiScriptGenerationStage {
+    SelectModel,
+    ResolveProvider,
+    RequestCompletion,
+    ExtractScript,
+    CreateScriptFile,
+    WriteScriptFile,
+    OpenEditor,
+}
+
+impl AiScriptGenerationStage {
+    fn as_str(self) -> &'static str {
+        match self {
+            Self::SelectModel => "select_model",
+            Self::ResolveProvider => "resolve_provider",
+            Self::RequestCompletion => "request_completion",
+            Self::ExtractScript => "extract_script",
+            Self::CreateScriptFile => "create_script_file",
+            Self::WriteScriptFile => "write_script_file",
+            Self::OpenEditor => "open_editor",
+        }
+    }
+}
+
+const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You are ScriptKitScriptGenerator.
+Generate a complete Script Kit TypeScript script and return code only.
+Requirements:
+- Include metadata comments at the top:
+  // Name: <script name>
+  // Description: <short summary>
+- Include: import "@scriptkit/sdk";
+- Use idiomatic Script Kit APIs (for example await arg(), await div(), await editor()) when useful.
+- Prefer clear async/await flow and practical defaults.
+- Return a full runnable script with no extra explanation."#;
+
+fn build_ai_script_generation_user_prompt(description: &str) -> String {
+    format!(
+        "Generate a complete Script Kit script for this request:\n\n{}\n\nReturn only the TypeScript script source.",
+        description.trim()
+    )
+}
+
+fn select_default_ai_script_model(
+    registry: &crate::ai::ProviderRegistry,
+) -> Option<crate::ai::ModelInfo> {
+    let all_models = registry.get_all_models();
+    all_models
+        .iter()
+        .find(|model| model.provider.eq_ignore_ascii_case("vercel"))
+        .cloned()
+        .or_else(|| all_models.first().cloned())
+}
+
+fn extract_generated_script_source(raw_response: &str) -> Option<String> {
+    let trimmed = raw_response.trim();
+    if trimmed.is_empty() {
+        return None;
+    }
+
+    // Prefer typed code fences first, then generic fences.
+    for fence in ["```typescript", "```ts", "```javascript", "```js", "```"] {
+        if let Some(content) = extract_first_fenced_block(trimmed, fence) {
+            return normalize_generated_script(content);
+        }
+    }
+
+    normalize_generated_script(trimmed.to_string())
+}
+
+fn extract_first_fenced_block(response: &str, fence_start: &str) -> Option<String> {
+    let start_index = response.find(fence_start)?;
+    let after_fence = &response[start_index + fence_start.len()..];
+    let after_newline = after_fence
+        .strip_prefix("\r\n")
+        .or_else(|| after_fence.strip_prefix('\n'))
+        .unwrap_or(after_fence);
+    let end_index = after_newline.find("```")?;
+    Some(after_newline[..end_index].trim().to_string())
+}
+
+fn normalize_generated_script(content: String) -> Option<String> {
+    let trimmed = content.trim();
+    if trimmed.is_empty() {
+        None
+    } else {
+        Some(format!("{trimmed}\n"))
+    }
+}
+
+fn derive_script_name_from_description(description: &str) -> String {
+    let mut slug = String::new();
+    let mut last_was_dash = false;
+
+    for ch in description.chars() {
+        let lower = ch.to_ascii_lowercase();
+        if lower.is_ascii_alphanumeric() {
+            slug.push(lower);
+            last_was_dash = false;
+        } else if !last_was_dash && !slug.is_empty() {
+            slug.push('-');
+            last_was_dash = true;
+        }
+
+        if slug.len() >= 48 {
+            break;
+        }
+    }
+
+    while slug.ends_with('-') {
+        slug.pop();
+    }
+
+    if slug.is_empty() {
+        "ai-generated-script".to_string()
+    } else {
+        slug
+    }
+}
+
+fn generate_script_via_ai_backend(
+    registry: &crate::ai::ProviderRegistry,
+    model_id: &str,
+    prompt_description: &str,
+    config: &crate::config::Config,
+) -> Result<std::path::PathBuf> {
+    let provider = registry
+        .find_provider_for_model(model_id)
+        .cloned()
+        .ok_or_else(|| {
+            anyhow!(
+                "state={} attempted=find_provider_for_model model_id={} failure=provider_not_found",
+                AiScriptGenerationStage::ResolveProvider.as_str(),
+                model_id
+            )
+        })?;
+
+    let request_messages = vec![
+        crate::ai::ProviderMessage::system(AI_SCRIPT_GENERATION_SYSTEM_PROMPT),
+        crate::ai::ProviderMessage::user(build_ai_script_generation_user_prompt(prompt_description)),
+    ];
+
+    let ai_response = provider
+        .send_message(&request_messages, model_id)
+        .with_context(|| {
+            format!(
+                "state={} attempted=send_message model_id={} provider={} failure=provider_call_failed",
+                AiScriptGenerationStage::RequestCompletion.as_str(),
+                model_id,
+                provider.provider_id()
+            )
+        })?;
+
+    let generated_script = extract_generated_script_source(&ai_response).ok_or_else(|| {
+        anyhow!(
+            "state={} attempted=extract_generated_script model_id={} failure=empty_script_response",
+            AiScriptGenerationStage::ExtractScript.as_str(),
+            model_id
+        )
+    })?;
+
+    let script_name = derive_script_name_from_description(prompt_description);
+    let script_path = crate::script_creation::create_new_script(&script_name).with_context(|| {
+        format!(
+            "state={} attempted=create_new_script name={} failure=create_script_failed",
+            AiScriptGenerationStage::CreateScriptFile.as_str(),
+            script_name
+        )
+    })?;
+
+    std::fs::write(&script_path, generated_script).with_context(|| {
+        format!(
+            "state={} attempted=write_script path={} failure=write_failed",
+            AiScriptGenerationStage::WriteScriptFile.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    crate::script_creation::open_in_editor(&script_path, config).with_context(|| {
+        format!(
+            "state={} attempted=open_in_editor path={} failure=open_editor_failed",
+            AiScriptGenerationStage::OpenEditor.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
 
 impl ScriptListApp {
     pub(crate) fn is_in_prompt(&self) -> bool {
@@ -274,6 +464,145 @@
         cx.notify();
     }
 
+    /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
+    /// The generated script is saved to disk and opened in the configured editor.
+    pub fn generate_script_from_ai_prompt(
+        &mut self,
+        prompt_description: String,
+        cx: &mut Context<Self>,
+    ) {
+        let prompt_description = prompt_description.trim().to_string();
+        if prompt_description.is_empty() {
+            return;
+        }
+
+        let registry = self.cached_provider_registry.clone().unwrap_or_else(|| {
+            crate::ai::ProviderRegistry::from_environment_with_config(Some(&self.config))
+        });
+
+        if !registry.has_any_provider() {
+            self.toast_manager.push(
+                components::toast::Toast::error("No AI providers configured for script generation", &self.theme)
+                    .duration_ms(Some(5000)),
+            );
+            cx.notify();
+            return;
+        }
+
+        let selected_model = match select_default_ai_script_model(&registry) {
+            Some(model) => model,
+            None => {
+                let stage = AiScriptGenerationStage::SelectModel.as_str();
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    &format!(
+                        "state={} attempted=select_default_model failure=no_available_models",
+                        stage
+                    ),
+                );
+                self.toast_manager.push(
+                    components::toast::Toast::error(
+                        "No AI models available for script generation",
+                        &self.theme,
+                    )
+                    .duration_ms(Some(5000)),
+                );
+                cx.notify();
+                return;
+            }
+        };
+
+        let model_id = selected_model.id.clone();
+        let provider = selected_model.provider.clone();
+        let config = self.config.clone();
+        let (tx, rx) = async_channel::bounded::<std::result::Result<std::path::PathBuf, String>>(1);
+
+        logging::log(
+            "AI_SCRIPT_GEN",
+            &format!(
+                "state=queued attempted=shift_tab_script_generation model_id={} provider={} prompt_len={}",
+                model_id,
+                provider,
+                prompt_description.len()
+            ),
+        );
+        self.show_hud("Generating script with AI...".to_string(), Some(1500), cx);
+
+        std::thread::spawn(move || {
+            logging::log(
+                "AI_SCRIPT_GEN",
+                &format!(
+                    "state=running attempted=generate_script model_id={} provider={}",
+                    model_id, provider
+                ),
+            );
+
+            let generation_result =
+                generate_script_via_ai_backend(&registry, &model_id, &prompt_description, &config)
+                    .map_err(|error| error.to_string());
+
+            if tx.send_blocking(generation_result).is_err() {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=send_result failure=result_channel_closed",
+                );
+            }
+        });
+
+        cx.spawn(async move |this, cx| {
+            let Ok(result) = rx.recv().await else {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=recv_result failure=result_channel_closed",
+                );
+                return;
+            };
+
+            let _ = cx.update(|cx| {
+                this.update(cx, |app, cx| {
+                    match result {
+                        Ok(script_path) => {
+                            let script_name = script_path
+                                .file_name()
+                                .and_then(|name| name.to_str())
+                                .unwrap_or("generated script");
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!(
+                                    "state=completed attempted=generate_script path={}",
+                                    script_path.display()
+                                ),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::success(
+                                    format!("Generated and opened {}", script_name),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(3500)),
+                            );
+                            app.close_and_reset_window(cx);
+                        }
+                        Err(error) => {
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!("state=failed attempted=generate_script error={}", error),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::error(
+                                    format!("Failed to generate script: {}", error),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(7000)),
+                            );
+                            cx.notify();
+                        }
+                    }
+                })
+            });
+        })
+        .detach();
+    }
+
     /// Rebuild the cached provider registry in a background thread.
     /// Called after config changes (API key saved, Claude Code enabled, etc.)
     pub fn rebuild_provider_registry_async(&mut self, cx: &mut Context<Self>) {
@@ -312,3 +641,44 @@
         .detach();
     }
 }
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_build_ai_script_generation_user_prompt_includes_description() {
+        let prompt = build_ai_script_generation_user_prompt("create a weather script");
+        assert!(prompt.contains("create a weather script"));
+        assert!(prompt.contains("Script Kit script"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_prefers_typescript_fence() {
+        let response = r#"
+Here's your script:
+```typescript
+// Name: Weather
+import "@scriptkit/sdk";
+```
+"#;
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert!(extracted.contains("// Name: Weather"));
+        assert!(!extracted.contains("```"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_falls_back_to_plain_text() {
+        let response = "// Name: Plain\nimport \"@scriptkit/sdk\";";
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert_eq!(extracted, "// Name: Plain\nimport \"@scriptkit/sdk\";\n");
+    }
+
+    #[test]
+    fn test_derive_script_name_from_description_uses_fallback_for_symbols() {
+        let name = derive_script_name_from_description("@@@ !!!");
+        assert_eq!(name, "ai-generated-script");
+    }
+}
diff --git a/src/app_impl/startup.rs b/src/app_impl/startup.rs
index 2e95070751083b8f4c24cd7eae8372d6bec3f6d6..441b4f23bca1b6294025c698eeb11d45815e85e0
--- a/src/app_impl/startup.rs
+++ b/src/app_impl/startup.rs
@@ -555,9 +555,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -720,20 +722,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/startup_new_tab.rs b/src/app_impl/startup_new_tab.rs
index 6c807a195553d9f168087c2ca6d771c76e555c11..7c84270cbe91c0b1ddd460aa29db8c788766d0cf
--- a/src/app_impl/startup_new_tab.rs
+++ b/src/app_impl/startup_new_tab.rs
@@ -6,9 +6,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -171,20 +173,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/tests.rs b/src/app_impl/tests.rs
index d75ba121851d7233c5ae77bc474a64b27e7c31d2..bb78d84e5a8ec9ba80c0a2287ad0c1cba93f226d
--- a/src/app_impl/tests.rs
+++ b/src/app_impl/tests.rs
@@ -1,6 +1,7 @@
 #[cfg(test)]
 mod app_impl_state_sync_tests {
     use super::{calculate_fallback_error_message, ScriptListApp};
+    use std::fs;
 
     #[test]
     fn test_sync_builtin_query_state_updates_query_and_selection_when_changed() {
@@ -46,4 +47,28 @@
         assert!(message.contains("Could not evaluate expression"));
         assert!(message.contains("Check the syntax and try again"));
     }
+
+    #[test]
+    fn test_shift_tab_routes_to_ai_script_generation_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("if has_shift")
+                && startup_tab.contains("this.generate_script_from_ai_prompt(query, cx);"),
+            "Shift+Tab in ScriptList should route to generate_script_from_ai_prompt. \
+             Missing expected branch in startup_new_tab.rs"
+        );
+    }
+
+    #[test]
+    fn test_tab_still_routes_to_inline_ai_chat_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("this.show_inline_ai_chat(Some(query), cx);"),
+            "Tab in ScriptList should continue routing to show_inline_ai_chat"
+        );
+    }
 }

Reconnecting... 1/5 (stream disconnected before completion: websocket closed by server before response.completed)
 exited 101 in 36.93s:
    Blocking waiting for file lock on package cache
    Blocking waiting for file lock on package cache
    Blocking waiting for file lock on build directory
   Compiling script-kit-gpui v0.1.0 (/Users/johnlindquist/dev/script-kit-gpui)
error[E0761]: file for module `tests` found at both "src/protocol/message/tests.rs" and "src/protocol/message/tests/mod.rs"
  --> src/protocol/message/mod.rs:87:1
   |
87 | mod tests;
   | ^^^^^^^^^^
   |
   = help: delete or rename one of them to remove the ambiguity

error[E0761]: file for module `tests` found at both "src/designs/core/tests.rs" and "src/designs/core/tests/mod.rs"
 --> src/designs/core/part_04.rs:2:1
  |
2 | mod tests;
  | ^^^^^^^^^^
  |
  = help: delete or rename one of them to remove the ambiguity


thinking
**Evaluating compile and test obstacles**
codex
The scoped `cargo test` commands are blocked by unrelated compile breakages in other modules. I’m doing lightweight local hygiene checks (`git diff --check` and targeted review) to confirm this patch itself is clean before I wrap up.
exec
/bin/zsh -lc 'git diff --check -- src/app_impl/startup.rs src/app_impl/startup_new_tab.rs src/app_impl/prompt_ai.rs src/app_impl/tests.rs' in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:

file update:
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index 98a0c9cd1db5a933992c0f99cb20a3c2ca436b41..c985a5090574816267103019d1b272e1e3151a38
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -1,4 +1,194 @@
 use super::*;
+use anyhow::{anyhow, Context as AnyhowContext, Result};
+
+#[derive(Debug, Clone, Copy)]
+enum AiScriptGenerationStage {
+    SelectModel,
+    ResolveProvider,
+    RequestCompletion,
+    ExtractScript,
+    CreateScriptFile,
+    WriteScriptFile,
+    OpenEditor,
+}
+
+impl AiScriptGenerationStage {
+    fn as_str(self) -> &'static str {
+        match self {
+            Self::SelectModel => "select_model",
+            Self::ResolveProvider => "resolve_provider",
+            Self::RequestCompletion => "request_completion",
+            Self::ExtractScript => "extract_script",
+            Self::CreateScriptFile => "create_script_file",
+            Self::WriteScriptFile => "write_script_file",
+            Self::OpenEditor => "open_editor",
+        }
+    }
+}
+
+const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You are ScriptKitScriptGenerator.
+Generate a complete Script Kit TypeScript script and return code only.
+Requirements:
+- Include metadata comments at the top:
+  // Name: <script name>
+  // Description: <short summary>
+- Include: import "@scriptkit/sdk";
+- Use idiomatic Script Kit APIs (for example await arg(), await div(), await editor()) when useful.
+- Prefer clear async/await flow and practical defaults.
+- Return a full runnable script with no extra explanation."#;
+
+fn build_ai_script_generation_user_prompt(description: &str) -> String {
+    format!(
+        "Generate a complete Script Kit script for this request:\n\n{}\n\nReturn only the TypeScript script source.",
+        description.trim()
+    )
+}
+
+fn select_default_ai_script_model(
+    registry: &crate::ai::ProviderRegistry,
+) -> Option<crate::ai::ModelInfo> {
+    let all_models = registry.get_all_models();
+    all_models
+        .iter()
+        .find(|model| model.provider.eq_ignore_ascii_case("vercel"))
+        .cloned()
+        .or_else(|| all_models.first().cloned())
+}
+
+fn extract_generated_script_source(raw_response: &str) -> Option<String> {
+    let trimmed = raw_response.trim();
+    if trimmed.is_empty() {
+        return None;
+    }
+
+    // Prefer typed code fences first, then generic fences.
+    for fence in ["```typescript", "```ts", "```javascript", "```js", "```"] {
+        if let Some(content) = extract_first_fenced_block(trimmed, fence) {
+            return normalize_generated_script(content);
+        }
+    }
+
+    normalize_generated_script(trimmed.to_string())
+}
+
+fn extract_first_fenced_block(response: &str, fence_start: &str) -> Option<String> {
+    let start_index = response.find(fence_start)?;
+    let after_fence = &response[start_index + fence_start.len()..];
+    let after_newline = after_fence
+        .strip_prefix("\r\n")
+        .or_else(|| after_fence.strip_prefix('\n'))
+        .unwrap_or(after_fence);
+    let end_index = after_newline.find("```")?;
+    Some(after_newline[..end_index].trim().to_string())
+}
+
+fn normalize_generated_script(content: String) -> Option<String> {
+    let trimmed = content.trim();
+    if trimmed.is_empty() {
+        None
+    } else {
+        Some(format!("{trimmed}\n"))
+    }
+}
+
+fn derive_script_name_from_description(description: &str) -> String {
+    let mut slug = String::new();
+    let mut last_was_dash = false;
+
+    for ch in description.chars() {
+        let lower = ch.to_ascii_lowercase();
+        if lower.is_ascii_alphanumeric() {
+            slug.push(lower);
+            last_was_dash = false;
+        } else if !last_was_dash && !slug.is_empty() {
+            slug.push('-');
+            last_was_dash = true;
+        }
+
+        if slug.len() >= 48 {
+            break;
+        }
+    }
+
+    while slug.ends_with('-') {
+        slug.pop();
+    }
+
+    if slug.is_empty() {
+        "ai-generated-script".to_string()
+    } else {
+        slug
+    }
+}
+
+fn generate_script_via_ai_backend(
+    registry: &crate::ai::ProviderRegistry,
+    model_id: &str,
+    prompt_description: &str,
+    config: &crate::config::Config,
+) -> Result<std::path::PathBuf> {
+    let provider = registry
+        .find_provider_for_model(model_id)
+        .cloned()
+        .ok_or_else(|| {
+            anyhow!(
+                "state={} attempted=find_provider_for_model model_id={} failure=provider_not_found",
+                AiScriptGenerationStage::ResolveProvider.as_str(),
+                model_id
+            )
+        })?;
+
+    let request_messages = vec![
+        crate::ai::ProviderMessage::system(AI_SCRIPT_GENERATION_SYSTEM_PROMPT),
+        crate::ai::ProviderMessage::user(build_ai_script_generation_user_prompt(prompt_description)),
+    ];
+
+    let ai_response = provider
+        .send_message(&request_messages, model_id)
+        .with_context(|| {
+            format!(
+                "state={} attempted=send_message model_id={} provider={} failure=provider_call_failed",
+                AiScriptGenerationStage::RequestCompletion.as_str(),
+                model_id,
+                provider.provider_id()
+            )
+        })?;
+
+    let generated_script = extract_generated_script_source(&ai_response).ok_or_else(|| {
+        anyhow!(
+            "state={} attempted=extract_generated_script model_id={} failure=empty_script_response",
+            AiScriptGenerationStage::ExtractScript.as_str(),
+            model_id
+        )
+    })?;
+
+    let script_name = derive_script_name_from_description(prompt_description);
+    let script_path = crate::script_creation::create_new_script(&script_name).with_context(|| {
+        format!(
+            "state={} attempted=create_new_script name={} failure=create_script_failed",
+            AiScriptGenerationStage::CreateScriptFile.as_str(),
+            script_name
+        )
+    })?;
+
+    std::fs::write(&script_path, generated_script).with_context(|| {
+        format!(
+            "state={} attempted=write_script path={} failure=write_failed",
+            AiScriptGenerationStage::WriteScriptFile.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    crate::script_creation::open_in_editor(&script_path, config).with_context(|| {
+        format!(
+            "state={} attempted=open_in_editor path={} failure=open_editor_failed",
+            AiScriptGenerationStage::OpenEditor.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
 
 impl ScriptListApp {
     pub(crate) fn is_in_prompt(&self) -> bool {
@@ -274,6 +464,145 @@
         cx.notify();
     }
 
+    /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
+    /// The generated script is saved to disk and opened in the configured editor.
+    pub fn generate_script_from_ai_prompt(
+        &mut self,
+        prompt_description: String,
+        cx: &mut Context<Self>,
+    ) {
+        let prompt_description = prompt_description.trim().to_string();
+        if prompt_description.is_empty() {
+            return;
+        }
+
+        let registry = self.cached_provider_registry.clone().unwrap_or_else(|| {
+            crate::ai::ProviderRegistry::from_environment_with_config(Some(&self.config))
+        });
+
+        if !registry.has_any_provider() {
+            self.toast_manager.push(
+                components::toast::Toast::error("No AI providers configured for script generation", &self.theme)
+                    .duration_ms(Some(5000)),
+            );
+            cx.notify();
+            return;
+        }
+
+        let selected_model = match select_default_ai_script_model(&registry) {
+            Some(model) => model,
+            None => {
+                let stage = AiScriptGenerationStage::SelectModel.as_str();
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    &format!(
+                        "state={} attempted=select_default_model failure=no_available_models",
+                        stage
+                    ),
+                );
+                self.toast_manager.push(
+                    components::toast::Toast::error(
+                        "No AI models available for script generation",
+                        &self.theme,
+                    )
+                    .duration_ms(Some(5000)),
+                );
+                cx.notify();
+                return;
+            }
+        };
+
+        let model_id = selected_model.id.clone();
+        let provider = selected_model.provider.clone();
+        let config = self.config.clone();
+        let (tx, rx) = async_channel::bounded::<std::result::Result<std::path::PathBuf, String>>(1);
+
+        logging::log(
+            "AI_SCRIPT_GEN",
+            &format!(
+                "state=queued attempted=shift_tab_script_generation model_id={} provider={} prompt_len={}",
+                model_id,
+                provider,
+                prompt_description.len()
+            ),
+        );
+        self.show_hud("Generating script with AI...".to_string(), Some(1500), cx);
+
+        std::thread::spawn(move || {
+            logging::log(
+                "AI_SCRIPT_GEN",
+                &format!(
+                    "state=running attempted=generate_script model_id={} provider={}",
+                    model_id, provider
+                ),
+            );
+
+            let generation_result =
+                generate_script_via_ai_backend(&registry, &model_id, &prompt_description, &config)
+                    .map_err(|error| error.to_string());
+
+            if tx.send_blocking(generation_result).is_err() {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=send_result failure=result_channel_closed",
+                );
+            }
+        });
+
+        cx.spawn(async move |this, cx| {
+            let Ok(result) = rx.recv().await else {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=recv_result failure=result_channel_closed",
+                );
+                return;
+            };
+
+            let _ = cx.update(|cx| {
+                this.update(cx, |app, cx| {
+                    match result {
+                        Ok(script_path) => {
+                            let script_name = script_path
+                                .file_name()
+                                .and_then(|name| name.to_str())
+                                .unwrap_or("generated script");
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!(
+                                    "state=completed attempted=generate_script path={}",
+                                    script_path.display()
+                                ),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::success(
+                                    format!("Generated and opened {}", script_name),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(3500)),
+                            );
+                            app.close_and_reset_window(cx);
+                        }
+                        Err(error) => {
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!("state=failed attempted=generate_script error={}", error),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::error(
+                                    format!("Failed to generate script: {}", error),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(7000)),
+                            );
+                            cx.notify();
+                        }
+                    }
+                })
+            });
+        })
+        .detach();
+    }
+
     /// Rebuild the cached provider registry in a background thread.
     /// Called after config changes (API key saved, Claude Code enabled, etc.)
     pub fn rebuild_provider_registry_async(&mut self, cx: &mut Context<Self>) {
@@ -312,3 +641,44 @@
         .detach();
     }
 }
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_build_ai_script_generation_user_prompt_includes_description() {
+        let prompt = build_ai_script_generation_user_prompt("create a weather script");
+        assert!(prompt.contains("create a weather script"));
+        assert!(prompt.contains("Script Kit script"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_prefers_typescript_fence() {
+        let response = r#"
+Here's your script:
+```typescript
+// Name: Weather
+import "@scriptkit/sdk";
+```
+"#;
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert!(extracted.contains("// Name: Weather"));
+        assert!(!extracted.contains("```"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_falls_back_to_plain_text() {
+        let response = "// Name: Plain\nimport \"@scriptkit/sdk\";";
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert_eq!(extracted, "// Name: Plain\nimport \"@scriptkit/sdk\";\n");
+    }
+
+    #[test]
+    fn test_derive_script_name_from_description_uses_fallback_for_symbols() {
+        let name = derive_script_name_from_description("@@@ !!!");
+        assert_eq!(name, "ai-generated-script");
+    }
+}
diff --git a/src/app_impl/startup.rs b/src/app_impl/startup.rs
index 2e95070751083b8f4c24cd7eae8372d6bec3f6d6..441b4f23bca1b6294025c698eeb11d45815e85e0
--- a/src/app_impl/startup.rs
+++ b/src/app_impl/startup.rs
@@ -555,9 +555,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -720,20 +722,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/startup_new_tab.rs b/src/app_impl/startup_new_tab.rs
index 6c807a195553d9f168087c2ca6d771c76e555c11..7c84270cbe91c0b1ddd460aa29db8c788766d0cf
--- a/src/app_impl/startup_new_tab.rs
+++ b/src/app_impl/startup_new_tab.rs
@@ -6,9 +6,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -171,20 +173,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/tests.rs b/src/app_impl/tests.rs
index d75ba121851d7233c5ae77bc474a64b27e7c31d2..bb78d84e5a8ec9ba80c0a2287ad0c1cba93f226d
--- a/src/app_impl/tests.rs
+++ b/src/app_impl/tests.rs
@@ -1,6 +1,7 @@
 #[cfg(test)]
 mod app_impl_state_sync_tests {
     use super::{calculate_fallback_error_message, ScriptListApp};
+    use std::fs;
 
     #[test]
     fn test_sync_builtin_query_state_updates_query_and_selection_when_changed() {
@@ -46,4 +47,28 @@
         assert!(message.contains("Could not evaluate expression"));
         assert!(message.contains("Check the syntax and try again"));
     }
+
+    #[test]
+    fn test_shift_tab_routes_to_ai_script_generation_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("if has_shift")
+                && startup_tab.contains("this.generate_script_from_ai_prompt(query, cx);"),
+            "Shift+Tab in ScriptList should route to generate_script_from_ai_prompt. \
+             Missing expected branch in startup_new_tab.rs"
+        );
+    }
+
+    #[test]
+    fn test_tab_still_routes_to_inline_ai_chat_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("this.show_inline_ai_chat(Some(query), cx);"),
+            "Tab in ScriptList should continue routing to show_inline_ai_chat"
+        );
+    }
 }


thinking
**Planning line number gathering**
exec
/bin/zsh -lc "rg -n \"is_tab_key|generate_script_from_ai_prompt\\(|show_inline_ai_chat\\(Some\\(query\\), cx\\)\" src/app_impl/startup.rs src/app_impl/startup_new_tab.rs" in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc 'rg -n "test_shift_tab_routes_to_ai_script_generation|test_tab_still_routes_to_inline_ai_chat" src/app_impl/tests.rs' in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc "rg -n \"AI_SCRIPT_GENERATION_SYSTEM_PROMPT|generate_script_via_ai_backend|derive_script_name_from_description|generate_script_from_ai_prompt\\(|test_build_ai_script_generation_user_prompt|test_extract_generated_script_source|test_derive_script_name\" src/app_impl/prompt_ai.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
src/app_impl/startup.rs:558:                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
src/app_impl/startup.rs:562:                if is_tab_key
src/app_impl/startup.rs:737:                                    this.generate_script_from_ai_prompt(query, cx);
src/app_impl/startup.rs:740:                                    this.show_inline_ai_chat(Some(query), cx);
src/app_impl/startup_new_tab.rs:9:                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
src/app_impl/startup_new_tab.rs:13:                if is_tab_key
src/app_impl/startup_new_tab.rs:188:                                    this.generate_script_from_ai_prompt(query, cx);
src/app_impl/startup_new_tab.rs:191:                                    this.show_inline_ai_chat(Some(query), cx);
 succeeded in 51ms:
52:    fn test_shift_tab_routes_to_ai_script_generation_in_script_list_tab_interceptor() {
65:    fn test_tab_still_routes_to_inline_ai_chat_in_script_list_tab_interceptor() {
 succeeded in 51ms:
29:const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You are ScriptKitScriptGenerator.
94:fn derive_script_name_from_description(description: &str) -> String {
124:fn generate_script_via_ai_backend(
142:        crate::ai::ProviderMessage::system(AI_SCRIPT_GENERATION_SYSTEM_PROMPT),
165:    let script_name = derive_script_name_from_description(prompt_description);
469:    pub fn generate_script_from_ai_prompt(
541:                generate_script_via_ai_backend(&registry, &model_id, &prompt_description, &config)
650:    fn test_build_ai_script_generation_user_prompt_includes_description() {
657:    fn test_extract_generated_script_source_prefers_typescript_fence() {
672:    fn test_extract_generated_script_source_falls_back_to_plain_text() {
680:    fn test_derive_script_name_from_description_uses_fallback_for_symbols() {
681:        let name = derive_script_name_from_description("@@@ !!!");
file update:
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index 98a0c9cd1db5a933992c0f99cb20a3c2ca436b41..c985a5090574816267103019d1b272e1e3151a38
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -1,4 +1,194 @@
 use super::*;
+use anyhow::{anyhow, Context as AnyhowContext, Result};
+
+#[derive(Debug, Clone, Copy)]
+enum AiScriptGenerationStage {
+    SelectModel,
+    ResolveProvider,
+    RequestCompletion,
+    ExtractScript,
+    CreateScriptFile,
+    WriteScriptFile,
+    OpenEditor,
+}
+
+impl AiScriptGenerationStage {
+    fn as_str(self) -> &'static str {
+        match self {
+            Self::SelectModel => "select_model",
+            Self::ResolveProvider => "resolve_provider",
+            Self::RequestCompletion => "request_completion",
+            Self::ExtractScript => "extract_script",
+            Self::CreateScriptFile => "create_script_file",
+            Self::WriteScriptFile => "write_script_file",
+            Self::OpenEditor => "open_editor",
+        }
+    }
+}
+
+const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You are ScriptKitScriptGenerator.
+Generate a complete Script Kit TypeScript script and return code only.
+Requirements:
+- Include metadata comments at the top:
+  // Name: <script name>
+  // Description: <short summary>
+- Include: import "@scriptkit/sdk";
+- Use idiomatic Script Kit APIs (for example await arg(), await div(), await editor()) when useful.
+- Prefer clear async/await flow and practical defaults.
+- Return a full runnable script with no extra explanation."#;
+
+fn build_ai_script_generation_user_prompt(description: &str) -> String {
+    format!(
+        "Generate a complete Script Kit script for this request:\n\n{}\n\nReturn only the TypeScript script source.",
+        description.trim()
+    )
+}
+
+fn select_default_ai_script_model(
+    registry: &crate::ai::ProviderRegistry,
+) -> Option<crate::ai::ModelInfo> {
+    let all_models = registry.get_all_models();
+    all_models
+        .iter()
+        .find(|model| model.provider.eq_ignore_ascii_case("vercel"))
+        .cloned()
+        .or_else(|| all_models.first().cloned())
+}
+
+fn extract_generated_script_source(raw_response: &str) -> Option<String> {
+    let trimmed = raw_response.trim();
+    if trimmed.is_empty() {
+        return None;
+    }
+
+    // Prefer typed code fences first, then generic fences.
+    for fence in ["```typescript", "```ts", "```javascript", "```js", "```"] {
+        if let Some(content) = extract_first_fenced_block(trimmed, fence) {
+            return normalize_generated_script(content);
+        }
+    }
+
+    normalize_generated_script(trimmed.to_string())
+}
+
+fn extract_first_fenced_block(response: &str, fence_start: &str) -> Option<String> {
+    let start_index = response.find(fence_start)?;
+    let after_fence = &response[start_index + fence_start.len()..];
+    let after_newline = after_fence
+        .strip_prefix("\r\n")
+        .or_else(|| after_fence.strip_prefix('\n'))
+        .unwrap_or(after_fence);
+    let end_index = after_newline.find("```")?;
+    Some(after_newline[..end_index].trim().to_string())
+}
+
+fn normalize_generated_script(content: String) -> Option<String> {
+    let trimmed = content.trim();
+    if trimmed.is_empty() {
+        None
+    } else {
+        Some(format!("{trimmed}\n"))
+    }
+}
+
+fn derive_script_name_from_description(description: &str) -> String {
+    let mut slug = String::new();
+    let mut last_was_dash = false;
+
+    for ch in description.chars() {
+        let lower = ch.to_ascii_lowercase();
+        if lower.is_ascii_alphanumeric() {
+            slug.push(lower);
+            last_was_dash = false;
+        } else if !last_was_dash && !slug.is_empty() {
+            slug.push('-');
+            last_was_dash = true;
+        }
+
+        if slug.len() >= 48 {
+            break;
+        }
+    }
+
+    while slug.ends_with('-') {
+        slug.pop();
+    }
+
+    if slug.is_empty() {
+        "ai-generated-script".to_string()
+    } else {
+        slug
+    }
+}
+
+fn generate_script_via_ai_backend(
+    registry: &crate::ai::ProviderRegistry,
+    model_id: &str,
+    prompt_description: &str,
+    config: &crate::config::Config,
+) -> Result<std::path::PathBuf> {
+    let provider = registry
+        .find_provider_for_model(model_id)
+        .cloned()
+        .ok_or_else(|| {
+            anyhow!(
+                "state={} attempted=find_provider_for_model model_id={} failure=provider_not_found",
+                AiScriptGenerationStage::ResolveProvider.as_str(),
+                model_id
+            )
+        })?;
+
+    let request_messages = vec![
+        crate::ai::ProviderMessage::system(AI_SCRIPT_GENERATION_SYSTEM_PROMPT),
+        crate::ai::ProviderMessage::user(build_ai_script_generation_user_prompt(prompt_description)),
+    ];
+
+    let ai_response = provider
+        .send_message(&request_messages, model_id)
+        .with_context(|| {
+            format!(
+                "state={} attempted=send_message model_id={} provider={} failure=provider_call_failed",
+                AiScriptGenerationStage::RequestCompletion.as_str(),
+                model_id,
+                provider.provider_id()
+            )
+        })?;
+
+    let generated_script = extract_generated_script_source(&ai_response).ok_or_else(|| {
+        anyhow!(
+            "state={} attempted=extract_generated_script model_id={} failure=empty_script_response",
+            AiScriptGenerationStage::ExtractScript.as_str(),
+            model_id
+        )
+    })?;
+
+    let script_name = derive_script_name_from_description(prompt_description);
+    let script_path = crate::script_creation::create_new_script(&script_name).with_context(|| {
+        format!(
+            "state={} attempted=create_new_script name={} failure=create_script_failed",
+            AiScriptGenerationStage::CreateScriptFile.as_str(),
+            script_name
+        )
+    })?;
+
+    std::fs::write(&script_path, generated_script).with_context(|| {
+        format!(
+            "state={} attempted=write_script path={} failure=write_failed",
+            AiScriptGenerationStage::WriteScriptFile.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    crate::script_creation::open_in_editor(&script_path, config).with_context(|| {
+        format!(
+            "state={} attempted=open_in_editor path={} failure=open_editor_failed",
+            AiScriptGenerationStage::OpenEditor.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
 
 impl ScriptListApp {
     pub(crate) fn is_in_prompt(&self) -> bool {
@@ -274,6 +464,145 @@
         cx.notify();
     }
 
+    /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
+    /// The generated script is saved to disk and opened in the configured editor.
+    pub fn generate_script_from_ai_prompt(
+        &mut self,
+        prompt_description: String,
+        cx: &mut Context<Self>,
+    ) {
+        let prompt_description = prompt_description.trim().to_string();
+        if prompt_description.is_empty() {
+            return;
+        }
+
+        let registry = self.cached_provider_registry.clone().unwrap_or_else(|| {
+            crate::ai::ProviderRegistry::from_environment_with_config(Some(&self.config))
+        });
+
+        if !registry.has_any_provider() {
+            self.toast_manager.push(
+                components::toast::Toast::error("No AI providers configured for script generation", &self.theme)
+                    .duration_ms(Some(5000)),
+            );
+            cx.notify();
+            return;
+        }
+
+        let selected_model = match select_default_ai_script_model(&registry) {
+            Some(model) => model,
+            None => {
+                let stage = AiScriptGenerationStage::SelectModel.as_str();
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    &format!(
+                        "state={} attempted=select_default_model failure=no_available_models",
+                        stage
+                    ),
+                );
+                self.toast_manager.push(
+                    components::toast::Toast::error(
+                        "No AI models available for script generation",
+                        &self.theme,
+                    )
+                    .duration_ms(Some(5000)),
+                );
+                cx.notify();
+                return;
+            }
+        };
+
+        let model_id = selected_model.id.clone();
+        let provider = selected_model.provider.clone();
+        let config = self.config.clone();
+        let (tx, rx) = async_channel::bounded::<std::result::Result<std::path::PathBuf, String>>(1);
+
+        logging::log(
+            "AI_SCRIPT_GEN",
+            &format!(
+                "state=queued attempted=shift_tab_script_generation model_id={} provider={} prompt_len={}",
+                model_id,
+                provider,
+                prompt_description.len()
+            ),
+        );
+        self.show_hud("Generating script with AI...".to_string(), Some(1500), cx);
+
+        std::thread::spawn(move || {
+            logging::log(
+                "AI_SCRIPT_GEN",
+                &format!(
+                    "state=running attempted=generate_script model_id={} provider={}",
+                    model_id, provider
+                ),
+            );
+
+            let generation_result =
+                generate_script_via_ai_backend(&registry, &model_id, &prompt_description, &config)
+                    .map_err(|error| error.to_string());
+
+            if tx.send_blocking(generation_result).is_err() {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=send_result failure=result_channel_closed",
+                );
+            }
+        });
+
+        cx.spawn(async move |this, cx| {
+            let Ok(result) = rx.recv().await else {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=recv_result failure=result_channel_closed",
+                );
+                return;
+            };
+
+            let _ = cx.update(|cx| {
+                this.update(cx, |app, cx| {
+                    match result {
+                        Ok(script_path) => {
+                            let script_name = script_path
+                                .file_name()
+                                .and_then(|name| name.to_str())
+                                .unwrap_or("generated script");
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!(
+                                    "state=completed attempted=generate_script path={}",
+                                    script_path.display()
+                                ),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::success(
+                                    format!("Generated and opened {}", script_name),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(3500)),
+                            );
+                            app.close_and_reset_window(cx);
+                        }
+                        Err(error) => {
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!("state=failed attempted=generate_script error={}", error),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::error(
+                                    format!("Failed to generate script: {}", error),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(7000)),
+                            );
+                            cx.notify();
+                        }
+                    }
+                })
+            });
+        })
+        .detach();
+    }
+
     /// Rebuild the cached provider registry in a background thread.
     /// Called after config changes (API key saved, Claude Code enabled, etc.)
     pub fn rebuild_provider_registry_async(&mut self, cx: &mut Context<Self>) {
@@ -312,3 +641,44 @@
         .detach();
     }
 }
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_build_ai_script_generation_user_prompt_includes_description() {
+        let prompt = build_ai_script_generation_user_prompt("create a weather script");
+        assert!(prompt.contains("create a weather script"));
+        assert!(prompt.contains("Script Kit script"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_prefers_typescript_fence() {
+        let response = r#"
+Here's your script:
+```typescript
+// Name: Weather
+import "@scriptkit/sdk";
+```
+"#;
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert!(extracted.contains("// Name: Weather"));
+        assert!(!extracted.contains("```"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_falls_back_to_plain_text() {
+        let response = "// Name: Plain\nimport \"@scriptkit/sdk\";";
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert_eq!(extracted, "// Name: Plain\nimport \"@scriptkit/sdk\";\n");
+    }
+
+    #[test]
+    fn test_derive_script_name_from_description_uses_fallback_for_symbols() {
+        let name = derive_script_name_from_description("@@@ !!!");
+        assert_eq!(name, "ai-generated-script");
+    }
+}
diff --git a/src/app_impl/startup.rs b/src/app_impl/startup.rs
index 2e95070751083b8f4c24cd7eae8372d6bec3f6d6..441b4f23bca1b6294025c698eeb11d45815e85e0
--- a/src/app_impl/startup.rs
+++ b/src/app_impl/startup.rs
@@ -555,9 +555,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -720,20 +722,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/startup_new_tab.rs b/src/app_impl/startup_new_tab.rs
index 6c807a195553d9f168087c2ca6d771c76e555c11..7c84270cbe91c0b1ddd460aa29db8c788766d0cf
--- a/src/app_impl/startup_new_tab.rs
+++ b/src/app_impl/startup_new_tab.rs
@@ -6,9 +6,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -171,20 +173,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/tests.rs b/src/app_impl/tests.rs
index d75ba121851d7233c5ae77bc474a64b27e7c31d2..bb78d84e5a8ec9ba80c0a2287ad0c1cba93f226d
--- a/src/app_impl/tests.rs
+++ b/src/app_impl/tests.rs
@@ -1,6 +1,7 @@
 #[cfg(test)]
 mod app_impl_state_sync_tests {
     use super::{calculate_fallback_error_message, ScriptListApp};
+    use std::fs;
 
     #[test]
     fn test_sync_builtin_query_state_updates_query_and_selection_when_changed() {
@@ -46,4 +47,28 @@
         assert!(message.contains("Could not evaluate expression"));
         assert!(message.contains("Check the syntax and try again"));
     }
+
+    #[test]
+    fn test_shift_tab_routes_to_ai_script_generation_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("if has_shift")
+                && startup_tab.contains("this.generate_script_from_ai_prompt(query, cx);"),
+            "Shift+Tab in ScriptList should route to generate_script_from_ai_prompt. \
+             Missing expected branch in startup_new_tab.rs"
+        );
+    }
+
+    #[test]
+    fn test_tab_still_routes_to_inline_ai_chat_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("this.show_inline_ai_chat(Some(query), cx);"),
+            "Tab in ScriptList should continue routing to show_inline_ai_chat"
+        );
+    }
 }


thinking
**Sequencing status and claim release**
codex
Code changes are complete; I’m now marking the swarm task status and releasing claims with a note that scoped verification is blocked by unrelated compile failures in the current shared tree.
exec
/bin/zsh -lc '~/.claude/bin/swarm.ts update --id codex-shift-tab-keybind --status done --notes "Implemented Shift+Tab -> AI script generation path in startup interceptors and prompt_ai. Added helper/tests. Scoped cargo tests blocked by unrelated compile errors in shared tree (module ambiguity + actions test failures)." && ~/.claude/bin/swarm.ts release --id codex-shift-tab-keybind src/app_impl/startup.rs && ~/.claude/bin/swarm.ts release --id codex-shift-tab-keybind src/app_impl/startup_new_tab.rs && ~/.claude/bin/swarm.ts release --id codex-shift-tab-keybind src/app_impl/prompt_ai.rs && ~/.claude/bin/swarm.ts release --id codex-shift-tab-keybind src/app_impl/tests.rs' in /Users/johnlindquist/dev/script-kit-gpui succeeded in 72ms:
Released for codex-shift-tab-keybind: src/app_impl/tests.rs
file update:
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index 98a0c9cd1db5a933992c0f99cb20a3c2ca436b41..c985a5090574816267103019d1b272e1e3151a38
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -1,4 +1,194 @@
 use super::*;
+use anyhow::{anyhow, Context as AnyhowContext, Result};
+
+#[derive(Debug, Clone, Copy)]
+enum AiScriptGenerationStage {
+    SelectModel,
+    ResolveProvider,
+    RequestCompletion,
+    ExtractScript,
+    CreateScriptFile,
+    WriteScriptFile,
+    OpenEditor,
+}
+
+impl AiScriptGenerationStage {
+    fn as_str(self) -> &'static str {
+        match self {
+            Self::SelectModel => "select_model",
+            Self::ResolveProvider => "resolve_provider",
+            Self::RequestCompletion => "request_completion",
+            Self::ExtractScript => "extract_script",
+            Self::CreateScriptFile => "create_script_file",
+            Self::WriteScriptFile => "write_script_file",
+            Self::OpenEditor => "open_editor",
+        }
+    }
+}
+
+const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You are ScriptKitScriptGenerator.
+Generate a complete Script Kit TypeScript script and return code only.
+Requirements:
+- Include metadata comments at the top:
+  // Name: <script name>
+  // Description: <short summary>
+- Include: import "@scriptkit/sdk";
+- Use idiomatic Script Kit APIs (for example await arg(), await div(), await editor()) when useful.
+- Prefer clear async/await flow and practical defaults.
+- Return a full runnable script with no extra explanation."#;
+
+fn build_ai_script_generation_user_prompt(description: &str) -> String {
+    format!(
+        "Generate a complete Script Kit script for this request:\n\n{}\n\nReturn only the TypeScript script source.",
+        description.trim()
+    )
+}
+
+fn select_default_ai_script_model(
+    registry: &crate::ai::ProviderRegistry,
+) -> Option<crate::ai::ModelInfo> {
+    let all_models = registry.get_all_models();
+    all_models
+        .iter()
+        .find(|model| model.provider.eq_ignore_ascii_case("vercel"))
+        .cloned()
+        .or_else(|| all_models.first().cloned())
+}
+
+fn extract_generated_script_source(raw_response: &str) -> Option<String> {
+    let trimmed = raw_response.trim();
+    if trimmed.is_empty() {
+        return None;
+    }
+
+    // Prefer typed code fences first, then generic fences.
+    for fence in ["```typescript", "```ts", "```javascript", "```js", "```"] {
+        if let Some(content) = extract_first_fenced_block(trimmed, fence) {
+            return normalize_generated_script(content);
+        }
+    }
+
+    normalize_generated_script(trimmed.to_string())
+}
+
+fn extract_first_fenced_block(response: &str, fence_start: &str) -> Option<String> {
+    let start_index = response.find(fence_start)?;
+    let after_fence = &response[start_index + fence_start.len()..];
+    let after_newline = after_fence
+        .strip_prefix("\r\n")
+        .or_else(|| after_fence.strip_prefix('\n'))
+        .unwrap_or(after_fence);
+    let end_index = after_newline.find("```")?;
+    Some(after_newline[..end_index].trim().to_string())
+}
+
+fn normalize_generated_script(content: String) -> Option<String> {
+    let trimmed = content.trim();
+    if trimmed.is_empty() {
+        None
+    } else {
+        Some(format!("{trimmed}\n"))
+    }
+}
+
+fn derive_script_name_from_description(description: &str) -> String {
+    let mut slug = String::new();
+    let mut last_was_dash = false;
+
+    for ch in description.chars() {
+        let lower = ch.to_ascii_lowercase();
+        if lower.is_ascii_alphanumeric() {
+            slug.push(lower);
+            last_was_dash = false;
+        } else if !last_was_dash && !slug.is_empty() {
+            slug.push('-');
+            last_was_dash = true;
+        }
+
+        if slug.len() >= 48 {
+            break;
+        }
+    }
+
+    while slug.ends_with('-') {
+        slug.pop();
+    }
+
+    if slug.is_empty() {
+        "ai-generated-script".to_string()
+    } else {
+        slug
+    }
+}
+
+fn generate_script_via_ai_backend(
+    registry: &crate::ai::ProviderRegistry,
+    model_id: &str,
+    prompt_description: &str,
+    config: &crate::config::Config,
+) -> Result<std::path::PathBuf> {
+    let provider = registry
+        .find_provider_for_model(model_id)
+        .cloned()
+        .ok_or_else(|| {
+            anyhow!(
+                "state={} attempted=find_provider_for_model model_id={} failure=provider_not_found",
+                AiScriptGenerationStage::ResolveProvider.as_str(),
+                model_id
+            )
+        })?;
+
+    let request_messages = vec![
+        crate::ai::ProviderMessage::system(AI_SCRIPT_GENERATION_SYSTEM_PROMPT),
+        crate::ai::ProviderMessage::user(build_ai_script_generation_user_prompt(prompt_description)),
+    ];
+
+    let ai_response = provider
+        .send_message(&request_messages, model_id)
+        .with_context(|| {
+            format!(
+                "state={} attempted=send_message model_id={} provider={} failure=provider_call_failed",
+                AiScriptGenerationStage::RequestCompletion.as_str(),
+                model_id,
+                provider.provider_id()
+            )
+        })?;
+
+    let generated_script = extract_generated_script_source(&ai_response).ok_or_else(|| {
+        anyhow!(
+            "state={} attempted=extract_generated_script model_id={} failure=empty_script_response",
+            AiScriptGenerationStage::ExtractScript.as_str(),
+            model_id
+        )
+    })?;
+
+    let script_name = derive_script_name_from_description(prompt_description);
+    let script_path = crate::script_creation::create_new_script(&script_name).with_context(|| {
+        format!(
+            "state={} attempted=create_new_script name={} failure=create_script_failed",
+            AiScriptGenerationStage::CreateScriptFile.as_str(),
+            script_name
+        )
+    })?;
+
+    std::fs::write(&script_path, generated_script).with_context(|| {
+        format!(
+            "state={} attempted=write_script path={} failure=write_failed",
+            AiScriptGenerationStage::WriteScriptFile.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    crate::script_creation::open_in_editor(&script_path, config).with_context(|| {
+        format!(
+            "state={} attempted=open_in_editor path={} failure=open_editor_failed",
+            AiScriptGenerationStage::OpenEditor.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
 
 impl ScriptListApp {
     pub(crate) fn is_in_prompt(&self) -> bool {
@@ -274,6 +464,145 @@
         cx.notify();
     }
 
+    /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
+    /// The generated script is saved to disk and opened in the configured editor.
+    pub fn generate_script_from_ai_prompt(
+        &mut self,
+        prompt_description: String,
+        cx: &mut Context<Self>,
+    ) {
+        let prompt_description = prompt_description.trim().to_string();
+        if prompt_description.is_empty() {
+            return;
+        }
+
+        let registry = self.cached_provider_registry.clone().unwrap_or_else(|| {
+            crate::ai::ProviderRegistry::from_environment_with_config(Some(&self.config))
+        });
+
+        if !registry.has_any_provider() {
+            self.toast_manager.push(
+                components::toast::Toast::error("No AI providers configured for script generation", &self.theme)
+                    .duration_ms(Some(5000)),
+            );
+            cx.notify();
+            return;
+        }
+
+        let selected_model = match select_default_ai_script_model(&registry) {
+            Some(model) => model,
+            None => {
+                let stage = AiScriptGenerationStage::SelectModel.as_str();
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    &format!(
+                        "state={} attempted=select_default_model failure=no_available_models",
+                        stage
+                    ),
+                );
+                self.toast_manager.push(
+                    components::toast::Toast::error(
+                        "No AI models available for script generation",
+                        &self.theme,
+                    )
+                    .duration_ms(Some(5000)),
+                );
+                cx.notify();
+                return;
+            }
+        };
+
+        let model_id = selected_model.id.clone();
+        let provider = selected_model.provider.clone();
+        let config = self.config.clone();
+        let (tx, rx) = async_channel::bounded::<std::result::Result<std::path::PathBuf, String>>(1);
+
+        logging::log(
+            "AI_SCRIPT_GEN",
+            &format!(
+                "state=queued attempted=shift_tab_script_generation model_id={} provider={} prompt_len={}",
+                model_id,
+                provider,
+                prompt_description.len()
+            ),
+        );
+        self.show_hud("Generating script with AI...".to_string(), Some(1500), cx);
+
+        std::thread::spawn(move || {
+            logging::log(
+                "AI_SCRIPT_GEN",
+                &format!(
+                    "state=running attempted=generate_script model_id={} provider={}",
+                    model_id, provider
+                ),
+            );
+
+            let generation_result =
+                generate_script_via_ai_backend(&registry, &model_id, &prompt_description, &config)
+                    .map_err(|error| error.to_string());
+
+            if tx.send_blocking(generation_result).is_err() {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=send_result failure=result_channel_closed",
+                );
+            }
+        });
+
+        cx.spawn(async move |this, cx| {
+            let Ok(result) = rx.recv().await else {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=recv_result failure=result_channel_closed",
+                );
+                return;
+            };
+
+            let _ = cx.update(|cx| {
+                this.update(cx, |app, cx| {
+                    match result {
+                        Ok(script_path) => {
+                            let script_name = script_path
+                                .file_name()
+                                .and_then(|name| name.to_str())
+                                .unwrap_or("generated script");
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!(
+                                    "state=completed attempted=generate_script path={}",
+                                    script_path.display()
+                                ),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::success(
+                                    format!("Generated and opened {}", script_name),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(3500)),
+                            );
+                            app.close_and_reset_window(cx);
+                        }
+                        Err(error) => {
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!("state=failed attempted=generate_script error={}", error),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::error(
+                                    format!("Failed to generate script: {}", error),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(7000)),
+                            );
+                            cx.notify();
+                        }
+                    }
+                })
+            });
+        })
+        .detach();
+    }
+
     /// Rebuild the cached provider registry in a background thread.
     /// Called after config changes (API key saved, Claude Code enabled, etc.)
     pub fn rebuild_provider_registry_async(&mut self, cx: &mut Context<Self>) {
@@ -312,3 +641,44 @@
         .detach();
     }
 }
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_build_ai_script_generation_user_prompt_includes_description() {
+        let prompt = build_ai_script_generation_user_prompt("create a weather script");
+        assert!(prompt.contains("create a weather script"));
+        assert!(prompt.contains("Script Kit script"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_prefers_typescript_fence() {
+        let response = r#"
+Here's your script:
+```typescript
+// Name: Weather
+import "@scriptkit/sdk";
+```
+"#;
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert!(extracted.contains("// Name: Weather"));
+        assert!(!extracted.contains("```"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_falls_back_to_plain_text() {
+        let response = "// Name: Plain\nimport \"@scriptkit/sdk\";";
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert_eq!(extracted, "// Name: Plain\nimport \"@scriptkit/sdk\";\n");
+    }
+
+    #[test]
+    fn test_derive_script_name_from_description_uses_fallback_for_symbols() {
+        let name = derive_script_name_from_description("@@@ !!!");
+        assert_eq!(name, "ai-generated-script");
+    }
+}
diff --git a/src/app_impl/startup.rs b/src/app_impl/startup.rs
index 2e95070751083b8f4c24cd7eae8372d6bec3f6d6..441b4f23bca1b6294025c698eeb11d45815e85e0
--- a/src/app_impl/startup.rs
+++ b/src/app_impl/startup.rs
@@ -555,9 +555,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -720,20 +722,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/startup_new_tab.rs b/src/app_impl/startup_new_tab.rs
index 6c807a195553d9f168087c2ca6d771c76e555c11..7c84270cbe91c0b1ddd460aa29db8c788766d0cf
--- a/src/app_impl/startup_new_tab.rs
+++ b/src/app_impl/startup_new_tab.rs
@@ -6,9 +6,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -171,20 +173,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/tests.rs b/src/app_impl/tests.rs
index d75ba121851d7233c5ae77bc474a64b27e7c31d2..bb78d84e5a8ec9ba80c0a2287ad0c1cba93f226d
--- a/src/app_impl/tests.rs
+++ b/src/app_impl/tests.rs
@@ -1,6 +1,7 @@
 #[cfg(test)]
 mod app_impl_state_sync_tests {
     use super::{calculate_fallback_error_message, ScriptListApp};
+    use std::fs;
 
     #[test]
     fn test_sync_builtin_query_state_updates_query_and_selection_when_changed() {
@@ -46,4 +47,28 @@
         assert!(message.contains("Could not evaluate expression"));
         assert!(message.contains("Check the syntax and try again"));
     }
+
+    #[test]
+    fn test_shift_tab_routes_to_ai_script_generation_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("if has_shift")
+                && startup_tab.contains("this.generate_script_from_ai_prompt(query, cx);"),
+            "Shift+Tab in ScriptList should route to generate_script_from_ai_prompt. \
+             Missing expected branch in startup_new_tab.rs"
+        );
+    }
+
+    #[test]
+    fn test_tab_still_routes_to_inline_ai_chat_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("this.show_inline_ai_chat(Some(query), cx);"),
+            "Tab in ScriptList should continue routing to show_inline_ai_chat"
+        );
+    }
 }


thinking
**Verifying final system status**
exec
/bin/zsh -lc "~/.claude/bin/swarm.ts status | sed -n '1,80p'" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
Repo: /Users/johnlindquist/dev/script-kit-gpui
Updated: 2026-02-08T02:54:35.095Z

Active agents (1):
codex-script-gen-builtin [in_progress] codex/gpt-5.3-codex/exec
  task: In Script Kit GPUI, there needs to be a built-in feature or action for AI script generation triggered by Shift+Tab. Loo…
  scope: src/builtins/**,src/app_execute/**,src/ai/**,src/kit_store/**
  claims(6): src/builtins/part_001_entries/entries_002.rs, src/builtins/part_000.rs, src/builtins/tests/part_000.rs, src/app_execute/builtin_execution.rs, src/ai/script_gen…
  notes: Claimed builtin/app_execute/ai files; implementing AI script generation command + tests

Inactive agents (254):
- codex-accessibility [done] done. exit=0. final=.ai/logs/codex-accessibility.final.md. See final message file.
- codex-action-labels [failed] failed. exit=1. final=.ai/logs/codex-action-labels.final.md.
- codex-actions-audit [failed] failed. exit=1. final=.ai/logs/codex-actions-audit.final.md. See final message file.
- codex-actions-consistency [failed] failed. exit=1. final=.ai/logs/codex-actions-consistency.final.md.
- codex-actions-dialog [done] done. exit=0. final=.ai/logs/codex-actions-dialog.final.md. See final message file.
- codex-actions-feedback [failed] failed. exit=1. final=.ai/logs/codex-actions-feedback.final.md.
- codex-actions-feedback-v2 [done] done. exit=0. final=.ai/logs/codex-actions-feedback-v2.final.md. See final message file.
- codex-actions-ux [done] done. exit=0. final=.ai/logs/codex-actions-ux.final.md. See final message file.
- codex-agents-md [done] done. exit=0. final=.ai/logs/codex-agents-md.final.md. See final message file.
file update:
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index 98a0c9cd1db5a933992c0f99cb20a3c2ca436b41..c985a5090574816267103019d1b272e1e3151a38
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -1,4 +1,194 @@
 use super::*;
+use anyhow::{anyhow, Context as AnyhowContext, Result};
+
+#[derive(Debug, Clone, Copy)]
+enum AiScriptGenerationStage {
+    SelectModel,
+    ResolveProvider,
+    RequestCompletion,
+    ExtractScript,
+    CreateScriptFile,
+    WriteScriptFile,
+    OpenEditor,
+}
+
+impl AiScriptGenerationStage {
+    fn as_str(self) -> &'static str {
+        match self {
+            Self::SelectModel => "select_model",
+            Self::ResolveProvider => "resolve_provider",
+            Self::RequestCompletion => "request_completion",
+            Self::ExtractScript => "extract_script",
+            Self::CreateScriptFile => "create_script_file",
+            Self::WriteScriptFile => "write_script_file",
+            Self::OpenEditor => "open_editor",
+        }
+    }
+}
+
+const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You are ScriptKitScriptGenerator.
+Generate a complete Script Kit TypeScript script and return code only.
+Requirements:
+- Include metadata comments at the top:
+  // Name: <script name>
+  // Description: <short summary>
+- Include: import "@scriptkit/sdk";
+- Use idiomatic Script Kit APIs (for example await arg(), await div(), await editor()) when useful.
+- Prefer clear async/await flow and practical defaults.
+- Return a full runnable script with no extra explanation."#;
+
+fn build_ai_script_generation_user_prompt(description: &str) -> String {
+    format!(
+        "Generate a complete Script Kit script for this request:\n\n{}\n\nReturn only the TypeScript script source.",
+        description.trim()
+    )
+}
+
+fn select_default_ai_script_model(
+    registry: &crate::ai::ProviderRegistry,
+) -> Option<crate::ai::ModelInfo> {
+    let all_models = registry.get_all_models();
+    all_models
+        .iter()
+        .find(|model| model.provider.eq_ignore_ascii_case("vercel"))
+        .cloned()
+        .or_else(|| all_models.first().cloned())
+}
+
+fn extract_generated_script_source(raw_response: &str) -> Option<String> {
+    let trimmed = raw_response.trim();
+    if trimmed.is_empty() {
+        return None;
+    }
+
+    // Prefer typed code fences first, then generic fences.
+    for fence in ["```typescript", "```ts", "```javascript", "```js", "```"] {
+        if let Some(content) = extract_first_fenced_block(trimmed, fence) {
+            return normalize_generated_script(content);
+        }
+    }
+
+    normalize_generated_script(trimmed.to_string())
+}
+
+fn extract_first_fenced_block(response: &str, fence_start: &str) -> Option<String> {
+    let start_index = response.find(fence_start)?;
+    let after_fence = &response[start_index + fence_start.len()..];
+    let after_newline = after_fence
+        .strip_prefix("\r\n")
+        .or_else(|| after_fence.strip_prefix('\n'))
+        .unwrap_or(after_fence);
+    let end_index = after_newline.find("```")?;
+    Some(after_newline[..end_index].trim().to_string())
+}
+
+fn normalize_generated_script(content: String) -> Option<String> {
+    let trimmed = content.trim();
+    if trimmed.is_empty() {
+        None
+    } else {
+        Some(format!("{trimmed}\n"))
+    }
+}
+
+fn derive_script_name_from_description(description: &str) -> String {
+    let mut slug = String::new();
+    let mut last_was_dash = false;
+
+    for ch in description.chars() {
+        let lower = ch.to_ascii_lowercase();
+        if lower.is_ascii_alphanumeric() {
+            slug.push(lower);
+            last_was_dash = false;
+        } else if !last_was_dash && !slug.is_empty() {
+            slug.push('-');
+            last_was_dash = true;
+        }
+
+        if slug.len() >= 48 {
+            break;
+        }
+    }
+
+    while slug.ends_with('-') {
+        slug.pop();
+    }
+
+    if slug.is_empty() {
+        "ai-generated-script".to_string()
+    } else {
+        slug
+    }
+}
+
+fn generate_script_via_ai_backend(
+    registry: &crate::ai::ProviderRegistry,
+    model_id: &str,
+    prompt_description: &str,
+    config: &crate::config::Config,
+) -> Result<std::path::PathBuf> {
+    let provider = registry
+        .find_provider_for_model(model_id)
+        .cloned()
+        .ok_or_else(|| {
+            anyhow!(
+                "state={} attempted=find_provider_for_model model_id={} failure=provider_not_found",
+                AiScriptGenerationStage::ResolveProvider.as_str(),
+                model_id
+            )
+        })?;
+
+    let request_messages = vec![
+        crate::ai::ProviderMessage::system(AI_SCRIPT_GENERATION_SYSTEM_PROMPT),
+        crate::ai::ProviderMessage::user(build_ai_script_generation_user_prompt(prompt_description)),
+    ];
+
+    let ai_response = provider
+        .send_message(&request_messages, model_id)
+        .with_context(|| {
+            format!(
+                "state={} attempted=send_message model_id={} provider={} failure=provider_call_failed",
+                AiScriptGenerationStage::RequestCompletion.as_str(),
+                model_id,
+                provider.provider_id()
+            )
+        })?;
+
+    let generated_script = extract_generated_script_source(&ai_response).ok_or_else(|| {
+        anyhow!(
+            "state={} attempted=extract_generated_script model_id={} failure=empty_script_response",
+            AiScriptGenerationStage::ExtractScript.as_str(),
+            model_id
+        )
+    })?;
+
+    let script_name = derive_script_name_from_description(prompt_description);
+    let script_path = crate::script_creation::create_new_script(&script_name).with_context(|| {
+        format!(
+            "state={} attempted=create_new_script name={} failure=create_script_failed",
+            AiScriptGenerationStage::CreateScriptFile.as_str(),
+            script_name
+        )
+    })?;
+
+    std::fs::write(&script_path, generated_script).with_context(|| {
+        format!(
+            "state={} attempted=write_script path={} failure=write_failed",
+            AiScriptGenerationStage::WriteScriptFile.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    crate::script_creation::open_in_editor(&script_path, config).with_context(|| {
+        format!(
+            "state={} attempted=open_in_editor path={} failure=open_editor_failed",
+            AiScriptGenerationStage::OpenEditor.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
 
 impl ScriptListApp {
     pub(crate) fn is_in_prompt(&self) -> bool {
@@ -274,6 +464,145 @@
         cx.notify();
     }
 
+    /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
+    /// The generated script is saved to disk and opened in the configured editor.
+    pub fn generate_script_from_ai_prompt(
+        &mut self,
+        prompt_description: String,
+        cx: &mut Context<Self>,
+    ) {
+        let prompt_description = prompt_description.trim().to_string();
+        if prompt_description.is_empty() {
+            return;
+        }
+
+        let registry = self.cached_provider_registry.clone().unwrap_or_else(|| {
+            crate::ai::ProviderRegistry::from_environment_with_config(Some(&self.config))
+        });
+
+        if !registry.has_any_provider() {
+            self.toast_manager.push(
+                components::toast::Toast::error("No AI providers configured for script generation", &self.theme)
+                    .duration_ms(Some(5000)),
+            );
+            cx.notify();
+            return;
+        }
+
+        let selected_model = match select_default_ai_script_model(&registry) {
+            Some(model) => model,
+            None => {
+                let stage = AiScriptGenerationStage::SelectModel.as_str();
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    &format!(
+                        "state={} attempted=select_default_model failure=no_available_models",
+                        stage
+                    ),
+                );
+                self.toast_manager.push(
+                    components::toast::Toast::error(
+                        "No AI models available for script generation",
+                        &self.theme,
+                    )
+                    .duration_ms(Some(5000)),
+                );
+                cx.notify();
+                return;
+            }
+        };
+
+        let model_id = selected_model.id.clone();
+        let provider = selected_model.provider.clone();
+        let config = self.config.clone();
+        let (tx, rx) = async_channel::bounded::<std::result::Result<std::path::PathBuf, String>>(1);
+
+        logging::log(
+            "AI_SCRIPT_GEN",
+            &format!(
+                "state=queued attempted=shift_tab_script_generation model_id={} provider={} prompt_len={}",
+                model_id,
+                provider,
+                prompt_description.len()
+            ),
+        );
+        self.show_hud("Generating script with AI...".to_string(), Some(1500), cx);
+
+        std::thread::spawn(move || {
+            logging::log(
+                "AI_SCRIPT_GEN",
+                &format!(
+                    "state=running attempted=generate_script model_id={} provider={}",
+                    model_id, provider
+                ),
+            );
+
+            let generation_result =
+                generate_script_via_ai_backend(&registry, &model_id, &prompt_description, &config)
+                    .map_err(|error| error.to_string());
+
+            if tx.send_blocking(generation_result).is_err() {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=send_result failure=result_channel_closed",
+                );
+            }
+        });
+
+        cx.spawn(async move |this, cx| {
+            let Ok(result) = rx.recv().await else {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=recv_result failure=result_channel_closed",
+                );
+                return;
+            };
+
+            let _ = cx.update(|cx| {
+                this.update(cx, |app, cx| {
+                    match result {
+                        Ok(script_path) => {
+                            let script_name = script_path
+                                .file_name()
+                                .and_then(|name| name.to_str())
+                                .unwrap_or("generated script");
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!(
+                                    "state=completed attempted=generate_script path={}",
+                                    script_path.display()
+                                ),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::success(
+                                    format!("Generated and opened {}", script_name),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(3500)),
+                            );
+                            app.close_and_reset_window(cx);
+                        }
+                        Err(error) => {
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!("state=failed attempted=generate_script error={}", error),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::error(
+                                    format!("Failed to generate script: {}", error),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(7000)),
+                            );
+                            cx.notify();
+                        }
+                    }
+                })
+            });
+        })
+        .detach();
+    }
+
     /// Rebuild the cached provider registry in a background thread.
     /// Called after config changes (API key saved, Claude Code enabled, etc.)
     pub fn rebuild_provider_registry_async(&mut self, cx: &mut Context<Self>) {
@@ -312,3 +641,44 @@
         .detach();
     }
 }
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_build_ai_script_generation_user_prompt_includes_description() {
+        let prompt = build_ai_script_generation_user_prompt("create a weather script");
+        assert!(prompt.contains("create a weather script"));
+        assert!(prompt.contains("Script Kit script"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_prefers_typescript_fence() {
+        let response = r#"
+Here's your script:
+```typescript
+// Name: Weather
+import "@scriptkit/sdk";
+```
+"#;
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert!(extracted.contains("// Name: Weather"));
+        assert!(!extracted.contains("```"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_falls_back_to_plain_text() {
+        let response = "// Name: Plain\nimport \"@scriptkit/sdk\";";
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert_eq!(extracted, "// Name: Plain\nimport \"@scriptkit/sdk\";\n");
+    }
+
+    #[test]
+    fn test_derive_script_name_from_description_uses_fallback_for_symbols() {
+        let name = derive_script_name_from_description("@@@ !!!");
+        assert_eq!(name, "ai-generated-script");
+    }
+}
diff --git a/src/app_impl/startup.rs b/src/app_impl/startup.rs
index 2e95070751083b8f4c24cd7eae8372d6bec3f6d6..441b4f23bca1b6294025c698eeb11d45815e85e0
--- a/src/app_impl/startup.rs
+++ b/src/app_impl/startup.rs
@@ -555,9 +555,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -720,20 +722,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/startup_new_tab.rs b/src/app_impl/startup_new_tab.rs
index 6c807a195553d9f168087c2ca6d771c76e555c11..7c84270cbe91c0b1ddd460aa29db8c788766d0cf
--- a/src/app_impl/startup_new_tab.rs
+++ b/src/app_impl/startup_new_tab.rs
@@ -6,9 +6,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -171,20 +173,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/tests.rs b/src/app_impl/tests.rs
index d75ba121851d7233c5ae77bc474a64b27e7c31d2..bb78d84e5a8ec9ba80c0a2287ad0c1cba93f226d
--- a/src/app_impl/tests.rs
+++ b/src/app_impl/tests.rs
@@ -1,6 +1,7 @@
 #[cfg(test)]
 mod app_impl_state_sync_tests {
     use super::{calculate_fallback_error_message, ScriptListApp};
+    use std::fs;
 
     #[test]
     fn test_sync_builtin_query_state_updates_query_and_selection_when_changed() {
@@ -46,4 +47,28 @@
         assert!(message.contains("Could not evaluate expression"));
         assert!(message.contains("Check the syntax and try again"));
     }
+
+    #[test]
+    fn test_shift_tab_routes_to_ai_script_generation_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("if has_shift")
+                && startup_tab.contains("this.generate_script_from_ai_prompt(query, cx);"),
+            "Shift+Tab in ScriptList should route to generate_script_from_ai_prompt. \
+             Missing expected branch in startup_new_tab.rs"
+        );
+    }
+
+    #[test]
+    fn test_tab_still_routes_to_inline_ai_chat_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("this.show_inline_ai_chat(Some(query), cx);"),
+            "Tab in ScriptList should continue routing to show_inline_ai_chat"
+        );
+    }
 }


thinking
**Checking modified files**
exec
/bin/zsh -lc 'git status --short -- src/app_impl/startup.rs src/app_impl/startup_new_tab.rs src/app_impl/prompt_ai.rs src/app_impl/tests.rs' in /Users/johnlindquist/dev/script-kit-gpui succeeded in 52ms:
 M src/app_impl/prompt_ai.rs
 M src/app_impl/startup.rs
 M src/app_impl/startup_new_tab.rs
 M src/app_impl/tests.rs
file update:
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index 98a0c9cd1db5a933992c0f99cb20a3c2ca436b41..c985a5090574816267103019d1b272e1e3151a38
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -1,4 +1,194 @@
 use super::*;
+use anyhow::{anyhow, Context as AnyhowContext, Result};
+
+#[derive(Debug, Clone, Copy)]
+enum AiScriptGenerationStage {
+    SelectModel,
+    ResolveProvider,
+    RequestCompletion,
+    ExtractScript,
+    CreateScriptFile,
+    WriteScriptFile,
+    OpenEditor,
+}
+
+impl AiScriptGenerationStage {
+    fn as_str(self) -> &'static str {
+        match self {
+            Self::SelectModel => "select_model",
+            Self::ResolveProvider => "resolve_provider",
+            Self::RequestCompletion => "request_completion",
+            Self::ExtractScript => "extract_script",
+            Self::CreateScriptFile => "create_script_file",
+            Self::WriteScriptFile => "write_script_file",
+            Self::OpenEditor => "open_editor",
+        }
+    }
+}
+
+const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You are ScriptKitScriptGenerator.
+Generate a complete Script Kit TypeScript script and return code only.
+Requirements:
+- Include metadata comments at the top:
+  // Name: <script name>
+  // Description: <short summary>
+- Include: import "@scriptkit/sdk";
+- Use idiomatic Script Kit APIs (for example await arg(), await div(), await editor()) when useful.
+- Prefer clear async/await flow and practical defaults.
+- Return a full runnable script with no extra explanation."#;
+
+fn build_ai_script_generation_user_prompt(description: &str) -> String {
+    format!(
+        "Generate a complete Script Kit script for this request:\n\n{}\n\nReturn only the TypeScript script source.",
+        description.trim()
+    )
+}
+
+fn select_default_ai_script_model(
+    registry: &crate::ai::ProviderRegistry,
+) -> Option<crate::ai::ModelInfo> {
+    let all_models = registry.get_all_models();
+    all_models
+        .iter()
+        .find(|model| model.provider.eq_ignore_ascii_case("vercel"))
+        .cloned()
+        .or_else(|| all_models.first().cloned())
+}
+
+fn extract_generated_script_source(raw_response: &str) -> Option<String> {
+    let trimmed = raw_response.trim();
+    if trimmed.is_empty() {
+        return None;
+    }
+
+    // Prefer typed code fences first, then generic fences.
+    for fence in ["```typescript", "```ts", "```javascript", "```js", "```"] {
+        if let Some(content) = extract_first_fenced_block(trimmed, fence) {
+            return normalize_generated_script(content);
+        }
+    }
+
+    normalize_generated_script(trimmed.to_string())
+}
+
+fn extract_first_fenced_block(response: &str, fence_start: &str) -> Option<String> {
+    let start_index = response.find(fence_start)?;
+    let after_fence = &response[start_index + fence_start.len()..];
+    let after_newline = after_fence
+        .strip_prefix("\r\n")
+        .or_else(|| after_fence.strip_prefix('\n'))
+        .unwrap_or(after_fence);
+    let end_index = after_newline.find("```")?;
+    Some(after_newline[..end_index].trim().to_string())
+}
+
+fn normalize_generated_script(content: String) -> Option<String> {
+    let trimmed = content.trim();
+    if trimmed.is_empty() {
+        None
+    } else {
+        Some(format!("{trimmed}\n"))
+    }
+}
+
+fn derive_script_name_from_description(description: &str) -> String {
+    let mut slug = String::new();
+    let mut last_was_dash = false;
+
+    for ch in description.chars() {
+        let lower = ch.to_ascii_lowercase();
+        if lower.is_ascii_alphanumeric() {
+            slug.push(lower);
+            last_was_dash = false;
+        } else if !last_was_dash && !slug.is_empty() {
+            slug.push('-');
+            last_was_dash = true;
+        }
+
+        if slug.len() >= 48 {
+            break;
+        }
+    }
+
+    while slug.ends_with('-') {
+        slug.pop();
+    }
+
+    if slug.is_empty() {
+        "ai-generated-script".to_string()
+    } else {
+        slug
+    }
+}
+
+fn generate_script_via_ai_backend(
+    registry: &crate::ai::ProviderRegistry,
+    model_id: &str,
+    prompt_description: &str,
+    config: &crate::config::Config,
+) -> Result<std::path::PathBuf> {
+    let provider = registry
+        .find_provider_for_model(model_id)
+        .cloned()
+        .ok_or_else(|| {
+            anyhow!(
+                "state={} attempted=find_provider_for_model model_id={} failure=provider_not_found",
+                AiScriptGenerationStage::ResolveProvider.as_str(),
+                model_id
+            )
+        })?;
+
+    let request_messages = vec![
+        crate::ai::ProviderMessage::system(AI_SCRIPT_GENERATION_SYSTEM_PROMPT),
+        crate::ai::ProviderMessage::user(build_ai_script_generation_user_prompt(prompt_description)),
+    ];
+
+    let ai_response = provider
+        .send_message(&request_messages, model_id)
+        .with_context(|| {
+            format!(
+                "state={} attempted=send_message model_id={} provider={} failure=provider_call_failed",
+                AiScriptGenerationStage::RequestCompletion.as_str(),
+                model_id,
+                provider.provider_id()
+            )
+        })?;
+
+    let generated_script = extract_generated_script_source(&ai_response).ok_or_else(|| {
+        anyhow!(
+            "state={} attempted=extract_generated_script model_id={} failure=empty_script_response",
+            AiScriptGenerationStage::ExtractScript.as_str(),
+            model_id
+        )
+    })?;
+
+    let script_name = derive_script_name_from_description(prompt_description);
+    let script_path = crate::script_creation::create_new_script(&script_name).with_context(|| {
+        format!(
+            "state={} attempted=create_new_script name={} failure=create_script_failed",
+            AiScriptGenerationStage::CreateScriptFile.as_str(),
+            script_name
+        )
+    })?;
+
+    std::fs::write(&script_path, generated_script).with_context(|| {
+        format!(
+            "state={} attempted=write_script path={} failure=write_failed",
+            AiScriptGenerationStage::WriteScriptFile.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    crate::script_creation::open_in_editor(&script_path, config).with_context(|| {
+        format!(
+            "state={} attempted=open_in_editor path={} failure=open_editor_failed",
+            AiScriptGenerationStage::OpenEditor.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
 
 impl ScriptListApp {
     pub(crate) fn is_in_prompt(&self) -> bool {
@@ -274,6 +464,145 @@
         cx.notify();
     }
 
+    /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
+    /// The generated script is saved to disk and opened in the configured editor.
+    pub fn generate_script_from_ai_prompt(
+        &mut self,
+        prompt_description: String,
+        cx: &mut Context<Self>,
+    ) {
+        let prompt_description = prompt_description.trim().to_string();
+        if prompt_description.is_empty() {
+            return;
+        }
+
+        let registry = self.cached_provider_registry.clone().unwrap_or_else(|| {
+            crate::ai::ProviderRegistry::from_environment_with_config(Some(&self.config))
+        });
+
+        if !registry.has_any_provider() {
+            self.toast_manager.push(
+                components::toast::Toast::error("No AI providers configured for script generation", &self.theme)
+                    .duration_ms(Some(5000)),
+            );
+            cx.notify();
+            return;
+        }
+
+        let selected_model = match select_default_ai_script_model(&registry) {
+            Some(model) => model,
+            None => {
+                let stage = AiScriptGenerationStage::SelectModel.as_str();
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    &format!(
+                        "state={} attempted=select_default_model failure=no_available_models",
+                        stage
+                    ),
+                );
+                self.toast_manager.push(
+                    components::toast::Toast::error(
+                        "No AI models available for script generation",
+                        &self.theme,
+                    )
+                    .duration_ms(Some(5000)),
+                );
+                cx.notify();
+                return;
+            }
+        };
+
+        let model_id = selected_model.id.clone();
+        let provider = selected_model.provider.clone();
+        let config = self.config.clone();
+        let (tx, rx) = async_channel::bounded::<std::result::Result<std::path::PathBuf, String>>(1);
+
+        logging::log(
+            "AI_SCRIPT_GEN",
+            &format!(
+                "state=queued attempted=shift_tab_script_generation model_id={} provider={} prompt_len={}",
+                model_id,
+                provider,
+                prompt_description.len()
+            ),
+        );
+        self.show_hud("Generating script with AI...".to_string(), Some(1500), cx);
+
+        std::thread::spawn(move || {
+            logging::log(
+                "AI_SCRIPT_GEN",
+                &format!(
+                    "state=running attempted=generate_script model_id={} provider={}",
+                    model_id, provider
+                ),
+            );
+
+            let generation_result =
+                generate_script_via_ai_backend(&registry, &model_id, &prompt_description, &config)
+                    .map_err(|error| error.to_string());
+
+            if tx.send_blocking(generation_result).is_err() {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=send_result failure=result_channel_closed",
+                );
+            }
+        });
+
+        cx.spawn(async move |this, cx| {
+            let Ok(result) = rx.recv().await else {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=recv_result failure=result_channel_closed",
+                );
+                return;
+            };
+
+            let _ = cx.update(|cx| {
+                this.update(cx, |app, cx| {
+                    match result {
+                        Ok(script_path) => {
+                            let script_name = script_path
+                                .file_name()
+                                .and_then(|name| name.to_str())
+                                .unwrap_or("generated script");
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!(
+                                    "state=completed attempted=generate_script path={}",
+                                    script_path.display()
+                                ),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::success(
+                                    format!("Generated and opened {}", script_name),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(3500)),
+                            );
+                            app.close_and_reset_window(cx);
+                        }
+                        Err(error) => {
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!("state=failed attempted=generate_script error={}", error),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::error(
+                                    format!("Failed to generate script: {}", error),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(7000)),
+                            );
+                            cx.notify();
+                        }
+                    }
+                })
+            });
+        })
+        .detach();
+    }
+
     /// Rebuild the cached provider registry in a background thread.
     /// Called after config changes (API key saved, Claude Code enabled, etc.)
     pub fn rebuild_provider_registry_async(&mut self, cx: &mut Context<Self>) {
@@ -312,3 +641,44 @@
         .detach();
     }
 }
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_build_ai_script_generation_user_prompt_includes_description() {
+        let prompt = build_ai_script_generation_user_prompt("create a weather script");
+        assert!(prompt.contains("create a weather script"));
+        assert!(prompt.contains("Script Kit script"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_prefers_typescript_fence() {
+        let response = r#"
+Here's your script:
+```typescript
+// Name: Weather
+import "@scriptkit/sdk";
+```
+"#;
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert!(extracted.contains("// Name: Weather"));
+        assert!(!extracted.contains("```"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_falls_back_to_plain_text() {
+        let response = "// Name: Plain\nimport \"@scriptkit/sdk\";";
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert_eq!(extracted, "// Name: Plain\nimport \"@scriptkit/sdk\";\n");
+    }
+
+    #[test]
+    fn test_derive_script_name_from_description_uses_fallback_for_symbols() {
+        let name = derive_script_name_from_description("@@@ !!!");
+        assert_eq!(name, "ai-generated-script");
+    }
+}
diff --git a/src/app_impl/startup.rs b/src/app_impl/startup.rs
index 2e95070751083b8f4c24cd7eae8372d6bec3f6d6..441b4f23bca1b6294025c698eeb11d45815e85e0
--- a/src/app_impl/startup.rs
+++ b/src/app_impl/startup.rs
@@ -555,9 +555,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -720,20 +722,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/startup_new_tab.rs b/src/app_impl/startup_new_tab.rs
index 6c807a195553d9f168087c2ca6d771c76e555c11..7c84270cbe91c0b1ddd460aa29db8c788766d0cf
--- a/src/app_impl/startup_new_tab.rs
+++ b/src/app_impl/startup_new_tab.rs
@@ -6,9 +6,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -171,20 +173,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/tests.rs b/src/app_impl/tests.rs
index d75ba121851d7233c5ae77bc474a64b27e7c31d2..bb78d84e5a8ec9ba80c0a2287ad0c1cba93f226d
--- a/src/app_impl/tests.rs
+++ b/src/app_impl/tests.rs
@@ -1,6 +1,7 @@
 #[cfg(test)]
 mod app_impl_state_sync_tests {
     use super::{calculate_fallback_error_message, ScriptListApp};
+    use std::fs;
 
     #[test]
     fn test_sync_builtin_query_state_updates_query_and_selection_when_changed() {
@@ -46,4 +47,28 @@
         assert!(message.contains("Could not evaluate expression"));
         assert!(message.contains("Check the syntax and try again"));
     }
+
+    #[test]
+    fn test_shift_tab_routes_to_ai_script_generation_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("if has_shift")
+                && startup_tab.contains("this.generate_script_from_ai_prompt(query, cx);"),
+            "Shift+Tab in ScriptList should route to generate_script_from_ai_prompt. \
+             Missing expected branch in startup_new_tab.rs"
+        );
+    }
+
+    #[test]
+    fn test_tab_still_routes_to_inline_ai_chat_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("this.show_inline_ai_chat(Some(query), cx);"),
+            "Tab in ScriptList should continue routing to show_inline_ai_chat"
+        );
+    }
 }


thinking
**Assessing commit readiness and thread safety**
exec
/bin/zsh -lc "rg -n \"pub fn open_in_editor|fn create_new_script\" src/script_creation.rs src -g '*.rs'" in /Users/johnlindquist/dev/script-kit-gpui exited 2 in 51ms:
rg: src/script_creation.rs: No such file or directory (os error 2)
src/script_creation/part_000.rs:252:pub fn create_new_script(name: &str) -> Result<PathBuf> {
src/script_creation/part_000.rs:255:fn create_new_script_in_dir(name: &str, scripts_dir: &Path) -> Result<PathBuf> {
src/script_creation/part_000.rs:403:pub fn open_in_editor(path: &Path, config: &Config) -> Result<()> {
file update:
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index 98a0c9cd1db5a933992c0f99cb20a3c2ca436b41..c985a5090574816267103019d1b272e1e3151a38
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -1,4 +1,194 @@
 use super::*;
+use anyhow::{anyhow, Context as AnyhowContext, Result};
+
+#[derive(Debug, Clone, Copy)]
+enum AiScriptGenerationStage {
+    SelectModel,
+    ResolveProvider,
+    RequestCompletion,
+    ExtractScript,
+    CreateScriptFile,
+    WriteScriptFile,
+    OpenEditor,
+}
+
+impl AiScriptGenerationStage {
+    fn as_str(self) -> &'static str {
+        match self {
+            Self::SelectModel => "select_model",
+            Self::ResolveProvider => "resolve_provider",
+            Self::RequestCompletion => "request_completion",
+            Self::ExtractScript => "extract_script",
+            Self::CreateScriptFile => "create_script_file",
+            Self::WriteScriptFile => "write_script_file",
+            Self::OpenEditor => "open_editor",
+        }
+    }
+}
+
+const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You are ScriptKitScriptGenerator.
+Generate a complete Script Kit TypeScript script and return code only.
+Requirements:
+- Include metadata comments at the top:
+  // Name: <script name>
+  // Description: <short summary>
+- Include: import "@scriptkit/sdk";
+- Use idiomatic Script Kit APIs (for example await arg(), await div(), await editor()) when useful.
+- Prefer clear async/await flow and practical defaults.
+- Return a full runnable script with no extra explanation."#;
+
+fn build_ai_script_generation_user_prompt(description: &str) -> String {
+    format!(
+        "Generate a complete Script Kit script for this request:\n\n{}\n\nReturn only the TypeScript script source.",
+        description.trim()
+    )
+}
+
+fn select_default_ai_script_model(
+    registry: &crate::ai::ProviderRegistry,
+) -> Option<crate::ai::ModelInfo> {
+    let all_models = registry.get_all_models();
+    all_models
+        .iter()
+        .find(|model| model.provider.eq_ignore_ascii_case("vercel"))
+        .cloned()
+        .or_else(|| all_models.first().cloned())
+}
+
+fn extract_generated_script_source(raw_response: &str) -> Option<String> {
+    let trimmed = raw_response.trim();
+    if trimmed.is_empty() {
+        return None;
+    }
+
+    // Prefer typed code fences first, then generic fences.
+    for fence in ["```typescript", "```ts", "```javascript", "```js", "```"] {
+        if let Some(content) = extract_first_fenced_block(trimmed, fence) {
+            return normalize_generated_script(content);
+        }
+    }
+
+    normalize_generated_script(trimmed.to_string())
+}
+
+fn extract_first_fenced_block(response: &str, fence_start: &str) -> Option<String> {
+    let start_index = response.find(fence_start)?;
+    let after_fence = &response[start_index + fence_start.len()..];
+    let after_newline = after_fence
+        .strip_prefix("\r\n")
+        .or_else(|| after_fence.strip_prefix('\n'))
+        .unwrap_or(after_fence);
+    let end_index = after_newline.find("```")?;
+    Some(after_newline[..end_index].trim().to_string())
+}
+
+fn normalize_generated_script(content: String) -> Option<String> {
+    let trimmed = content.trim();
+    if trimmed.is_empty() {
+        None
+    } else {
+        Some(format!("{trimmed}\n"))
+    }
+}
+
+fn derive_script_name_from_description(description: &str) -> String {
+    let mut slug = String::new();
+    let mut last_was_dash = false;
+
+    for ch in description.chars() {
+        let lower = ch.to_ascii_lowercase();
+        if lower.is_ascii_alphanumeric() {
+            slug.push(lower);
+            last_was_dash = false;
+        } else if !last_was_dash && !slug.is_empty() {
+            slug.push('-');
+            last_was_dash = true;
+        }
+
+        if slug.len() >= 48 {
+            break;
+        }
+    }
+
+    while slug.ends_with('-') {
+        slug.pop();
+    }
+
+    if slug.is_empty() {
+        "ai-generated-script".to_string()
+    } else {
+        slug
+    }
+}
+
+fn generate_script_via_ai_backend(
+    registry: &crate::ai::ProviderRegistry,
+    model_id: &str,
+    prompt_description: &str,
+    config: &crate::config::Config,
+) -> Result<std::path::PathBuf> {
+    let provider = registry
+        .find_provider_for_model(model_id)
+        .cloned()
+        .ok_or_else(|| {
+            anyhow!(
+                "state={} attempted=find_provider_for_model model_id={} failure=provider_not_found",
+                AiScriptGenerationStage::ResolveProvider.as_str(),
+                model_id
+            )
+        })?;
+
+    let request_messages = vec![
+        crate::ai::ProviderMessage::system(AI_SCRIPT_GENERATION_SYSTEM_PROMPT),
+        crate::ai::ProviderMessage::user(build_ai_script_generation_user_prompt(prompt_description)),
+    ];
+
+    let ai_response = provider
+        .send_message(&request_messages, model_id)
+        .with_context(|| {
+            format!(
+                "state={} attempted=send_message model_id={} provider={} failure=provider_call_failed",
+                AiScriptGenerationStage::RequestCompletion.as_str(),
+                model_id,
+                provider.provider_id()
+            )
+        })?;
+
+    let generated_script = extract_generated_script_source(&ai_response).ok_or_else(|| {
+        anyhow!(
+            "state={} attempted=extract_generated_script model_id={} failure=empty_script_response",
+            AiScriptGenerationStage::ExtractScript.as_str(),
+            model_id
+        )
+    })?;
+
+    let script_name = derive_script_name_from_description(prompt_description);
+    let script_path = crate::script_creation::create_new_script(&script_name).with_context(|| {
+        format!(
+            "state={} attempted=create_new_script name={} failure=create_script_failed",
+            AiScriptGenerationStage::CreateScriptFile.as_str(),
+            script_name
+        )
+    })?;
+
+    std::fs::write(&script_path, generated_script).with_context(|| {
+        format!(
+            "state={} attempted=write_script path={} failure=write_failed",
+            AiScriptGenerationStage::WriteScriptFile.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    crate::script_creation::open_in_editor(&script_path, config).with_context(|| {
+        format!(
+            "state={} attempted=open_in_editor path={} failure=open_editor_failed",
+            AiScriptGenerationStage::OpenEditor.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
 
 impl ScriptListApp {
     pub(crate) fn is_in_prompt(&self) -> bool {
@@ -274,6 +464,145 @@
         cx.notify();
     }
 
+    /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
+    /// The generated script is saved to disk and opened in the configured editor.
+    pub fn generate_script_from_ai_prompt(
+        &mut self,
+        prompt_description: String,
+        cx: &mut Context<Self>,
+    ) {
+        let prompt_description = prompt_description.trim().to_string();
+        if prompt_description.is_empty() {
+            return;
+        }
+
+        let registry = self.cached_provider_registry.clone().unwrap_or_else(|| {
+            crate::ai::ProviderRegistry::from_environment_with_config(Some(&self.config))
+        });
+
+        if !registry.has_any_provider() {
+            self.toast_manager.push(
+                components::toast::Toast::error("No AI providers configured for script generation", &self.theme)
+                    .duration_ms(Some(5000)),
+            );
+            cx.notify();
+            return;
+        }
+
+        let selected_model = match select_default_ai_script_model(&registry) {
+            Some(model) => model,
+            None => {
+                let stage = AiScriptGenerationStage::SelectModel.as_str();
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    &format!(
+                        "state={} attempted=select_default_model failure=no_available_models",
+                        stage
+                    ),
+                );
+                self.toast_manager.push(
+                    components::toast::Toast::error(
+                        "No AI models available for script generation",
+                        &self.theme,
+                    )
+                    .duration_ms(Some(5000)),
+                );
+                cx.notify();
+                return;
+            }
+        };
+
+        let model_id = selected_model.id.clone();
+        let provider = selected_model.provider.clone();
+        let config = self.config.clone();
+        let (tx, rx) = async_channel::bounded::<std::result::Result<std::path::PathBuf, String>>(1);
+
+        logging::log(
+            "AI_SCRIPT_GEN",
+            &format!(
+                "state=queued attempted=shift_tab_script_generation model_id={} provider={} prompt_len={}",
+                model_id,
+                provider,
+                prompt_description.len()
+            ),
+        );
+        self.show_hud("Generating script with AI...".to_string(), Some(1500), cx);
+
+        std::thread::spawn(move || {
+            logging::log(
+                "AI_SCRIPT_GEN",
+                &format!(
+                    "state=running attempted=generate_script model_id={} provider={}",
+                    model_id, provider
+                ),
+            );
+
+            let generation_result =
+                generate_script_via_ai_backend(&registry, &model_id, &prompt_description, &config)
+                    .map_err(|error| error.to_string());
+
+            if tx.send_blocking(generation_result).is_err() {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=send_result failure=result_channel_closed",
+                );
+            }
+        });
+
+        cx.spawn(async move |this, cx| {
+            let Ok(result) = rx.recv().await else {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=recv_result failure=result_channel_closed",
+                );
+                return;
+            };
+
+            let _ = cx.update(|cx| {
+                this.update(cx, |app, cx| {
+                    match result {
+                        Ok(script_path) => {
+                            let script_name = script_path
+                                .file_name()
+                                .and_then(|name| name.to_str())
+                                .unwrap_or("generated script");
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!(
+                                    "state=completed attempted=generate_script path={}",
+                                    script_path.display()
+                                ),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::success(
+                                    format!("Generated and opened {}", script_name),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(3500)),
+                            );
+                            app.close_and_reset_window(cx);
+                        }
+                        Err(error) => {
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!("state=failed attempted=generate_script error={}", error),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::error(
+                                    format!("Failed to generate script: {}", error),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(7000)),
+                            );
+                            cx.notify();
+                        }
+                    }
+                })
+            });
+        })
+        .detach();
+    }
+
     /// Rebuild the cached provider registry in a background thread.
     /// Called after config changes (API key saved, Claude Code enabled, etc.)
     pub fn rebuild_provider_registry_async(&mut self, cx: &mut Context<Self>) {
@@ -312,3 +641,44 @@
         .detach();
     }
 }
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_build_ai_script_generation_user_prompt_includes_description() {
+        let prompt = build_ai_script_generation_user_prompt("create a weather script");
+        assert!(prompt.contains("create a weather script"));
+        assert!(prompt.contains("Script Kit script"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_prefers_typescript_fence() {
+        let response = r#"
+Here's your script:
+```typescript
+// Name: Weather
+import "@scriptkit/sdk";
+```
+"#;
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert!(extracted.contains("// Name: Weather"));
+        assert!(!extracted.contains("```"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_falls_back_to_plain_text() {
+        let response = "// Name: Plain\nimport \"@scriptkit/sdk\";";
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert_eq!(extracted, "// Name: Plain\nimport \"@scriptkit/sdk\";\n");
+    }
+
+    #[test]
+    fn test_derive_script_name_from_description_uses_fallback_for_symbols() {
+        let name = derive_script_name_from_description("@@@ !!!");
+        assert_eq!(name, "ai-generated-script");
+    }
+}
diff --git a/src/app_impl/startup.rs b/src/app_impl/startup.rs
index 2e95070751083b8f4c24cd7eae8372d6bec3f6d6..441b4f23bca1b6294025c698eeb11d45815e85e0
--- a/src/app_impl/startup.rs
+++ b/src/app_impl/startup.rs
@@ -555,9 +555,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -720,20 +722,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/startup_new_tab.rs b/src/app_impl/startup_new_tab.rs
index 6c807a195553d9f168087c2ca6d771c76e555c11..7c84270cbe91c0b1ddd460aa29db8c788766d0cf
--- a/src/app_impl/startup_new_tab.rs
+++ b/src/app_impl/startup_new_tab.rs
@@ -6,9 +6,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -171,20 +173,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/tests.rs b/src/app_impl/tests.rs
index d75ba121851d7233c5ae77bc474a64b27e7c31d2..bb78d84e5a8ec9ba80c0a2287ad0c1cba93f226d
--- a/src/app_impl/tests.rs
+++ b/src/app_impl/tests.rs
@@ -1,6 +1,7 @@
 #[cfg(test)]
 mod app_impl_state_sync_tests {
     use super::{calculate_fallback_error_message, ScriptListApp};
+    use std::fs;
 
     #[test]
     fn test_sync_builtin_query_state_updates_query_and_selection_when_changed() {
@@ -46,4 +47,28 @@
         assert!(message.contains("Could not evaluate expression"));
         assert!(message.contains("Check the syntax and try again"));
     }
+
+    #[test]
+    fn test_shift_tab_routes_to_ai_script_generation_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("if has_shift")
+                && startup_tab.contains("this.generate_script_from_ai_prompt(query, cx);"),
+            "Shift+Tab in ScriptList should route to generate_script_from_ai_prompt. \
+             Missing expected branch in startup_new_tab.rs"
+        );
+    }
+
+    #[test]
+    fn test_tab_still_routes_to_inline_ai_chat_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("this.show_inline_ai_chat(Some(query), cx);"),
+            "Tab in ScriptList should continue routing to show_inline_ai_chat"
+        );
+    }
 }


thinking
**Reviewing editor open function**
exec
/bin/zsh -lc "sed -n '360,460p' src/script_creation/part_000.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 50ms:
                    if let Some(next) = chars.next() {
                        current.push(next);
                    } else {
                        current.push(ch);
                    }
                } else {
                    current.push(ch);
                }
            }
            None => match ch {
                '\'' | '"' => quote = Some(ch),
                '\\' => {
                    if let Some(next) = chars.next() {
                        current.push(next);
                    } else {
                        current.push(ch);
                    }
                }
                c if c.is_whitespace() => {
                    if !current.is_empty() {
file update:
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index 98a0c9cd1db5a933992c0f99cb20a3c2ca436b41..c985a5090574816267103019d1b272e1e3151a38
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -1,4 +1,194 @@
 use super::*;
+use anyhow::{anyhow, Context as AnyhowContext, Result};
+
+#[derive(Debug, Clone, Copy)]
+enum AiScriptGenerationStage {
+    SelectModel,
+    ResolveProvider,
+    RequestCompletion,
+    ExtractScript,
+    CreateScriptFile,
+    WriteScriptFile,
+    OpenEditor,
+}
+
+impl AiScriptGenerationStage {
+    fn as_str(self) -> &'static str {
+        match self {
+            Self::SelectModel => "select_model",
+            Self::ResolveProvider => "resolve_provider",
+            Self::RequestCompletion => "request_completion",
+            Self::ExtractScript => "extract_script",
+            Self::CreateScriptFile => "create_script_file",
+            Self::WriteScriptFile => "write_script_file",
+            Self::OpenEditor => "open_editor",
+        }
+    }
+}
+
+const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You are ScriptKitScriptGenerator.
+Generate a complete Script Kit TypeScript script and return code only.
+Requirements:
+- Include metadata comments at the top:
+  // Name: <script name>
+  // Description: <short summary>
+- Include: import "@scriptkit/sdk";
+- Use idiomatic Script Kit APIs (for example await arg(), await div(), await editor()) when useful.
+- Prefer clear async/await flow and practical defaults.
+- Return a full runnable script with no extra explanation."#;
+
+fn build_ai_script_generation_user_prompt(description: &str) -> String {
+    format!(
+        "Generate a complete Script Kit script for this request:\n\n{}\n\nReturn only the TypeScript script source.",
+        description.trim()
+    )
+}
+
+fn select_default_ai_script_model(
+    registry: &crate::ai::ProviderRegistry,
+) -> Option<crate::ai::ModelInfo> {
+    let all_models = registry.get_all_models();
+    all_models
+        .iter()
+        .find(|model| model.provider.eq_ignore_ascii_case("vercel"))
+        .cloned()
+        .or_else(|| all_models.first().cloned())
+}
+
+fn extract_generated_script_source(raw_response: &str) -> Option<String> {
+    let trimmed = raw_response.trim();
+    if trimmed.is_empty() {
+        return None;
+    }
+
+    // Prefer typed code fences first, then generic fences.
+    for fence in ["```typescript", "```ts", "```javascript", "```js", "```"] {
+        if let Some(content) = extract_first_fenced_block(trimmed, fence) {
+            return normalize_generated_script(content);
+        }
+    }
+
+    normalize_generated_script(trimmed.to_string())
+}
+
+fn extract_first_fenced_block(response: &str, fence_start: &str) -> Option<String> {
+    let start_index = response.find(fence_start)?;
+    let after_fence = &response[start_index + fence_start.len()..];
+    let after_newline = after_fence
+        .strip_prefix("\r\n")
+        .or_else(|| after_fence.strip_prefix('\n'))
+        .unwrap_or(after_fence);
+    let end_index = after_newline.find("```")?;
+    Some(after_newline[..end_index].trim().to_string())
+}
+
+fn normalize_generated_script(content: String) -> Option<String> {
+    let trimmed = content.trim();
+    if trimmed.is_empty() {
+        None
+    } else {
+        Some(format!("{trimmed}\n"))
+    }
+}
+
+fn derive_script_name_from_description(description: &str) -> String {
+    let mut slug = String::new();
+    let mut last_was_dash = false;
+
+    for ch in description.chars() {
+        let lower = ch.to_ascii_lowercase();
+        if lower.is_ascii_alphanumeric() {
+            slug.push(lower);
+            last_was_dash = false;
+        } else if !last_was_dash && !slug.is_empty() {
+            slug.push('-');
+            last_was_dash = true;
+        }
+
+        if slug.len() >= 48 {
+            break;
+        }
+    }
+
+    while slug.ends_with('-') {
+        slug.pop();
+    }
+
+    if slug.is_empty() {
+        "ai-generated-script".to_string()
+    } else {
+        slug
+    }
+}
+
+fn generate_script_via_ai_backend(
+    registry: &crate::ai::ProviderRegistry,
+    model_id: &str,
+    prompt_description: &str,
+    config: &crate::config::Config,
+) -> Result<std::path::PathBuf> {
+    let provider = registry
+        .find_provider_for_model(model_id)
+        .cloned()
+        .ok_or_else(|| {
+            anyhow!(
+                "state={} attempted=find_provider_for_model model_id={} failure=provider_not_found",
+                AiScriptGenerationStage::ResolveProvider.as_str(),
+                model_id
+            )
+        })?;
+
+    let request_messages = vec![
+        crate::ai::ProviderMessage::system(AI_SCRIPT_GENERATION_SYSTEM_PROMPT),
+        crate::ai::ProviderMessage::user(build_ai_script_generation_user_prompt(prompt_description)),
+    ];
+
+    let ai_response = provider
+        .send_message(&request_messages, model_id)
+        .with_context(|| {
+            format!(
+                "state={} attempted=send_message model_id={} provider={} failure=provider_call_failed",
+                AiScriptGenerationStage::RequestCompletion.as_str(),
+                model_id,
+                provider.provider_id()
+            )
+        })?;
+
+    let generated_script = extract_generated_script_source(&ai_response).ok_or_else(|| {
+        anyhow!(
+            "state={} attempted=extract_generated_script model_id={} failure=empty_script_response",
+            AiScriptGenerationStage::ExtractScript.as_str(),
+            model_id
+        )
+    })?;
+
+    let script_name = derive_script_name_from_description(prompt_description);
+    let script_path = crate::script_creation::create_new_script(&script_name).with_context(|| {
+        format!(
+            "state={} attempted=create_new_script name={} failure=create_script_failed",
+            AiScriptGenerationStage::CreateScriptFile.as_str(),
+            script_name
+        )
+    })?;
+
+    std::fs::write(&script_path, generated_script).with_context(|| {
+        format!(
+            "state={} attempted=write_script path={} failure=write_failed",
+            AiScriptGenerationStage::WriteScriptFile.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    crate::script_creation::open_in_editor(&script_path, config).with_context(|| {
+        format!(
+            "state={} attempted=open_in_editor path={} failure=open_editor_failed",
+            AiScriptGenerationStage::OpenEditor.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
 
 impl ScriptListApp {
     pub(crate) fn is_in_prompt(&self) -> bool {
@@ -274,6 +464,145 @@
         cx.notify();
     }
 
+    /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
+    /// The generated script is saved to disk and opened in the configured editor.
+    pub fn generate_script_from_ai_prompt(
+        &mut self,
+        prompt_description: String,
+        cx: &mut Context<Self>,
+    ) {
+        let prompt_description = prompt_description.trim().to_string();
+        if prompt_description.is_empty() {
+            return;
+        }
+
+        let registry = self.cached_provider_registry.clone().unwrap_or_else(|| {
+            crate::ai::ProviderRegistry::from_environment_with_config(Some(&self.config))
+        });
+
+        if !registry.has_any_provider() {
+            self.toast_manager.push(
+                components::toast::Toast::error("No AI providers configured for script generation", &self.theme)
+                    .duration_ms(Some(5000)),
+            );
+            cx.notify();
+            return;
+        }
+
+        let selected_model = match select_default_ai_script_model(&registry) {
+            Some(model) => model,
+            None => {
+                let stage = AiScriptGenerationStage::SelectModel.as_str();
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    &format!(
+                        "state={} attempted=select_default_model failure=no_available_models",
+                        stage
+                    ),
+                );
+                self.toast_manager.push(
+                    components::toast::Toast::error(
+                        "No AI models available for script generation",
+                        &self.theme,
+                    )
+                    .duration_ms(Some(5000)),
+                );
+                cx.notify();
+                return;
+            }
+        };
+
+        let model_id = selected_model.id.clone();
+        let provider = selected_model.provider.clone();
+        let config = self.config.clone();
+        let (tx, rx) = async_channel::bounded::<std::result::Result<std::path::PathBuf, String>>(1);
+
+        logging::log(
+            "AI_SCRIPT_GEN",
+            &format!(
+                "state=queued attempted=shift_tab_script_generation model_id={} provider={} prompt_len={}",
+                model_id,
+                provider,
+                prompt_description.len()
+            ),
+        );
+        self.show_hud("Generating script with AI...".to_string(), Some(1500), cx);
+
+        std::thread::spawn(move || {
+            logging::log(
+                "AI_SCRIPT_GEN",
+                &format!(
+                    "state=running attempted=generate_script model_id={} provider={}",
+                    model_id, provider
+                ),
+            );
+
+            let generation_result =
+                generate_script_via_ai_backend(&registry, &model_id, &prompt_description, &config)
+                    .map_err(|error| error.to_string());
+
+            if tx.send_blocking(generation_result).is_err() {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=send_result failure=result_channel_closed",
+                );
+            }
+        });
+
+        cx.spawn(async move |this, cx| {
+            let Ok(result) = rx.recv().await else {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=recv_result failure=result_channel_closed",
+                );
+                return;
+            };
+
+            let _ = cx.update(|cx| {
+                this.update(cx, |app, cx| {
+                    match result {
+                        Ok(script_path) => {
+                            let script_name = script_path
+                                .file_name()
+                                .and_then(|name| name.to_str())
+                                .unwrap_or("generated script");
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!(
+                                    "state=completed attempted=generate_script path={}",
+                                    script_path.display()
+                                ),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::success(
+                                    format!("Generated and opened {}", script_name),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(3500)),
+                            );
+                            app.close_and_reset_window(cx);
+                        }
+                        Err(error) => {
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!("state=failed attempted=generate_script error={}", error),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::error(
+                                    format!("Failed to generate script: {}", error),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(7000)),
+                            );
+                            cx.notify();
+                        }
+                    }
+                })
+            });
+        })
+        .detach();
+    }
+
     /// Rebuild the cached provider registry in a background thread.
     /// Called after config changes (API key saved, Claude Code enabled, etc.)
     pub fn rebuild_provider_registry_async(&mut self, cx: &mut Context<Self>) {
@@ -312,3 +641,44 @@
         .detach();
     }
 }
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_build_ai_script_generation_user_prompt_includes_description() {
+        let prompt = build_ai_script_generation_user_prompt("create a weather script");
+        assert!(prompt.contains("create a weather script"));
+        assert!(prompt.contains("Script Kit script"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_prefers_typescript_fence() {
+        let response = r#"
+Here's your script:
+```typescript
+// Name: Weather
+import "@scriptkit/sdk";
+```
+"#;
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert!(extracted.contains("// Name: Weather"));
+        assert!(!extracted.contains("```"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_falls_back_to_plain_text() {
+        let response = "// Name: Plain\nimport \"@scriptkit/sdk\";";
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert_eq!(extracted, "// Name: Plain\nimport \"@scriptkit/sdk\";\n");
+    }
+
+    #[test]
+    fn test_derive_script_name_from_description_uses_fallback_for_symbols() {
+        let name = derive_script_name_from_description("@@@ !!!");
+        assert_eq!(name, "ai-generated-script");
+    }
+}
diff --git a/src/app_impl/startup.rs b/src/app_impl/startup.rs
index 2e95070751083b8f4c24cd7eae8372d6bec3f6d6..441b4f23bca1b6294025c698eeb11d45815e85e0
--- a/src/app_impl/startup.rs
+++ b/src/app_impl/startup.rs
@@ -555,9 +555,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -720,20 +722,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/startup_new_tab.rs b/src/app_impl/startup_new_tab.rs
index 6c807a195553d9f168087c2ca6d771c76e555c11..7c84270cbe91c0b1ddd460aa29db8c788766d0cf
--- a/src/app_impl/startup_new_tab.rs
+++ b/src/app_impl/startup_new_tab.rs
@@ -6,9 +6,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -171,20 +173,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/tests.rs b/src/app_impl/tests.rs
index d75ba121851d7233c5ae77bc474a64b27e7c31d2..bb78d84e5a8ec9ba80c0a2287ad0c1cba93f226d
--- a/src/app_impl/tests.rs
+++ b/src/app_impl/tests.rs
@@ -1,6 +1,7 @@
 #[cfg(test)]
 mod app_impl_state_sync_tests {
     use super::{calculate_fallback_error_message, ScriptListApp};
+    use std::fs;
 
     #[test]
     fn test_sync_builtin_query_state_updates_query_and_selection_when_changed() {
@@ -46,4 +47,28 @@
         assert!(message.contains("Could not evaluate expression"));
         assert!(message.contains("Check the syntax and try again"));
     }
+
+    #[test]
+    fn test_shift_tab_routes_to_ai_script_generation_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("if has_shift")
+                && startup_tab.contains("this.generate_script_from_ai_prompt(query, cx);"),
+            "Shift+Tab in ScriptList should route to generate_script_from_ai_prompt. \
+             Missing expected branch in startup_new_tab.rs"
+        );
+    }
+
+    #[test]
+    fn test_tab_still_routes_to_inline_ai_chat_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("this.show_inline_ai_chat(Some(query), cx);"),
+            "Tab in ScriptList should continue routing to show_inline_ai_chat"
+        );
+    }
 }


thinking
**Validating key variant handling**
codex
I’m adding one small extra test assertion to lock in the explicit key-variant requirement (`\"tab\" | \"Tab\"`) in the tab interceptor source, then I’ll re-run a quick local check and re-release claims.
exec
/bin/zsh -lc '~/.claude/bin/swarm.ts claim --wait --id codex-shift-tab-keybind src/app_impl/tests.rs' in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
Claimed for codex-shift-tab-keybind: src/app_impl/tests.rs
file update:
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index 98a0c9cd1db5a933992c0f99cb20a3c2ca436b41..c985a5090574816267103019d1b272e1e3151a38
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -1,4 +1,194 @@
 use super::*;
+use anyhow::{anyhow, Context as AnyhowContext, Result};
+
+#[derive(Debug, Clone, Copy)]
+enum AiScriptGenerationStage {
+    SelectModel,
+    ResolveProvider,
+    RequestCompletion,
+    ExtractScript,
+    CreateScriptFile,
+    WriteScriptFile,
+    OpenEditor,
+}
+
+impl AiScriptGenerationStage {
+    fn as_str(self) -> &'static str {
+        match self {
+            Self::SelectModel => "select_model",
+            Self::ResolveProvider => "resolve_provider",
+            Self::RequestCompletion => "request_completion",
+            Self::ExtractScript => "extract_script",
+            Self::CreateScriptFile => "create_script_file",
+            Self::WriteScriptFile => "write_script_file",
+            Self::OpenEditor => "open_editor",
+        }
+    }
+}
+
+const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You are ScriptKitScriptGenerator.
+Generate a complete Script Kit TypeScript script and return code only.
+Requirements:
+- Include metadata comments at the top:
+  // Name: <script name>
+  // Description: <short summary>
+- Include: import "@scriptkit/sdk";
+- Use idiomatic Script Kit APIs (for example await arg(), await div(), await editor()) when useful.
+- Prefer clear async/await flow and practical defaults.
+- Return a full runnable script with no extra explanation."#;
+
+fn build_ai_script_generation_user_prompt(description: &str) -> String {
+    format!(
+        "Generate a complete Script Kit script for this request:\n\n{}\n\nReturn only the TypeScript script source.",
+        description.trim()
+    )
+}
+
+fn select_default_ai_script_model(
+    registry: &crate::ai::ProviderRegistry,
+) -> Option<crate::ai::ModelInfo> {
+    let all_models = registry.get_all_models();
+    all_models
+        .iter()
+        .find(|model| model.provider.eq_ignore_ascii_case("vercel"))
+        .cloned()
+        .or_else(|| all_models.first().cloned())
+}
+
+fn extract_generated_script_source(raw_response: &str) -> Option<String> {
+    let trimmed = raw_response.trim();
+    if trimmed.is_empty() {
+        return None;
+    }
+
+    // Prefer typed code fences first, then generic fences.
+    for fence in ["```typescript", "```ts", "```javascript", "```js", "```"] {
+        if let Some(content) = extract_first_fenced_block(trimmed, fence) {
+            return normalize_generated_script(content);
+        }
+    }
+
+    normalize_generated_script(trimmed.to_string())
+}
+
+fn extract_first_fenced_block(response: &str, fence_start: &str) -> Option<String> {
+    let start_index = response.find(fence_start)?;
+    let after_fence = &response[start_index + fence_start.len()..];
+    let after_newline = after_fence
+        .strip_prefix("\r\n")
+        .or_else(|| after_fence.strip_prefix('\n'))
+        .unwrap_or(after_fence);
+    let end_index = after_newline.find("```")?;
+    Some(after_newline[..end_index].trim().to_string())
+}
+
+fn normalize_generated_script(content: String) -> Option<String> {
+    let trimmed = content.trim();
+    if trimmed.is_empty() {
+        None
+    } else {
+        Some(format!("{trimmed}\n"))
+    }
+}
+
+fn derive_script_name_from_description(description: &str) -> String {
+    let mut slug = String::new();
+    let mut last_was_dash = false;
+
+    for ch in description.chars() {
+        let lower = ch.to_ascii_lowercase();
+        if lower.is_ascii_alphanumeric() {
+            slug.push(lower);
+            last_was_dash = false;
+        } else if !last_was_dash && !slug.is_empty() {
+            slug.push('-');
+            last_was_dash = true;
+        }
+
+        if slug.len() >= 48 {
+            break;
+        }
+    }
+
+    while slug.ends_with('-') {
+        slug.pop();
+    }
+
+    if slug.is_empty() {
+        "ai-generated-script".to_string()
+    } else {
+        slug
+    }
+}
+
+fn generate_script_via_ai_backend(
+    registry: &crate::ai::ProviderRegistry,
+    model_id: &str,
+    prompt_description: &str,
+    config: &crate::config::Config,
+) -> Result<std::path::PathBuf> {
+    let provider = registry
+        .find_provider_for_model(model_id)
+        .cloned()
+        .ok_or_else(|| {
+            anyhow!(
+                "state={} attempted=find_provider_for_model model_id={} failure=provider_not_found",
+                AiScriptGenerationStage::ResolveProvider.as_str(),
+                model_id
+            )
+        })?;
+
+    let request_messages = vec![
+        crate::ai::ProviderMessage::system(AI_SCRIPT_GENERATION_SYSTEM_PROMPT),
+        crate::ai::ProviderMessage::user(build_ai_script_generation_user_prompt(prompt_description)),
+    ];
+
+    let ai_response = provider
+        .send_message(&request_messages, model_id)
+        .with_context(|| {
+            format!(
+                "state={} attempted=send_message model_id={} provider={} failure=provider_call_failed",
+                AiScriptGenerationStage::RequestCompletion.as_str(),
+                model_id,
+                provider.provider_id()
+            )
+        })?;
+
+    let generated_script = extract_generated_script_source(&ai_response).ok_or_else(|| {
+        anyhow!(
+            "state={} attempted=extract_generated_script model_id={} failure=empty_script_response",
+            AiScriptGenerationStage::ExtractScript.as_str(),
+            model_id
+        )
+    })?;
+
+    let script_name = derive_script_name_from_description(prompt_description);
+    let script_path = crate::script_creation::create_new_script(&script_name).with_context(|| {
+        format!(
+            "state={} attempted=create_new_script name={} failure=create_script_failed",
+            AiScriptGenerationStage::CreateScriptFile.as_str(),
+            script_name
+        )
+    })?;
+
+    std::fs::write(&script_path, generated_script).with_context(|| {
+        format!(
+            "state={} attempted=write_script path={} failure=write_failed",
+            AiScriptGenerationStage::WriteScriptFile.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    crate::script_creation::open_in_editor(&script_path, config).with_context(|| {
+        format!(
+            "state={} attempted=open_in_editor path={} failure=open_editor_failed",
+            AiScriptGenerationStage::OpenEditor.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
 
 impl ScriptListApp {
     pub(crate) fn is_in_prompt(&self) -> bool {
@@ -274,6 +464,145 @@
         cx.notify();
     }
 
+    /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
+    /// The generated script is saved to disk and opened in the configured editor.
+    pub fn generate_script_from_ai_prompt(
+        &mut self,
+        prompt_description: String,
+        cx: &mut Context<Self>,
+    ) {
+        let prompt_description = prompt_description.trim().to_string();
+        if prompt_description.is_empty() {
+            return;
+        }
+
+        let registry = self.cached_provider_registry.clone().unwrap_or_else(|| {
+            crate::ai::ProviderRegistry::from_environment_with_config(Some(&self.config))
+        });
+
+        if !registry.has_any_provider() {
+            self.toast_manager.push(
+                components::toast::Toast::error("No AI providers configured for script generation", &self.theme)
+                    .duration_ms(Some(5000)),
+            );
+            cx.notify();
+            return;
+        }
+
+        let selected_model = match select_default_ai_script_model(&registry) {
+            Some(model) => model,
+            None => {
+                let stage = AiScriptGenerationStage::SelectModel.as_str();
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    &format!(
+                        "state={} attempted=select_default_model failure=no_available_models",
+                        stage
+                    ),
+                );
+                self.toast_manager.push(
+                    components::toast::Toast::error(
+                        "No AI models available for script generation",
+                        &self.theme,
+                    )
+                    .duration_ms(Some(5000)),
+                );
+                cx.notify();
+                return;
+            }
+        };
+
+        let model_id = selected_model.id.clone();
+        let provider = selected_model.provider.clone();
+        let config = self.config.clone();
+        let (tx, rx) = async_channel::bounded::<std::result::Result<std::path::PathBuf, String>>(1);
+
+        logging::log(
+            "AI_SCRIPT_GEN",
+            &format!(
+                "state=queued attempted=shift_tab_script_generation model_id={} provider={} prompt_len={}",
+                model_id,
+                provider,
+                prompt_description.len()
+            ),
+        );
+        self.show_hud("Generating script with AI...".to_string(), Some(1500), cx);
+
+        std::thread::spawn(move || {
+            logging::log(
+                "AI_SCRIPT_GEN",
+                &format!(
+                    "state=running attempted=generate_script model_id={} provider={}",
+                    model_id, provider
+                ),
+            );
+
+            let generation_result =
+                generate_script_via_ai_backend(&registry, &model_id, &prompt_description, &config)
+                    .map_err(|error| error.to_string());
+
+            if tx.send_blocking(generation_result).is_err() {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=send_result failure=result_channel_closed",
+                );
+            }
+        });
+
+        cx.spawn(async move |this, cx| {
+            let Ok(result) = rx.recv().await else {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=recv_result failure=result_channel_closed",
+                );
+                return;
+            };
+
+            let _ = cx.update(|cx| {
+                this.update(cx, |app, cx| {
+                    match result {
+                        Ok(script_path) => {
+                            let script_name = script_path
+                                .file_name()
+                                .and_then(|name| name.to_str())
+                                .unwrap_or("generated script");
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!(
+                                    "state=completed attempted=generate_script path={}",
+                                    script_path.display()
+                                ),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::success(
+                                    format!("Generated and opened {}", script_name),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(3500)),
+                            );
+                            app.close_and_reset_window(cx);
+                        }
+                        Err(error) => {
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!("state=failed attempted=generate_script error={}", error),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::error(
+                                    format!("Failed to generate script: {}", error),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(7000)),
+                            );
+                            cx.notify();
+                        }
+                    }
+                })
+            });
+        })
+        .detach();
+    }
+
     /// Rebuild the cached provider registry in a background thread.
     /// Called after config changes (API key saved, Claude Code enabled, etc.)
     pub fn rebuild_provider_registry_async(&mut self, cx: &mut Context<Self>) {
@@ -312,3 +641,44 @@
         .detach();
     }
 }
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_build_ai_script_generation_user_prompt_includes_description() {
+        let prompt = build_ai_script_generation_user_prompt("create a weather script");
+        assert!(prompt.contains("create a weather script"));
+        assert!(prompt.contains("Script Kit script"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_prefers_typescript_fence() {
+        let response = r#"
+Here's your script:
+```typescript
+// Name: Weather
+import "@scriptkit/sdk";
+```
+"#;
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert!(extracted.contains("// Name: Weather"));
+        assert!(!extracted.contains("```"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_falls_back_to_plain_text() {
+        let response = "// Name: Plain\nimport \"@scriptkit/sdk\";";
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert_eq!(extracted, "// Name: Plain\nimport \"@scriptkit/sdk\";\n");
+    }
+
+    #[test]
+    fn test_derive_script_name_from_description_uses_fallback_for_symbols() {
+        let name = derive_script_name_from_description("@@@ !!!");
+        assert_eq!(name, "ai-generated-script");
+    }
+}
diff --git a/src/app_impl/startup.rs b/src/app_impl/startup.rs
index 2e95070751083b8f4c24cd7eae8372d6bec3f6d6..441b4f23bca1b6294025c698eeb11d45815e85e0
--- a/src/app_impl/startup.rs
+++ b/src/app_impl/startup.rs
@@ -555,9 +555,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -720,20 +722,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/startup_new_tab.rs b/src/app_impl/startup_new_tab.rs
index 6c807a195553d9f168087c2ca6d771c76e555c11..7c84270cbe91c0b1ddd460aa29db8c788766d0cf
--- a/src/app_impl/startup_new_tab.rs
+++ b/src/app_impl/startup_new_tab.rs
@@ -6,9 +6,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -171,20 +173,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/tests.rs b/src/app_impl/tests.rs
index d75ba121851d7233c5ae77bc474a64b27e7c31d2..bb78d84e5a8ec9ba80c0a2287ad0c1cba93f226d
--- a/src/app_impl/tests.rs
+++ b/src/app_impl/tests.rs
@@ -1,6 +1,7 @@
 #[cfg(test)]
 mod app_impl_state_sync_tests {
     use super::{calculate_fallback_error_message, ScriptListApp};
+    use std::fs;
 
     #[test]
     fn test_sync_builtin_query_state_updates_query_and_selection_when_changed() {
@@ -46,4 +47,28 @@
         assert!(message.contains("Could not evaluate expression"));
         assert!(message.contains("Check the syntax and try again"));
     }
+
+    #[test]
+    fn test_shift_tab_routes_to_ai_script_generation_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("if has_shift")
+                && startup_tab.contains("this.generate_script_from_ai_prompt(query, cx);"),
+            "Shift+Tab in ScriptList should route to generate_script_from_ai_prompt. \
+             Missing expected branch in startup_new_tab.rs"
+        );
+    }
+
+    #[test]
+    fn test_tab_still_routes_to_inline_ai_chat_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("this.show_inline_ai_chat(Some(query), cx);"),
+            "Tab in ScriptList should continue routing to show_inline_ai_chat"
+        );
+    }
 }


thinking
**Inspecting test file for patch**
exec
/bin/zsh -lc "sed -n '1,140p' src/app_impl/tests.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 50ms:
#[cfg(test)]
mod app_impl_state_sync_tests {
    use super::{calculate_fallback_error_message, ScriptListApp};
    use std::fs;

    #[test]
    fn test_sync_builtin_query_state_updates_query_and_selection_when_changed() {
        let mut query = String::from("old");
        let mut selected_index = 3;

        let changed =
            ScriptListApp::sync_builtin_query_state(&mut query, &mut selected_index, "new");

        assert!(changed);
        assert_eq!(query, "new");
        assert_eq!(selected_index, 0);
    }

    #[test]
    fn test_sync_builtin_query_state_noop_when_query_is_unchanged() {
file update:
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index 98a0c9cd1db5a933992c0f99cb20a3c2ca436b41..c985a5090574816267103019d1b272e1e3151a38
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -1,4 +1,194 @@
 use super::*;
+use anyhow::{anyhow, Context as AnyhowContext, Result};
+
+#[derive(Debug, Clone, Copy)]
+enum AiScriptGenerationStage {
+    SelectModel,
+    ResolveProvider,
+    RequestCompletion,
+    ExtractScript,
+    CreateScriptFile,
+    WriteScriptFile,
+    OpenEditor,
+}
+
+impl AiScriptGenerationStage {
+    fn as_str(self) -> &'static str {
+        match self {
+            Self::SelectModel => "select_model",
+            Self::ResolveProvider => "resolve_provider",
+            Self::RequestCompletion => "request_completion",
+            Self::ExtractScript => "extract_script",
+            Self::CreateScriptFile => "create_script_file",
+            Self::WriteScriptFile => "write_script_file",
+            Self::OpenEditor => "open_editor",
+        }
+    }
+}
+
+const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You are ScriptKitScriptGenerator.
+Generate a complete Script Kit TypeScript script and return code only.
+Requirements:
+- Include metadata comments at the top:
+  // Name: <script name>
+  // Description: <short summary>
+- Include: import "@scriptkit/sdk";
+- Use idiomatic Script Kit APIs (for example await arg(), await div(), await editor()) when useful.
+- Prefer clear async/await flow and practical defaults.
+- Return a full runnable script with no extra explanation."#;
+
+fn build_ai_script_generation_user_prompt(description: &str) -> String {
+    format!(
+        "Generate a complete Script Kit script for this request:\n\n{}\n\nReturn only the TypeScript script source.",
+        description.trim()
+    )
+}
+
+fn select_default_ai_script_model(
+    registry: &crate::ai::ProviderRegistry,
+) -> Option<crate::ai::ModelInfo> {
+    let all_models = registry.get_all_models();
+    all_models
+        .iter()
+        .find(|model| model.provider.eq_ignore_ascii_case("vercel"))
+        .cloned()
+        .or_else(|| all_models.first().cloned())
+}
+
+fn extract_generated_script_source(raw_response: &str) -> Option<String> {
+    let trimmed = raw_response.trim();
+    if trimmed.is_empty() {
+        return None;
+    }
+
+    // Prefer typed code fences first, then generic fences.
+    for fence in ["```typescript", "```ts", "```javascript", "```js", "```"] {
+        if let Some(content) = extract_first_fenced_block(trimmed, fence) {
+            return normalize_generated_script(content);
+        }
+    }
+
+    normalize_generated_script(trimmed.to_string())
+}
+
+fn extract_first_fenced_block(response: &str, fence_start: &str) -> Option<String> {
+    let start_index = response.find(fence_start)?;
+    let after_fence = &response[start_index + fence_start.len()..];
+    let after_newline = after_fence
+        .strip_prefix("\r\n")
+        .or_else(|| after_fence.strip_prefix('\n'))
+        .unwrap_or(after_fence);
+    let end_index = after_newline.find("```")?;
+    Some(after_newline[..end_index].trim().to_string())
+}
+
+fn normalize_generated_script(content: String) -> Option<String> {
+    let trimmed = content.trim();
+    if trimmed.is_empty() {
+        None
+    } else {
+        Some(format!("{trimmed}\n"))
+    }
+}
+
+fn derive_script_name_from_description(description: &str) -> String {
+    let mut slug = String::new();
+    let mut last_was_dash = false;
+
+    for ch in description.chars() {
+        let lower = ch.to_ascii_lowercase();
+        if lower.is_ascii_alphanumeric() {
+            slug.push(lower);
+            last_was_dash = false;
+        } else if !last_was_dash && !slug.is_empty() {
+            slug.push('-');
+            last_was_dash = true;
+        }
+
+        if slug.len() >= 48 {
+            break;
+        }
+    }
+
+    while slug.ends_with('-') {
+        slug.pop();
+    }
+
+    if slug.is_empty() {
+        "ai-generated-script".to_string()
+    } else {
+        slug
+    }
+}
+
+fn generate_script_via_ai_backend(
+    registry: &crate::ai::ProviderRegistry,
+    model_id: &str,
+    prompt_description: &str,
+    config: &crate::config::Config,
+) -> Result<std::path::PathBuf> {
+    let provider = registry
+        .find_provider_for_model(model_id)
+        .cloned()
+        .ok_or_else(|| {
+            anyhow!(
+                "state={} attempted=find_provider_for_model model_id={} failure=provider_not_found",
+                AiScriptGenerationStage::ResolveProvider.as_str(),
+                model_id
+            )
+        })?;
+
+    let request_messages = vec![
+        crate::ai::ProviderMessage::system(AI_SCRIPT_GENERATION_SYSTEM_PROMPT),
+        crate::ai::ProviderMessage::user(build_ai_script_generation_user_prompt(prompt_description)),
+    ];
+
+    let ai_response = provider
+        .send_message(&request_messages, model_id)
+        .with_context(|| {
+            format!(
+                "state={} attempted=send_message model_id={} provider={} failure=provider_call_failed",
+                AiScriptGenerationStage::RequestCompletion.as_str(),
+                model_id,
+                provider.provider_id()
+            )
+        })?;
+
+    let generated_script = extract_generated_script_source(&ai_response).ok_or_else(|| {
+        anyhow!(
+            "state={} attempted=extract_generated_script model_id={} failure=empty_script_response",
+            AiScriptGenerationStage::ExtractScript.as_str(),
+            model_id
+        )
+    })?;
+
+    let script_name = derive_script_name_from_description(prompt_description);
+    let script_path = crate::script_creation::create_new_script(&script_name).with_context(|| {
+        format!(
+            "state={} attempted=create_new_script name={} failure=create_script_failed",
+            AiScriptGenerationStage::CreateScriptFile.as_str(),
+            script_name
+        )
+    })?;
+
+    std::fs::write(&script_path, generated_script).with_context(|| {
+        format!(
+            "state={} attempted=write_script path={} failure=write_failed",
+            AiScriptGenerationStage::WriteScriptFile.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    crate::script_creation::open_in_editor(&script_path, config).with_context(|| {
+        format!(
+            "state={} attempted=open_in_editor path={} failure=open_editor_failed",
+            AiScriptGenerationStage::OpenEditor.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
 
 impl ScriptListApp {
     pub(crate) fn is_in_prompt(&self) -> bool {
@@ -274,6 +464,145 @@
         cx.notify();
     }
 
+    /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
+    /// The generated script is saved to disk and opened in the configured editor.
+    pub fn generate_script_from_ai_prompt(
+        &mut self,
+        prompt_description: String,
+        cx: &mut Context<Self>,
+    ) {
+        let prompt_description = prompt_description.trim().to_string();
+        if prompt_description.is_empty() {
+            return;
+        }
+
+        let registry = self.cached_provider_registry.clone().unwrap_or_else(|| {
+            crate::ai::ProviderRegistry::from_environment_with_config(Some(&self.config))
+        });
+
+        if !registry.has_any_provider() {
+            self.toast_manager.push(
+                components::toast::Toast::error("No AI providers configured for script generation", &self.theme)
+                    .duration_ms(Some(5000)),
+            );
+            cx.notify();
+            return;
+        }
+
+        let selected_model = match select_default_ai_script_model(&registry) {
+            Some(model) => model,
+            None => {
+                let stage = AiScriptGenerationStage::SelectModel.as_str();
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    &format!(
+                        "state={} attempted=select_default_model failure=no_available_models",
+                        stage
+                    ),
+                );
+                self.toast_manager.push(
+                    components::toast::Toast::error(
+                        "No AI models available for script generation",
+                        &self.theme,
+                    )
+                    .duration_ms(Some(5000)),
+                );
+                cx.notify();
+                return;
+            }
+        };
+
+        let model_id = selected_model.id.clone();
+        let provider = selected_model.provider.clone();
+        let config = self.config.clone();
+        let (tx, rx) = async_channel::bounded::<std::result::Result<std::path::PathBuf, String>>(1);
+
+        logging::log(
+            "AI_SCRIPT_GEN",
+            &format!(
+                "state=queued attempted=shift_tab_script_generation model_id={} provider={} prompt_len={}",
+                model_id,
+                provider,
+                prompt_description.len()
+            ),
+        );
+        self.show_hud("Generating script with AI...".to_string(), Some(1500), cx);
+
+        std::thread::spawn(move || {
+            logging::log(
+                "AI_SCRIPT_GEN",
+                &format!(
+                    "state=running attempted=generate_script model_id={} provider={}",
+                    model_id, provider
+                ),
+            );
+
+            let generation_result =
+                generate_script_via_ai_backend(&registry, &model_id, &prompt_description, &config)
+                    .map_err(|error| error.to_string());
+
+            if tx.send_blocking(generation_result).is_err() {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=send_result failure=result_channel_closed",
+                );
+            }
+        });
+
+        cx.spawn(async move |this, cx| {
+            let Ok(result) = rx.recv().await else {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=recv_result failure=result_channel_closed",
+                );
+                return;
+            };
+
+            let _ = cx.update(|cx| {
+                this.update(cx, |app, cx| {
+                    match result {
+                        Ok(script_path) => {
+                            let script_name = script_path
+                                .file_name()
+                                .and_then(|name| name.to_str())
+                                .unwrap_or("generated script");
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!(
+                                    "state=completed attempted=generate_script path={}",
+                                    script_path.display()
+                                ),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::success(
+                                    format!("Generated and opened {}", script_name),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(3500)),
+                            );
+                            app.close_and_reset_window(cx);
+                        }
+                        Err(error) => {
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!("state=failed attempted=generate_script error={}", error),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::error(
+                                    format!("Failed to generate script: {}", error),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(7000)),
+                            );
+                            cx.notify();
+                        }
+                    }
+                })
+            });
+        })
+        .detach();
+    }
+
     /// Rebuild the cached provider registry in a background thread.
     /// Called after config changes (API key saved, Claude Code enabled, etc.)
     pub fn rebuild_provider_registry_async(&mut self, cx: &mut Context<Self>) {
@@ -312,3 +641,44 @@
         .detach();
     }
 }
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_build_ai_script_generation_user_prompt_includes_description() {
+        let prompt = build_ai_script_generation_user_prompt("create a weather script");
+        assert!(prompt.contains("create a weather script"));
+        assert!(prompt.contains("Script Kit script"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_prefers_typescript_fence() {
+        let response = r#"
+Here's your script:
+```typescript
+// Name: Weather
+import "@scriptkit/sdk";
+```
+"#;
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert!(extracted.contains("// Name: Weather"));
+        assert!(!extracted.contains("```"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_falls_back_to_plain_text() {
+        let response = "// Name: Plain\nimport \"@scriptkit/sdk\";";
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert_eq!(extracted, "// Name: Plain\nimport \"@scriptkit/sdk\";\n");
+    }
+
+    #[test]
+    fn test_derive_script_name_from_description_uses_fallback_for_symbols() {
+        let name = derive_script_name_from_description("@@@ !!!");
+        assert_eq!(name, "ai-generated-script");
+    }
+}
diff --git a/src/app_impl/startup.rs b/src/app_impl/startup.rs
index 2e95070751083b8f4c24cd7eae8372d6bec3f6d6..441b4f23bca1b6294025c698eeb11d45815e85e0
--- a/src/app_impl/startup.rs
+++ b/src/app_impl/startup.rs
@@ -555,9 +555,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -720,20 +722,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/startup_new_tab.rs b/src/app_impl/startup_new_tab.rs
index 6c807a195553d9f168087c2ca6d771c76e555c11..7c84270cbe91c0b1ddd460aa29db8c788766d0cf
--- a/src/app_impl/startup_new_tab.rs
+++ b/src/app_impl/startup_new_tab.rs
@@ -6,9 +6,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -171,20 +173,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/tests.rs b/src/app_impl/tests.rs
index d75ba121851d7233c5ae77bc474a64b27e7c31d2..bb78d84e5a8ec9ba80c0a2287ad0c1cba93f226d
--- a/src/app_impl/tests.rs
+++ b/src/app_impl/tests.rs
@@ -1,6 +1,7 @@
 #[cfg(test)]
 mod app_impl_state_sync_tests {
     use super::{calculate_fallback_error_message, ScriptListApp};
+    use std::fs;
 
     #[test]
     fn test_sync_builtin_query_state_updates_query_and_selection_when_changed() {
@@ -46,4 +47,28 @@
         assert!(message.contains("Could not evaluate expression"));
         assert!(message.contains("Check the syntax and try again"));
     }
+
+    #[test]
+    fn test_shift_tab_routes_to_ai_script_generation_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("if has_shift")
+                && startup_tab.contains("this.generate_script_from_ai_prompt(query, cx);"),
+            "Shift+Tab in ScriptList should route to generate_script_from_ai_prompt. \
+             Missing expected branch in startup_new_tab.rs"
+        );
+    }
+
+    #[test]
+    fn test_tab_still_routes_to_inline_ai_chat_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("this.show_inline_ai_chat(Some(query), cx);"),
+            "Tab in ScriptList should continue routing to show_inline_ai_chat"
+        );
+    }
 }


thinking
**Applying grammar patch**
file update
M /Users/johnlindquist/dev/script-kit-gpui/src/app_impl/tests.rs
@@ -73,2 +73,13 @@
     }
+
+    #[test]
+    fn test_tab_interceptor_matches_both_tab_key_variants() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("matches!(event.keystroke.key.as_str(), \"tab\" | \"Tab\")"),
+            "Tab interceptor should match both tab key variants: \"tab\" and \"Tab\""
+        );
+    }
 }
apply_patch(auto_approved=true) exited 0 in 27ms:
Success. Updated the following files:
M src/app_impl/tests.rs
file update:
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index 98a0c9cd1db5a933992c0f99cb20a3c2ca436b41..c985a5090574816267103019d1b272e1e3151a38
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -1,4 +1,194 @@
 use super::*;
+use anyhow::{anyhow, Context as AnyhowContext, Result};
+
+#[derive(Debug, Clone, Copy)]
+enum AiScriptGenerationStage {
+    SelectModel,
+    ResolveProvider,
+    RequestCompletion,
+    ExtractScript,
+    CreateScriptFile,
+    WriteScriptFile,
+    OpenEditor,
+}
+
+impl AiScriptGenerationStage {
+    fn as_str(self) -> &'static str {
+        match self {
+            Self::SelectModel => "select_model",
+            Self::ResolveProvider => "resolve_provider",
+            Self::RequestCompletion => "request_completion",
+            Self::ExtractScript => "extract_script",
+            Self::CreateScriptFile => "create_script_file",
+            Self::WriteScriptFile => "write_script_file",
+            Self::OpenEditor => "open_editor",
+        }
+    }
+}
+
+const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You are ScriptKitScriptGenerator.
+Generate a complete Script Kit TypeScript script and return code only.
+Requirements:
+- Include metadata comments at the top:
+  // Name: <script name>
+  // Description: <short summary>
+- Include: import "@scriptkit/sdk";
+- Use idiomatic Script Kit APIs (for example await arg(), await div(), await editor()) when useful.
+- Prefer clear async/await flow and practical defaults.
+- Return a full runnable script with no extra explanation."#;
+
+fn build_ai_script_generation_user_prompt(description: &str) -> String {
+    format!(
+        "Generate a complete Script Kit script for this request:\n\n{}\n\nReturn only the TypeScript script source.",
+        description.trim()
+    )
+}
+
+fn select_default_ai_script_model(
+    registry: &crate::ai::ProviderRegistry,
+) -> Option<crate::ai::ModelInfo> {
+    let all_models = registry.get_all_models();
+    all_models
+        .iter()
+        .find(|model| model.provider.eq_ignore_ascii_case("vercel"))
+        .cloned()
+        .or_else(|| all_models.first().cloned())
+}
+
+fn extract_generated_script_source(raw_response: &str) -> Option<String> {
+    let trimmed = raw_response.trim();
+    if trimmed.is_empty() {
+        return None;
+    }
+
+    // Prefer typed code fences first, then generic fences.
+    for fence in ["```typescript", "```ts", "```javascript", "```js", "```"] {
+        if let Some(content) = extract_first_fenced_block(trimmed, fence) {
+            return normalize_generated_script(content);
+        }
+    }
+
+    normalize_generated_script(trimmed.to_string())
+}
+
+fn extract_first_fenced_block(response: &str, fence_start: &str) -> Option<String> {
+    let start_index = response.find(fence_start)?;
+    let after_fence = &response[start_index + fence_start.len()..];
+    let after_newline = after_fence
+        .strip_prefix("\r\n")
+        .or_else(|| after_fence.strip_prefix('\n'))
+        .unwrap_or(after_fence);
+    let end_index = after_newline.find("```")?;
+    Some(after_newline[..end_index].trim().to_string())
+}
+
+fn normalize_generated_script(content: String) -> Option<String> {
+    let trimmed = content.trim();
+    if trimmed.is_empty() {
+        None
+    } else {
+        Some(format!("{trimmed}\n"))
+    }
+}
+
+fn derive_script_name_from_description(description: &str) -> String {
+    let mut slug = String::new();
+    let mut last_was_dash = false;
+
+    for ch in description.chars() {
+        let lower = ch.to_ascii_lowercase();
+        if lower.is_ascii_alphanumeric() {
+            slug.push(lower);
+            last_was_dash = false;
+        } else if !last_was_dash && !slug.is_empty() {
+            slug.push('-');
+            last_was_dash = true;
+        }
+
+        if slug.len() >= 48 {
+            break;
+        }
+    }
+
+    while slug.ends_with('-') {
+        slug.pop();
+    }
+
+    if slug.is_empty() {
+        "ai-generated-script".to_string()
+    } else {
+        slug
+    }
+}
+
+fn generate_script_via_ai_backend(
+    registry: &crate::ai::ProviderRegistry,
+    model_id: &str,
+    prompt_description: &str,
+    config: &crate::config::Config,
+) -> Result<std::path::PathBuf> {
+    let provider = registry
+        .find_provider_for_model(model_id)
+        .cloned()
+        .ok_or_else(|| {
+            anyhow!(
+                "state={} attempted=find_provider_for_model model_id={} failure=provider_not_found",
+                AiScriptGenerationStage::ResolveProvider.as_str(),
+                model_id
+            )
+        })?;
+
+    let request_messages = vec![
+        crate::ai::ProviderMessage::system(AI_SCRIPT_GENERATION_SYSTEM_PROMPT),
+        crate::ai::ProviderMessage::user(build_ai_script_generation_user_prompt(prompt_description)),
+    ];
+
+    let ai_response = provider
+        .send_message(&request_messages, model_id)
+        .with_context(|| {
+            format!(
+                "state={} attempted=send_message model_id={} provider={} failure=provider_call_failed",
+                AiScriptGenerationStage::RequestCompletion.as_str(),
+                model_id,
+                provider.provider_id()
+            )
+        })?;
+
+    let generated_script = extract_generated_script_source(&ai_response).ok_or_else(|| {
+        anyhow!(
+            "state={} attempted=extract_generated_script model_id={} failure=empty_script_response",
+            AiScriptGenerationStage::ExtractScript.as_str(),
+            model_id
+        )
+    })?;
+
+    let script_name = derive_script_name_from_description(prompt_description);
+    let script_path = crate::script_creation::create_new_script(&script_name).with_context(|| {
+        format!(
+            "state={} attempted=create_new_script name={} failure=create_script_failed",
+            AiScriptGenerationStage::CreateScriptFile.as_str(),
+            script_name
+        )
+    })?;
+
+    std::fs::write(&script_path, generated_script).with_context(|| {
+        format!(
+            "state={} attempted=write_script path={} failure=write_failed",
+            AiScriptGenerationStage::WriteScriptFile.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    crate::script_creation::open_in_editor(&script_path, config).with_context(|| {
+        format!(
+            "state={} attempted=open_in_editor path={} failure=open_editor_failed",
+            AiScriptGenerationStage::OpenEditor.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
 
 impl ScriptListApp {
     pub(crate) fn is_in_prompt(&self) -> bool {
@@ -274,6 +464,145 @@
         cx.notify();
     }
 
+    /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
+    /// The generated script is saved to disk and opened in the configured editor.
+    pub fn generate_script_from_ai_prompt(
+        &mut self,
+        prompt_description: String,
+        cx: &mut Context<Self>,
+    ) {
+        let prompt_description = prompt_description.trim().to_string();
+        if prompt_description.is_empty() {
+            return;
+        }
+
+        let registry = self.cached_provider_registry.clone().unwrap_or_else(|| {
+            crate::ai::ProviderRegistry::from_environment_with_config(Some(&self.config))
+        });
+
+        if !registry.has_any_provider() {
+            self.toast_manager.push(
+                components::toast::Toast::error("No AI providers configured for script generation", &self.theme)
+                    .duration_ms(Some(5000)),
+            );
+            cx.notify();
+            return;
+        }
+
+        let selected_model = match select_default_ai_script_model(&registry) {
+            Some(model) => model,
+            None => {
+                let stage = AiScriptGenerationStage::SelectModel.as_str();
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    &format!(
+                        "state={} attempted=select_default_model failure=no_available_models",
+                        stage
+                    ),
+                );
+                self.toast_manager.push(
+                    components::toast::Toast::error(
+                        "No AI models available for script generation",
+                        &self.theme,
+                    )
+                    .duration_ms(Some(5000)),
+                );
+                cx.notify();
+                return;
+            }
+        };
+
+        let model_id = selected_model.id.clone();
+        let provider = selected_model.provider.clone();
+        let config = self.config.clone();
+        let (tx, rx) = async_channel::bounded::<std::result::Result<std::path::PathBuf, String>>(1);
+
+        logging::log(
+            "AI_SCRIPT_GEN",
+            &format!(
+                "state=queued attempted=shift_tab_script_generation model_id={} provider={} prompt_len={}",
+                model_id,
+                provider,
+                prompt_description.len()
+            ),
+        );
+        self.show_hud("Generating script with AI...".to_string(), Some(1500), cx);
+
+        std::thread::spawn(move || {
+            logging::log(
+                "AI_SCRIPT_GEN",
+                &format!(
+                    "state=running attempted=generate_script model_id={} provider={}",
+                    model_id, provider
+                ),
+            );
+
+            let generation_result =
+                generate_script_via_ai_backend(&registry, &model_id, &prompt_description, &config)
+                    .map_err(|error| error.to_string());
+
+            if tx.send_blocking(generation_result).is_err() {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=send_result failure=result_channel_closed",
+                );
+            }
+        });
+
+        cx.spawn(async move |this, cx| {
+            let Ok(result) = rx.recv().await else {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=recv_result failure=result_channel_closed",
+                );
+                return;
+            };
+
+            let _ = cx.update(|cx| {
+                this.update(cx, |app, cx| {
+                    match result {
+                        Ok(script_path) => {
+                            let script_name = script_path
+                                .file_name()
+                                .and_then(|name| name.to_str())
+                                .unwrap_or("generated script");
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!(
+                                    "state=completed attempted=generate_script path={}",
+                                    script_path.display()
+                                ),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::success(
+                                    format!("Generated and opened {}", script_name),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(3500)),
+                            );
+                            app.close_and_reset_window(cx);
+                        }
+                        Err(error) => {
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!("state=failed attempted=generate_script error={}", error),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::error(
+                                    format!("Failed to generate script: {}", error),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(7000)),
+                            );
+                            cx.notify();
+                        }
+                    }
+                })
+            });
+        })
+        .detach();
+    }
+
     /// Rebuild the cached provider registry in a background thread.
     /// Called after config changes (API key saved, Claude Code enabled, etc.)
     pub fn rebuild_provider_registry_async(&mut self, cx: &mut Context<Self>) {
@@ -312,3 +641,44 @@
         .detach();
     }
 }
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_build_ai_script_generation_user_prompt_includes_description() {
+        let prompt = build_ai_script_generation_user_prompt("create a weather script");
+        assert!(prompt.contains("create a weather script"));
+        assert!(prompt.contains("Script Kit script"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_prefers_typescript_fence() {
+        let response = r#"
+Here's your script:
+```typescript
+// Name: Weather
+import "@scriptkit/sdk";
+```
+"#;
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert!(extracted.contains("// Name: Weather"));
+        assert!(!extracted.contains("```"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_falls_back_to_plain_text() {
+        let response = "// Name: Plain\nimport \"@scriptkit/sdk\";";
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert_eq!(extracted, "// Name: Plain\nimport \"@scriptkit/sdk\";\n");
+    }
+
+    #[test]
+    fn test_derive_script_name_from_description_uses_fallback_for_symbols() {
+        let name = derive_script_name_from_description("@@@ !!!");
+        assert_eq!(name, "ai-generated-script");
+    }
+}
diff --git a/src/app_impl/startup.rs b/src/app_impl/startup.rs
index 2e95070751083b8f4c24cd7eae8372d6bec3f6d6..441b4f23bca1b6294025c698eeb11d45815e85e0
--- a/src/app_impl/startup.rs
+++ b/src/app_impl/startup.rs
@@ -555,9 +555,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -720,20 +722,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/startup_new_tab.rs b/src/app_impl/startup_new_tab.rs
index 6c807a195553d9f168087c2ca6d771c76e555c11..7c84270cbe91c0b1ddd460aa29db8c788766d0cf
--- a/src/app_impl/startup_new_tab.rs
+++ b/src/app_impl/startup_new_tab.rs
@@ -6,9 +6,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -171,20 +173,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/tests.rs b/src/app_impl/tests.rs
index d75ba121851d7233c5ae77bc474a64b27e7c31d2..3f0fbd1ccbc2f1e3d515ac1edf9666035f09eaf4
--- a/src/app_impl/tests.rs
+++ b/src/app_impl/tests.rs
@@ -1,6 +1,7 @@
 #[cfg(test)]
 mod app_impl_state_sync_tests {
     use super::{calculate_fallback_error_message, ScriptListApp};
+    use std::fs;
 
     #[test]
     fn test_sync_builtin_query_state_updates_query_and_selection_when_changed() {
@@ -46,4 +47,39 @@
         assert!(message.contains("Could not evaluate expression"));
         assert!(message.contains("Check the syntax and try again"));
     }
+
+    #[test]
+    fn test_shift_tab_routes_to_ai_script_generation_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("if has_shift")
+                && startup_tab.contains("this.generate_script_from_ai_prompt(query, cx);"),
+            "Shift+Tab in ScriptList should route to generate_script_from_ai_prompt. \
+             Missing expected branch in startup_new_tab.rs"
+        );
+    }
+
+    #[test]
+    fn test_tab_still_routes_to_inline_ai_chat_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("this.show_inline_ai_chat(Some(query), cx);"),
+            "Tab in ScriptList should continue routing to show_inline_ai_chat"
+        );
+    }
+
+    #[test]
+    fn test_tab_interceptor_matches_both_tab_key_variants() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("matches!(event.keystroke.key.as_str(), \"tab\" | \"Tab\")"),
+            "Tab interceptor should match both tab key variants: \"tab\" and \"Tab\""
+        );
+    }
 }

file update:
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index 98a0c9cd1db5a933992c0f99cb20a3c2ca436b41..c985a5090574816267103019d1b272e1e3151a38
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -1,4 +1,194 @@
 use super::*;
+use anyhow::{anyhow, Context as AnyhowContext, Result};
+
+#[derive(Debug, Clone, Copy)]
+enum AiScriptGenerationStage {
+    SelectModel,
+    ResolveProvider,
+    RequestCompletion,
+    ExtractScript,
+    CreateScriptFile,
+    WriteScriptFile,
+    OpenEditor,
+}
+
+impl AiScriptGenerationStage {
+    fn as_str(self) -> &'static str {
+        match self {
+            Self::SelectModel => "select_model",
+            Self::ResolveProvider => "resolve_provider",
+            Self::RequestCompletion => "request_completion",
+            Self::ExtractScript => "extract_script",
+            Self::CreateScriptFile => "create_script_file",
+            Self::WriteScriptFile => "write_script_file",
+            Self::OpenEditor => "open_editor",
+        }
+    }
+}
+
+const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You are ScriptKitScriptGenerator.
+Generate a complete Script Kit TypeScript script and return code only.
+Requirements:
+- Include metadata comments at the top:
+  // Name: <script name>
+  // Description: <short summary>
+- Include: import "@scriptkit/sdk";
+- Use idiomatic Script Kit APIs (for example await arg(), await div(), await editor()) when useful.
+- Prefer clear async/await flow and practical defaults.
+- Return a full runnable script with no extra explanation."#;
+
+fn build_ai_script_generation_user_prompt(description: &str) -> String {
+    format!(
+        "Generate a complete Script Kit script for this request:\n\n{}\n\nReturn only the TypeScript script source.",
+        description.trim()
+    )
+}
+
+fn select_default_ai_script_model(
+    registry: &crate::ai::ProviderRegistry,
+) -> Option<crate::ai::ModelInfo> {
+    let all_models = registry.get_all_models();
+    all_models
+        .iter()
+        .find(|model| model.provider.eq_ignore_ascii_case("vercel"))
+        .cloned()
+        .or_else(|| all_models.first().cloned())
+}
+
+fn extract_generated_script_source(raw_response: &str) -> Option<String> {
+    let trimmed = raw_response.trim();
+    if trimmed.is_empty() {
+        return None;
+    }
+
+    // Prefer typed code fences first, then generic fences.
+    for fence in ["```typescript", "```ts", "```javascript", "```js", "```"] {
+        if let Some(content) = extract_first_fenced_block(trimmed, fence) {
+            return normalize_generated_script(content);
+        }
+    }
+
+    normalize_generated_script(trimmed.to_string())
+}
+
+fn extract_first_fenced_block(response: &str, fence_start: &str) -> Option<String> {
+    let start_index = response.find(fence_start)?;
+    let after_fence = &response[start_index + fence_start.len()..];
+    let after_newline = after_fence
+        .strip_prefix("\r\n")
+        .or_else(|| after_fence.strip_prefix('\n'))
+        .unwrap_or(after_fence);
+    let end_index = after_newline.find("```")?;
+    Some(after_newline[..end_index].trim().to_string())
+}
+
+fn normalize_generated_script(content: String) -> Option<String> {
+    let trimmed = content.trim();
+    if trimmed.is_empty() {
+        None
+    } else {
+        Some(format!("{trimmed}\n"))
+    }
+}
+
+fn derive_script_name_from_description(description: &str) -> String {
+    let mut slug = String::new();
+    let mut last_was_dash = false;
+
+    for ch in description.chars() {
+        let lower = ch.to_ascii_lowercase();
+        if lower.is_ascii_alphanumeric() {
+            slug.push(lower);
+            last_was_dash = false;
+        } else if !last_was_dash && !slug.is_empty() {
+            slug.push('-');
+            last_was_dash = true;
+        }
+
+        if slug.len() >= 48 {
+            break;
+        }
+    }
+
+    while slug.ends_with('-') {
+        slug.pop();
+    }
+
+    if slug.is_empty() {
+        "ai-generated-script".to_string()
+    } else {
+        slug
+    }
+}
+
+fn generate_script_via_ai_backend(
+    registry: &crate::ai::ProviderRegistry,
+    model_id: &str,
+    prompt_description: &str,
+    config: &crate::config::Config,
+) -> Result<std::path::PathBuf> {
+    let provider = registry
+        .find_provider_for_model(model_id)
+        .cloned()
+        .ok_or_else(|| {
+            anyhow!(
+                "state={} attempted=find_provider_for_model model_id={} failure=provider_not_found",
+                AiScriptGenerationStage::ResolveProvider.as_str(),
+                model_id
+            )
+        })?;
+
+    let request_messages = vec![
+        crate::ai::ProviderMessage::system(AI_SCRIPT_GENERATION_SYSTEM_PROMPT),
+        crate::ai::ProviderMessage::user(build_ai_script_generation_user_prompt(prompt_description)),
+    ];
+
+    let ai_response = provider
+        .send_message(&request_messages, model_id)
+        .with_context(|| {
+            format!(
+                "state={} attempted=send_message model_id={} provider={} failure=provider_call_failed",
+                AiScriptGenerationStage::RequestCompletion.as_str(),
+                model_id,
+                provider.provider_id()
+            )
+        })?;
+
+    let generated_script = extract_generated_script_source(&ai_response).ok_or_else(|| {
+        anyhow!(
+            "state={} attempted=extract_generated_script model_id={} failure=empty_script_response",
+            AiScriptGenerationStage::ExtractScript.as_str(),
+            model_id
+        )
+    })?;
+
+    let script_name = derive_script_name_from_description(prompt_description);
+    let script_path = crate::script_creation::create_new_script(&script_name).with_context(|| {
+        format!(
+            "state={} attempted=create_new_script name={} failure=create_script_failed",
+            AiScriptGenerationStage::CreateScriptFile.as_str(),
+            script_name
+        )
+    })?;
+
+    std::fs::write(&script_path, generated_script).with_context(|| {
+        format!(
+            "state={} attempted=write_script path={} failure=write_failed",
+            AiScriptGenerationStage::WriteScriptFile.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    crate::script_creation::open_in_editor(&script_path, config).with_context(|| {
+        format!(
+            "state={} attempted=open_in_editor path={} failure=open_editor_failed",
+            AiScriptGenerationStage::OpenEditor.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
 
 impl ScriptListApp {
     pub(crate) fn is_in_prompt(&self) -> bool {
@@ -274,6 +464,145 @@
         cx.notify();
     }
 
+    /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
+    /// The generated script is saved to disk and opened in the configured editor.
+    pub fn generate_script_from_ai_prompt(
+        &mut self,
+        prompt_description: String,
+        cx: &mut Context<Self>,
+    ) {
+        let prompt_description = prompt_description.trim().to_string();
+        if prompt_description.is_empty() {
+            return;
+        }
+
+        let registry = self.cached_provider_registry.clone().unwrap_or_else(|| {
+            crate::ai::ProviderRegistry::from_environment_with_config(Some(&self.config))
+        });
+
+        if !registry.has_any_provider() {
+            self.toast_manager.push(
+                components::toast::Toast::error("No AI providers configured for script generation", &self.theme)
+                    .duration_ms(Some(5000)),
+            );
+            cx.notify();
+            return;
+        }
+
+        let selected_model = match select_default_ai_script_model(&registry) {
+            Some(model) => model,
+            None => {
+                let stage = AiScriptGenerationStage::SelectModel.as_str();
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    &format!(
+                        "state={} attempted=select_default_model failure=no_available_models",
+                        stage
+                    ),
+                );
+                self.toast_manager.push(
+                    components::toast::Toast::error(
+                        "No AI models available for script generation",
+                        &self.theme,
+                    )
+                    .duration_ms(Some(5000)),
+                );
+                cx.notify();
+                return;
+            }
+        };
+
+        let model_id = selected_model.id.clone();
+        let provider = selected_model.provider.clone();
+        let config = self.config.clone();
+        let (tx, rx) = async_channel::bounded::<std::result::Result<std::path::PathBuf, String>>(1);
+
+        logging::log(
+            "AI_SCRIPT_GEN",
+            &format!(
+                "state=queued attempted=shift_tab_script_generation model_id={} provider={} prompt_len={}",
+                model_id,
+                provider,
+                prompt_description.len()
+            ),
+        );
+        self.show_hud("Generating script with AI...".to_string(), Some(1500), cx);
+
+        std::thread::spawn(move || {
+            logging::log(
+                "AI_SCRIPT_GEN",
+                &format!(
+                    "state=running attempted=generate_script model_id={} provider={}",
+                    model_id, provider
+                ),
+            );
+
+            let generation_result =
+                generate_script_via_ai_backend(&registry, &model_id, &prompt_description, &config)
+                    .map_err(|error| error.to_string());
+
+            if tx.send_blocking(generation_result).is_err() {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=send_result failure=result_channel_closed",
+                );
+            }
+        });
+
+        cx.spawn(async move |this, cx| {
+            let Ok(result) = rx.recv().await else {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=recv_result failure=result_channel_closed",
+                );
+                return;
+            };
+
+            let _ = cx.update(|cx| {
+                this.update(cx, |app, cx| {
+                    match result {
+                        Ok(script_path) => {
+                            let script_name = script_path
+                                .file_name()
+                                .and_then(|name| name.to_str())
+                                .unwrap_or("generated script");
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!(
+                                    "state=completed attempted=generate_script path={}",
+                                    script_path.display()
+                                ),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::success(
+                                    format!("Generated and opened {}", script_name),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(3500)),
+                            );
+                            app.close_and_reset_window(cx);
+                        }
+                        Err(error) => {
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!("state=failed attempted=generate_script error={}", error),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::error(
+                                    format!("Failed to generate script: {}", error),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(7000)),
+                            );
+                            cx.notify();
+                        }
+                    }
+                })
+            });
+        })
+        .detach();
+    }
+
     /// Rebuild the cached provider registry in a background thread.
     /// Called after config changes (API key saved, Claude Code enabled, etc.)
     pub fn rebuild_provider_registry_async(&mut self, cx: &mut Context<Self>) {
@@ -312,3 +641,44 @@
         .detach();
     }
 }
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_build_ai_script_generation_user_prompt_includes_description() {
+        let prompt = build_ai_script_generation_user_prompt("create a weather script");
+        assert!(prompt.contains("create a weather script"));
+        assert!(prompt.contains("Script Kit script"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_prefers_typescript_fence() {
+        let response = r#"
+Here's your script:
+```typescript
+// Name: Weather
+import "@scriptkit/sdk";
+```
+"#;
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert!(extracted.contains("// Name: Weather"));
+        assert!(!extracted.contains("```"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_falls_back_to_plain_text() {
+        let response = "// Name: Plain\nimport \"@scriptkit/sdk\";";
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert_eq!(extracted, "// Name: Plain\nimport \"@scriptkit/sdk\";\n");
+    }
+
+    #[test]
+    fn test_derive_script_name_from_description_uses_fallback_for_symbols() {
+        let name = derive_script_name_from_description("@@@ !!!");
+        assert_eq!(name, "ai-generated-script");
+    }
+}
diff --git a/src/app_impl/startup.rs b/src/app_impl/startup.rs
index 2e95070751083b8f4c24cd7eae8372d6bec3f6d6..441b4f23bca1b6294025c698eeb11d45815e85e0
--- a/src/app_impl/startup.rs
+++ b/src/app_impl/startup.rs
@@ -555,9 +555,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -720,20 +722,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/startup_new_tab.rs b/src/app_impl/startup_new_tab.rs
index 6c807a195553d9f168087c2ca6d771c76e555c11..7c84270cbe91c0b1ddd460aa29db8c788766d0cf
--- a/src/app_impl/startup_new_tab.rs
+++ b/src/app_impl/startup_new_tab.rs
@@ -6,9 +6,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -171,20 +173,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/tests.rs b/src/app_impl/tests.rs
index d75ba121851d7233c5ae77bc474a64b27e7c31d2..3f0fbd1ccbc2f1e3d515ac1edf9666035f09eaf4
--- a/src/app_impl/tests.rs
+++ b/src/app_impl/tests.rs
@@ -1,6 +1,7 @@
 #[cfg(test)]
 mod app_impl_state_sync_tests {
     use super::{calculate_fallback_error_message, ScriptListApp};
+    use std::fs;
 
     #[test]
     fn test_sync_builtin_query_state_updates_query_and_selection_when_changed() {
@@ -46,4 +47,39 @@
         assert!(message.contains("Could not evaluate expression"));
         assert!(message.contains("Check the syntax and try again"));
     }
+
+    #[test]
+    fn test_shift_tab_routes_to_ai_script_generation_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("if has_shift")
+                && startup_tab.contains("this.generate_script_from_ai_prompt(query, cx);"),
+            "Shift+Tab in ScriptList should route to generate_script_from_ai_prompt. \
+             Missing expected branch in startup_new_tab.rs"
+        );
+    }
+
+    #[test]
+    fn test_tab_still_routes_to_inline_ai_chat_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("this.show_inline_ai_chat(Some(query), cx);"),
+            "Tab in ScriptList should continue routing to show_inline_ai_chat"
+        );
+    }
+
+    #[test]
+    fn test_tab_interceptor_matches_both_tab_key_variants() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("matches!(event.keystroke.key.as_str(), \"tab\" | \"Tab\")"),
+            "Tab interceptor should match both tab key variants: \"tab\" and \"Tab\""
+        );
+    }
 }


thinking
**Planning quick diff check**
codex
I added the key-variant test and I’m quickly checking the updated diff for correctness, then I’ll re-release the claim.
exec
/bin/zsh -lc 'git diff -- src/app_impl/tests.rs' in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
diff --git a/src/app_impl/tests.rs b/src/app_impl/tests.rs
index d75ba12..3f0fbd1 100644
--- a/src/app_impl/tests.rs
+++ b/src/app_impl/tests.rs
@@ -1,6 +1,7 @@
 #[cfg(test)]
 mod app_impl_state_sync_tests {
     use super::{calculate_fallback_error_message, ScriptListApp};
+    use std::fs;
 
     #[test]
     fn test_sync_builtin_query_state_updates_query_and_selection_when_changed() {
@@ -46,4 +47,39 @@ mod app_impl_state_sync_tests {
         assert!(message.contains("Could not evaluate expression"));
         assert!(message.contains("Check the syntax and try again"));
     }
+
+    #[test]
+    fn test_shift_tab_routes_to_ai_script_generation_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
file update:
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index 98a0c9cd1db5a933992c0f99cb20a3c2ca436b41..c985a5090574816267103019d1b272e1e3151a38
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -1,4 +1,194 @@
 use super::*;
+use anyhow::{anyhow, Context as AnyhowContext, Result};
+
+#[derive(Debug, Clone, Copy)]
+enum AiScriptGenerationStage {
+    SelectModel,
+    ResolveProvider,
+    RequestCompletion,
+    ExtractScript,
+    CreateScriptFile,
+    WriteScriptFile,
+    OpenEditor,
+}
+
+impl AiScriptGenerationStage {
+    fn as_str(self) -> &'static str {
+        match self {
+            Self::SelectModel => "select_model",
+            Self::ResolveProvider => "resolve_provider",
+            Self::RequestCompletion => "request_completion",
+            Self::ExtractScript => "extract_script",
+            Self::CreateScriptFile => "create_script_file",
+            Self::WriteScriptFile => "write_script_file",
+            Self::OpenEditor => "open_editor",
+        }
+    }
+}
+
+const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You are ScriptKitScriptGenerator.
+Generate a complete Script Kit TypeScript script and return code only.
+Requirements:
+- Include metadata comments at the top:
+  // Name: <script name>
+  // Description: <short summary>
+- Include: import "@scriptkit/sdk";
+- Use idiomatic Script Kit APIs (for example await arg(), await div(), await editor()) when useful.
+- Prefer clear async/await flow and practical defaults.
+- Return a full runnable script with no extra explanation."#;
+
+fn build_ai_script_generation_user_prompt(description: &str) -> String {
+    format!(
+        "Generate a complete Script Kit script for this request:\n\n{}\n\nReturn only the TypeScript script source.",
+        description.trim()
+    )
+}
+
+fn select_default_ai_script_model(
+    registry: &crate::ai::ProviderRegistry,
+) -> Option<crate::ai::ModelInfo> {
+    let all_models = registry.get_all_models();
+    all_models
+        .iter()
+        .find(|model| model.provider.eq_ignore_ascii_case("vercel"))
+        .cloned()
+        .or_else(|| all_models.first().cloned())
+}
+
+fn extract_generated_script_source(raw_response: &str) -> Option<String> {
+    let trimmed = raw_response.trim();
+    if trimmed.is_empty() {
+        return None;
+    }
+
+    // Prefer typed code fences first, then generic fences.
+    for fence in ["```typescript", "```ts", "```javascript", "```js", "```"] {
+        if let Some(content) = extract_first_fenced_block(trimmed, fence) {
+            return normalize_generated_script(content);
+        }
+    }
+
+    normalize_generated_script(trimmed.to_string())
+}
+
+fn extract_first_fenced_block(response: &str, fence_start: &str) -> Option<String> {
+    let start_index = response.find(fence_start)?;
+    let after_fence = &response[start_index + fence_start.len()..];
+    let after_newline = after_fence
+        .strip_prefix("\r\n")
+        .or_else(|| after_fence.strip_prefix('\n'))
+        .unwrap_or(after_fence);
+    let end_index = after_newline.find("```")?;
+    Some(after_newline[..end_index].trim().to_string())
+}
+
+fn normalize_generated_script(content: String) -> Option<String> {
+    let trimmed = content.trim();
+    if trimmed.is_empty() {
+        None
+    } else {
+        Some(format!("{trimmed}\n"))
+    }
+}
+
+fn derive_script_name_from_description(description: &str) -> String {
+    let mut slug = String::new();
+    let mut last_was_dash = false;
+
+    for ch in description.chars() {
+        let lower = ch.to_ascii_lowercase();
+        if lower.is_ascii_alphanumeric() {
+            slug.push(lower);
+            last_was_dash = false;
+        } else if !last_was_dash && !slug.is_empty() {
+            slug.push('-');
+            last_was_dash = true;
+        }
+
+        if slug.len() >= 48 {
+            break;
+        }
+    }
+
+    while slug.ends_with('-') {
+        slug.pop();
+    }
+
+    if slug.is_empty() {
+        "ai-generated-script".to_string()
+    } else {
+        slug
+    }
+}
+
+fn generate_script_via_ai_backend(
+    registry: &crate::ai::ProviderRegistry,
+    model_id: &str,
+    prompt_description: &str,
+    config: &crate::config::Config,
+) -> Result<std::path::PathBuf> {
+    let provider = registry
+        .find_provider_for_model(model_id)
+        .cloned()
+        .ok_or_else(|| {
+            anyhow!(
+                "state={} attempted=find_provider_for_model model_id={} failure=provider_not_found",
+                AiScriptGenerationStage::ResolveProvider.as_str(),
+                model_id
+            )
+        })?;
+
+    let request_messages = vec![
+        crate::ai::ProviderMessage::system(AI_SCRIPT_GENERATION_SYSTEM_PROMPT),
+        crate::ai::ProviderMessage::user(build_ai_script_generation_user_prompt(prompt_description)),
+    ];
+
+    let ai_response = provider
+        .send_message(&request_messages, model_id)
+        .with_context(|| {
+            format!(
+                "state={} attempted=send_message model_id={} provider={} failure=provider_call_failed",
+                AiScriptGenerationStage::RequestCompletion.as_str(),
+                model_id,
+                provider.provider_id()
+            )
+        })?;
+
+    let generated_script = extract_generated_script_source(&ai_response).ok_or_else(|| {
+        anyhow!(
+            "state={} attempted=extract_generated_script model_id={} failure=empty_script_response",
+            AiScriptGenerationStage::ExtractScript.as_str(),
+            model_id
+        )
+    })?;
+
+    let script_name = derive_script_name_from_description(prompt_description);
+    let script_path = crate::script_creation::create_new_script(&script_name).with_context(|| {
+        format!(
+            "state={} attempted=create_new_script name={} failure=create_script_failed",
+            AiScriptGenerationStage::CreateScriptFile.as_str(),
+            script_name
+        )
+    })?;
+
+    std::fs::write(&script_path, generated_script).with_context(|| {
+        format!(
+            "state={} attempted=write_script path={} failure=write_failed",
+            AiScriptGenerationStage::WriteScriptFile.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    crate::script_creation::open_in_editor(&script_path, config).with_context(|| {
+        format!(
+            "state={} attempted=open_in_editor path={} failure=open_editor_failed",
+            AiScriptGenerationStage::OpenEditor.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
 
 impl ScriptListApp {
     pub(crate) fn is_in_prompt(&self) -> bool {
@@ -274,6 +464,145 @@
         cx.notify();
     }
 
+    /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
+    /// The generated script is saved to disk and opened in the configured editor.
+    pub fn generate_script_from_ai_prompt(
+        &mut self,
+        prompt_description: String,
+        cx: &mut Context<Self>,
+    ) {
+        let prompt_description = prompt_description.trim().to_string();
+        if prompt_description.is_empty() {
+            return;
+        }
+
+        let registry = self.cached_provider_registry.clone().unwrap_or_else(|| {
+            crate::ai::ProviderRegistry::from_environment_with_config(Some(&self.config))
+        });
+
+        if !registry.has_any_provider() {
+            self.toast_manager.push(
+                components::toast::Toast::error("No AI providers configured for script generation", &self.theme)
+                    .duration_ms(Some(5000)),
+            );
+            cx.notify();
+            return;
+        }
+
+        let selected_model = match select_default_ai_script_model(&registry) {
+            Some(model) => model,
+            None => {
+                let stage = AiScriptGenerationStage::SelectModel.as_str();
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    &format!(
+                        "state={} attempted=select_default_model failure=no_available_models",
+                        stage
+                    ),
+                );
+                self.toast_manager.push(
+                    components::toast::Toast::error(
+                        "No AI models available for script generation",
+                        &self.theme,
+                    )
+                    .duration_ms(Some(5000)),
+                );
+                cx.notify();
+                return;
+            }
+        };
+
+        let model_id = selected_model.id.clone();
+        let provider = selected_model.provider.clone();
+        let config = self.config.clone();
+        let (tx, rx) = async_channel::bounded::<std::result::Result<std::path::PathBuf, String>>(1);
+
+        logging::log(
+            "AI_SCRIPT_GEN",
+            &format!(
+                "state=queued attempted=shift_tab_script_generation model_id={} provider={} prompt_len={}",
+                model_id,
+                provider,
+                prompt_description.len()
+            ),
+        );
+        self.show_hud("Generating script with AI...".to_string(), Some(1500), cx);
+
+        std::thread::spawn(move || {
+            logging::log(
+                "AI_SCRIPT_GEN",
+                &format!(
+                    "state=running attempted=generate_script model_id={} provider={}",
+                    model_id, provider
+                ),
+            );
+
+            let generation_result =
+                generate_script_via_ai_backend(&registry, &model_id, &prompt_description, &config)
+                    .map_err(|error| error.to_string());
+
+            if tx.send_blocking(generation_result).is_err() {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=send_result failure=result_channel_closed",
+                );
+            }
+        });
+
+        cx.spawn(async move |this, cx| {
+            let Ok(result) = rx.recv().await else {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=recv_result failure=result_channel_closed",
+                );
+                return;
+            };
+
+            let _ = cx.update(|cx| {
+                this.update(cx, |app, cx| {
+                    match result {
+                        Ok(script_path) => {
+                            let script_name = script_path
+                                .file_name()
+                                .and_then(|name| name.to_str())
+                                .unwrap_or("generated script");
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!(
+                                    "state=completed attempted=generate_script path={}",
+                                    script_path.display()
+                                ),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::success(
+                                    format!("Generated and opened {}", script_name),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(3500)),
+                            );
+                            app.close_and_reset_window(cx);
+                        }
+                        Err(error) => {
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!("state=failed attempted=generate_script error={}", error),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::error(
+                                    format!("Failed to generate script: {}", error),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(7000)),
+                            );
+                            cx.notify();
+                        }
+                    }
+                })
+            });
+        })
+        .detach();
+    }
+
     /// Rebuild the cached provider registry in a background thread.
     /// Called after config changes (API key saved, Claude Code enabled, etc.)
     pub fn rebuild_provider_registry_async(&mut self, cx: &mut Context<Self>) {
@@ -312,3 +641,44 @@
         .detach();
     }
 }
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_build_ai_script_generation_user_prompt_includes_description() {
+        let prompt = build_ai_script_generation_user_prompt("create a weather script");
+        assert!(prompt.contains("create a weather script"));
+        assert!(prompt.contains("Script Kit script"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_prefers_typescript_fence() {
+        let response = r#"
+Here's your script:
+```typescript
+// Name: Weather
+import "@scriptkit/sdk";
+```
+"#;
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert!(extracted.contains("// Name: Weather"));
+        assert!(!extracted.contains("```"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_falls_back_to_plain_text() {
+        let response = "// Name: Plain\nimport \"@scriptkit/sdk\";";
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert_eq!(extracted, "// Name: Plain\nimport \"@scriptkit/sdk\";\n");
+    }
+
+    #[test]
+    fn test_derive_script_name_from_description_uses_fallback_for_symbols() {
+        let name = derive_script_name_from_description("@@@ !!!");
+        assert_eq!(name, "ai-generated-script");
+    }
+}
diff --git a/src/app_impl/startup.rs b/src/app_impl/startup.rs
index 2e95070751083b8f4c24cd7eae8372d6bec3f6d6..441b4f23bca1b6294025c698eeb11d45815e85e0
--- a/src/app_impl/startup.rs
+++ b/src/app_impl/startup.rs
@@ -555,9 +555,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -720,20 +722,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/startup_new_tab.rs b/src/app_impl/startup_new_tab.rs
index 6c807a195553d9f168087c2ca6d771c76e555c11..7c84270cbe91c0b1ddd460aa29db8c788766d0cf
--- a/src/app_impl/startup_new_tab.rs
+++ b/src/app_impl/startup_new_tab.rs
@@ -6,9 +6,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -171,20 +173,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/tests.rs b/src/app_impl/tests.rs
index d75ba121851d7233c5ae77bc474a64b27e7c31d2..3f0fbd1ccbc2f1e3d515ac1edf9666035f09eaf4
--- a/src/app_impl/tests.rs
+++ b/src/app_impl/tests.rs
@@ -1,6 +1,7 @@
 #[cfg(test)]
 mod app_impl_state_sync_tests {
     use super::{calculate_fallback_error_message, ScriptListApp};
+    use std::fs;
 
     #[test]
     fn test_sync_builtin_query_state_updates_query_and_selection_when_changed() {
@@ -46,4 +47,39 @@
         assert!(message.contains("Could not evaluate expression"));
         assert!(message.contains("Check the syntax and try again"));
     }
+
+    #[test]
+    fn test_shift_tab_routes_to_ai_script_generation_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("if has_shift")
+                && startup_tab.contains("this.generate_script_from_ai_prompt(query, cx);"),
+            "Shift+Tab in ScriptList should route to generate_script_from_ai_prompt. \
+             Missing expected branch in startup_new_tab.rs"
+        );
+    }
+
+    #[test]
+    fn test_tab_still_routes_to_inline_ai_chat_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("this.show_inline_ai_chat(Some(query), cx);"),
+            "Tab in ScriptList should continue routing to show_inline_ai_chat"
+        );
+    }
+
+    #[test]
+    fn test_tab_interceptor_matches_both_tab_key_variants() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("matches!(event.keystroke.key.as_str(), \"tab\" | \"Tab\")"),
+            "Tab interceptor should match both tab key variants: \"tab\" and \"Tab\""
+        );
+    }
 }


thinking
**Rerunning whitespace check**
exec
/bin/zsh -lc 'git diff --check -- src/app_impl/startup.rs src/app_impl/startup_new_tab.rs src/app_impl/prompt_ai.rs src/app_impl/tests.rs' in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:

file update:
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index 98a0c9cd1db5a933992c0f99cb20a3c2ca436b41..c985a5090574816267103019d1b272e1e3151a38
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -1,4 +1,194 @@
 use super::*;
+use anyhow::{anyhow, Context as AnyhowContext, Result};
+
+#[derive(Debug, Clone, Copy)]
+enum AiScriptGenerationStage {
+    SelectModel,
+    ResolveProvider,
+    RequestCompletion,
+    ExtractScript,
+    CreateScriptFile,
+    WriteScriptFile,
+    OpenEditor,
+}
+
+impl AiScriptGenerationStage {
+    fn as_str(self) -> &'static str {
+        match self {
+            Self::SelectModel => "select_model",
+            Self::ResolveProvider => "resolve_provider",
+            Self::RequestCompletion => "request_completion",
+            Self::ExtractScript => "extract_script",
+            Self::CreateScriptFile => "create_script_file",
+            Self::WriteScriptFile => "write_script_file",
+            Self::OpenEditor => "open_editor",
+        }
+    }
+}
+
+const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You are ScriptKitScriptGenerator.
+Generate a complete Script Kit TypeScript script and return code only.
+Requirements:
+- Include metadata comments at the top:
+  // Name: <script name>
+  // Description: <short summary>
+- Include: import "@scriptkit/sdk";
+- Use idiomatic Script Kit APIs (for example await arg(), await div(), await editor()) when useful.
+- Prefer clear async/await flow and practical defaults.
+- Return a full runnable script with no extra explanation."#;
+
+fn build_ai_script_generation_user_prompt(description: &str) -> String {
+    format!(
+        "Generate a complete Script Kit script for this request:\n\n{}\n\nReturn only the TypeScript script source.",
+        description.trim()
+    )
+}
+
+fn select_default_ai_script_model(
+    registry: &crate::ai::ProviderRegistry,
+) -> Option<crate::ai::ModelInfo> {
+    let all_models = registry.get_all_models();
+    all_models
+        .iter()
+        .find(|model| model.provider.eq_ignore_ascii_case("vercel"))
+        .cloned()
+        .or_else(|| all_models.first().cloned())
+}
+
+fn extract_generated_script_source(raw_response: &str) -> Option<String> {
+    let trimmed = raw_response.trim();
+    if trimmed.is_empty() {
+        return None;
+    }
+
+    // Prefer typed code fences first, then generic fences.
+    for fence in ["```typescript", "```ts", "```javascript", "```js", "```"] {
+        if let Some(content) = extract_first_fenced_block(trimmed, fence) {
+            return normalize_generated_script(content);
+        }
+    }
+
+    normalize_generated_script(trimmed.to_string())
+}
+
+fn extract_first_fenced_block(response: &str, fence_start: &str) -> Option<String> {
+    let start_index = response.find(fence_start)?;
+    let after_fence = &response[start_index + fence_start.len()..];
+    let after_newline = after_fence
+        .strip_prefix("\r\n")
+        .or_else(|| after_fence.strip_prefix('\n'))
+        .unwrap_or(after_fence);
+    let end_index = after_newline.find("```")?;
+    Some(after_newline[..end_index].trim().to_string())
+}
+
+fn normalize_generated_script(content: String) -> Option<String> {
+    let trimmed = content.trim();
+    if trimmed.is_empty() {
+        None
+    } else {
+        Some(format!("{trimmed}\n"))
+    }
+}
+
+fn derive_script_name_from_description(description: &str) -> String {
+    let mut slug = String::new();
+    let mut last_was_dash = false;
+
+    for ch in description.chars() {
+        let lower = ch.to_ascii_lowercase();
+        if lower.is_ascii_alphanumeric() {
+            slug.push(lower);
+            last_was_dash = false;
+        } else if !last_was_dash && !slug.is_empty() {
+            slug.push('-');
+            last_was_dash = true;
+        }
+
+        if slug.len() >= 48 {
+            break;
+        }
+    }
+
+    while slug.ends_with('-') {
+        slug.pop();
+    }
+
+    if slug.is_empty() {
+        "ai-generated-script".to_string()
+    } else {
+        slug
+    }
+}
+
+fn generate_script_via_ai_backend(
+    registry: &crate::ai::ProviderRegistry,
+    model_id: &str,
+    prompt_description: &str,
+    config: &crate::config::Config,
+) -> Result<std::path::PathBuf> {
+    let provider = registry
+        .find_provider_for_model(model_id)
+        .cloned()
+        .ok_or_else(|| {
+            anyhow!(
+                "state={} attempted=find_provider_for_model model_id={} failure=provider_not_found",
+                AiScriptGenerationStage::ResolveProvider.as_str(),
+                model_id
+            )
+        })?;
+
+    let request_messages = vec![
+        crate::ai::ProviderMessage::system(AI_SCRIPT_GENERATION_SYSTEM_PROMPT),
+        crate::ai::ProviderMessage::user(build_ai_script_generation_user_prompt(prompt_description)),
+    ];
+
+    let ai_response = provider
+        .send_message(&request_messages, model_id)
+        .with_context(|| {
+            format!(
+                "state={} attempted=send_message model_id={} provider={} failure=provider_call_failed",
+                AiScriptGenerationStage::RequestCompletion.as_str(),
+                model_id,
+                provider.provider_id()
+            )
+        })?;
+
+    let generated_script = extract_generated_script_source(&ai_response).ok_or_else(|| {
+        anyhow!(
+            "state={} attempted=extract_generated_script model_id={} failure=empty_script_response",
+            AiScriptGenerationStage::ExtractScript.as_str(),
+            model_id
+        )
+    })?;
+
+    let script_name = derive_script_name_from_description(prompt_description);
+    let script_path = crate::script_creation::create_new_script(&script_name).with_context(|| {
+        format!(
+            "state={} attempted=create_new_script name={} failure=create_script_failed",
+            AiScriptGenerationStage::CreateScriptFile.as_str(),
+            script_name
+        )
+    })?;
+
+    std::fs::write(&script_path, generated_script).with_context(|| {
+        format!(
+            "state={} attempted=write_script path={} failure=write_failed",
+            AiScriptGenerationStage::WriteScriptFile.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    crate::script_creation::open_in_editor(&script_path, config).with_context(|| {
+        format!(
+            "state={} attempted=open_in_editor path={} failure=open_editor_failed",
+            AiScriptGenerationStage::OpenEditor.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
 
 impl ScriptListApp {
     pub(crate) fn is_in_prompt(&self) -> bool {
@@ -274,6 +464,145 @@
         cx.notify();
     }
 
+    /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
+    /// The generated script is saved to disk and opened in the configured editor.
+    pub fn generate_script_from_ai_prompt(
+        &mut self,
+        prompt_description: String,
+        cx: &mut Context<Self>,
+    ) {
+        let prompt_description = prompt_description.trim().to_string();
+        if prompt_description.is_empty() {
+            return;
+        }
+
+        let registry = self.cached_provider_registry.clone().unwrap_or_else(|| {
+            crate::ai::ProviderRegistry::from_environment_with_config(Some(&self.config))
+        });
+
+        if !registry.has_any_provider() {
+            self.toast_manager.push(
+                components::toast::Toast::error("No AI providers configured for script generation", &self.theme)
+                    .duration_ms(Some(5000)),
+            );
+            cx.notify();
+            return;
+        }
+
+        let selected_model = match select_default_ai_script_model(&registry) {
+            Some(model) => model,
+            None => {
+                let stage = AiScriptGenerationStage::SelectModel.as_str();
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    &format!(
+                        "state={} attempted=select_default_model failure=no_available_models",
+                        stage
+                    ),
+                );
+                self.toast_manager.push(
+                    components::toast::Toast::error(
+                        "No AI models available for script generation",
+                        &self.theme,
+                    )
+                    .duration_ms(Some(5000)),
+                );
+                cx.notify();
+                return;
+            }
+        };
+
+        let model_id = selected_model.id.clone();
+        let provider = selected_model.provider.clone();
+        let config = self.config.clone();
+        let (tx, rx) = async_channel::bounded::<std::result::Result<std::path::PathBuf, String>>(1);
+
+        logging::log(
+            "AI_SCRIPT_GEN",
+            &format!(
+                "state=queued attempted=shift_tab_script_generation model_id={} provider={} prompt_len={}",
+                model_id,
+                provider,
+                prompt_description.len()
+            ),
+        );
+        self.show_hud("Generating script with AI...".to_string(), Some(1500), cx);
+
+        std::thread::spawn(move || {
+            logging::log(
+                "AI_SCRIPT_GEN",
+                &format!(
+                    "state=running attempted=generate_script model_id={} provider={}",
+                    model_id, provider
+                ),
+            );
+
+            let generation_result =
+                generate_script_via_ai_backend(&registry, &model_id, &prompt_description, &config)
+                    .map_err(|error| error.to_string());
+
+            if tx.send_blocking(generation_result).is_err() {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=send_result failure=result_channel_closed",
+                );
+            }
+        });
+
+        cx.spawn(async move |this, cx| {
+            let Ok(result) = rx.recv().await else {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=recv_result failure=result_channel_closed",
+                );
+                return;
+            };
+
+            let _ = cx.update(|cx| {
+                this.update(cx, |app, cx| {
+                    match result {
+                        Ok(script_path) => {
+                            let script_name = script_path
+                                .file_name()
+                                .and_then(|name| name.to_str())
+                                .unwrap_or("generated script");
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!(
+                                    "state=completed attempted=generate_script path={}",
+                                    script_path.display()
+                                ),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::success(
+                                    format!("Generated and opened {}", script_name),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(3500)),
+                            );
+                            app.close_and_reset_window(cx);
+                        }
+                        Err(error) => {
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!("state=failed attempted=generate_script error={}", error),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::error(
+                                    format!("Failed to generate script: {}", error),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(7000)),
+                            );
+                            cx.notify();
+                        }
+                    }
+                })
+            });
+        })
+        .detach();
+    }
+
     /// Rebuild the cached provider registry in a background thread.
     /// Called after config changes (API key saved, Claude Code enabled, etc.)
     pub fn rebuild_provider_registry_async(&mut self, cx: &mut Context<Self>) {
@@ -312,3 +641,44 @@
         .detach();
     }
 }
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_build_ai_script_generation_user_prompt_includes_description() {
+        let prompt = build_ai_script_generation_user_prompt("create a weather script");
+        assert!(prompt.contains("create a weather script"));
+        assert!(prompt.contains("Script Kit script"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_prefers_typescript_fence() {
+        let response = r#"
+Here's your script:
+```typescript
+// Name: Weather
+import "@scriptkit/sdk";
+```
+"#;
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert!(extracted.contains("// Name: Weather"));
+        assert!(!extracted.contains("```"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_falls_back_to_plain_text() {
+        let response = "// Name: Plain\nimport \"@scriptkit/sdk\";";
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert_eq!(extracted, "// Name: Plain\nimport \"@scriptkit/sdk\";\n");
+    }
+
+    #[test]
+    fn test_derive_script_name_from_description_uses_fallback_for_symbols() {
+        let name = derive_script_name_from_description("@@@ !!!");
+        assert_eq!(name, "ai-generated-script");
+    }
+}
diff --git a/src/app_impl/startup.rs b/src/app_impl/startup.rs
index 2e95070751083b8f4c24cd7eae8372d6bec3f6d6..441b4f23bca1b6294025c698eeb11d45815e85e0
--- a/src/app_impl/startup.rs
+++ b/src/app_impl/startup.rs
@@ -555,9 +555,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -720,20 +722,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/startup_new_tab.rs b/src/app_impl/startup_new_tab.rs
index 6c807a195553d9f168087c2ca6d771c76e555c11..7c84270cbe91c0b1ddd460aa29db8c788766d0cf
--- a/src/app_impl/startup_new_tab.rs
+++ b/src/app_impl/startup_new_tab.rs
@@ -6,9 +6,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -171,20 +173,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/tests.rs b/src/app_impl/tests.rs
index d75ba121851d7233c5ae77bc474a64b27e7c31d2..3f0fbd1ccbc2f1e3d515ac1edf9666035f09eaf4
--- a/src/app_impl/tests.rs
+++ b/src/app_impl/tests.rs
@@ -1,6 +1,7 @@
 #[cfg(test)]
 mod app_impl_state_sync_tests {
     use super::{calculate_fallback_error_message, ScriptListApp};
+    use std::fs;
 
     #[test]
     fn test_sync_builtin_query_state_updates_query_and_selection_when_changed() {
@@ -46,4 +47,39 @@
         assert!(message.contains("Could not evaluate expression"));
         assert!(message.contains("Check the syntax and try again"));
     }
+
+    #[test]
+    fn test_shift_tab_routes_to_ai_script_generation_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("if has_shift")
+                && startup_tab.contains("this.generate_script_from_ai_prompt(query, cx);"),
+            "Shift+Tab in ScriptList should route to generate_script_from_ai_prompt. \
+             Missing expected branch in startup_new_tab.rs"
+        );
+    }
+
+    #[test]
+    fn test_tab_still_routes_to_inline_ai_chat_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("this.show_inline_ai_chat(Some(query), cx);"),
+            "Tab in ScriptList should continue routing to show_inline_ai_chat"
+        );
+    }
+
+    #[test]
+    fn test_tab_interceptor_matches_both_tab_key_variants() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("matches!(event.keystroke.key.as_str(), \"tab\" | \"Tab\")"),
+            "Tab interceptor should match both tab key variants: \"tab\" and \"Tab\""
+        );
+    }
 }


thinking
**Updating test release status**
exec
/bin/zsh -lc '~/.claude/bin/swarm.ts update --id codex-shift-tab-keybind --status done --notes "Shift+Tab routing + AI script generation action implemented; added prompt_ai helper tests and startup_new_tab source-routing assertions (including tab/tab key variants). Scoped cargo tests attempted but blocked by unrelated shared-tree compile errors." && ~/.claude/bin/swarm.ts release --id codex-shift-tab-keybind src/app_impl/tests.rs' in /Users/johnlindquist/dev/script-kit-gpui succeeded in 52ms:
Updated: codex-shift-tab-keybind
Released for codex-shift-tab-keybind: src/app_impl/tests.rs
file update:
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index 98a0c9cd1db5a933992c0f99cb20a3c2ca436b41..c985a5090574816267103019d1b272e1e3151a38
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -1,4 +1,194 @@
 use super::*;
+use anyhow::{anyhow, Context as AnyhowContext, Result};
+
+#[derive(Debug, Clone, Copy)]
+enum AiScriptGenerationStage {
+    SelectModel,
+    ResolveProvider,
+    RequestCompletion,
+    ExtractScript,
+    CreateScriptFile,
+    WriteScriptFile,
+    OpenEditor,
+}
+
+impl AiScriptGenerationStage {
+    fn as_str(self) -> &'static str {
+        match self {
+            Self::SelectModel => "select_model",
+            Self::ResolveProvider => "resolve_provider",
+            Self::RequestCompletion => "request_completion",
+            Self::ExtractScript => "extract_script",
+            Self::CreateScriptFile => "create_script_file",
+            Self::WriteScriptFile => "write_script_file",
+            Self::OpenEditor => "open_editor",
+        }
+    }
+}
+
+const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You are ScriptKitScriptGenerator.
+Generate a complete Script Kit TypeScript script and return code only.
+Requirements:
+- Include metadata comments at the top:
+  // Name: <script name>
+  // Description: <short summary>
+- Include: import "@scriptkit/sdk";
+- Use idiomatic Script Kit APIs (for example await arg(), await div(), await editor()) when useful.
+- Prefer clear async/await flow and practical defaults.
+- Return a full runnable script with no extra explanation."#;
+
+fn build_ai_script_generation_user_prompt(description: &str) -> String {
+    format!(
+        "Generate a complete Script Kit script for this request:\n\n{}\n\nReturn only the TypeScript script source.",
+        description.trim()
+    )
+}
+
+fn select_default_ai_script_model(
+    registry: &crate::ai::ProviderRegistry,
+) -> Option<crate::ai::ModelInfo> {
+    let all_models = registry.get_all_models();
+    all_models
+        .iter()
+        .find(|model| model.provider.eq_ignore_ascii_case("vercel"))
+        .cloned()
+        .or_else(|| all_models.first().cloned())
+}
+
+fn extract_generated_script_source(raw_response: &str) -> Option<String> {
+    let trimmed = raw_response.trim();
+    if trimmed.is_empty() {
+        return None;
+    }
+
+    // Prefer typed code fences first, then generic fences.
+    for fence in ["```typescript", "```ts", "```javascript", "```js", "```"] {
+        if let Some(content) = extract_first_fenced_block(trimmed, fence) {
+            return normalize_generated_script(content);
+        }
+    }
+
+    normalize_generated_script(trimmed.to_string())
+}
+
+fn extract_first_fenced_block(response: &str, fence_start: &str) -> Option<String> {
+    let start_index = response.find(fence_start)?;
+    let after_fence = &response[start_index + fence_start.len()..];
+    let after_newline = after_fence
+        .strip_prefix("\r\n")
+        .or_else(|| after_fence.strip_prefix('\n'))
+        .unwrap_or(after_fence);
+    let end_index = after_newline.find("```")?;
+    Some(after_newline[..end_index].trim().to_string())
+}
+
+fn normalize_generated_script(content: String) -> Option<String> {
+    let trimmed = content.trim();
+    if trimmed.is_empty() {
+        None
+    } else {
+        Some(format!("{trimmed}\n"))
+    }
+}
+
+fn derive_script_name_from_description(description: &str) -> String {
+    let mut slug = String::new();
+    let mut last_was_dash = false;
+
+    for ch in description.chars() {
+        let lower = ch.to_ascii_lowercase();
+        if lower.is_ascii_alphanumeric() {
+            slug.push(lower);
+            last_was_dash = false;
+        } else if !last_was_dash && !slug.is_empty() {
+            slug.push('-');
+            last_was_dash = true;
+        }
+
+        if slug.len() >= 48 {
+            break;
+        }
+    }
+
+    while slug.ends_with('-') {
+        slug.pop();
+    }
+
+    if slug.is_empty() {
+        "ai-generated-script".to_string()
+    } else {
+        slug
+    }
+}
+
+fn generate_script_via_ai_backend(
+    registry: &crate::ai::ProviderRegistry,
+    model_id: &str,
+    prompt_description: &str,
+    config: &crate::config::Config,
+) -> Result<std::path::PathBuf> {
+    let provider = registry
+        .find_provider_for_model(model_id)
+        .cloned()
+        .ok_or_else(|| {
+            anyhow!(
+                "state={} attempted=find_provider_for_model model_id={} failure=provider_not_found",
+                AiScriptGenerationStage::ResolveProvider.as_str(),
+                model_id
+            )
+        })?;
+
+    let request_messages = vec![
+        crate::ai::ProviderMessage::system(AI_SCRIPT_GENERATION_SYSTEM_PROMPT),
+        crate::ai::ProviderMessage::user(build_ai_script_generation_user_prompt(prompt_description)),
+    ];
+
+    let ai_response = provider
+        .send_message(&request_messages, model_id)
+        .with_context(|| {
+            format!(
+                "state={} attempted=send_message model_id={} provider={} failure=provider_call_failed",
+                AiScriptGenerationStage::RequestCompletion.as_str(),
+                model_id,
+                provider.provider_id()
+            )
+        })?;
+
+    let generated_script = extract_generated_script_source(&ai_response).ok_or_else(|| {
+        anyhow!(
+            "state={} attempted=extract_generated_script model_id={} failure=empty_script_response",
+            AiScriptGenerationStage::ExtractScript.as_str(),
+            model_id
+        )
+    })?;
+
+    let script_name = derive_script_name_from_description(prompt_description);
+    let script_path = crate::script_creation::create_new_script(&script_name).with_context(|| {
+        format!(
+            "state={} attempted=create_new_script name={} failure=create_script_failed",
+            AiScriptGenerationStage::CreateScriptFile.as_str(),
+            script_name
+        )
+    })?;
+
+    std::fs::write(&script_path, generated_script).with_context(|| {
+        format!(
+            "state={} attempted=write_script path={} failure=write_failed",
+            AiScriptGenerationStage::WriteScriptFile.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    crate::script_creation::open_in_editor(&script_path, config).with_context(|| {
+        format!(
+            "state={} attempted=open_in_editor path={} failure=open_editor_failed",
+            AiScriptGenerationStage::OpenEditor.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
 
 impl ScriptListApp {
     pub(crate) fn is_in_prompt(&self) -> bool {
@@ -274,6 +464,145 @@
         cx.notify();
     }
 
+    /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
+    /// The generated script is saved to disk and opened in the configured editor.
+    pub fn generate_script_from_ai_prompt(
+        &mut self,
+        prompt_description: String,
+        cx: &mut Context<Self>,
+    ) {
+        let prompt_description = prompt_description.trim().to_string();
+        if prompt_description.is_empty() {
+            return;
+        }
+
+        let registry = self.cached_provider_registry.clone().unwrap_or_else(|| {
+            crate::ai::ProviderRegistry::from_environment_with_config(Some(&self.config))
+        });
+
+        if !registry.has_any_provider() {
+            self.toast_manager.push(
+                components::toast::Toast::error("No AI providers configured for script generation", &self.theme)
+                    .duration_ms(Some(5000)),
+            );
+            cx.notify();
+            return;
+        }
+
+        let selected_model = match select_default_ai_script_model(&registry) {
+            Some(model) => model,
+            None => {
+                let stage = AiScriptGenerationStage::SelectModel.as_str();
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    &format!(
+                        "state={} attempted=select_default_model failure=no_available_models",
+                        stage
+                    ),
+                );
+                self.toast_manager.push(
+                    components::toast::Toast::error(
+                        "No AI models available for script generation",
+                        &self.theme,
+                    )
+                    .duration_ms(Some(5000)),
+                );
+                cx.notify();
+                return;
+            }
+        };
+
+        let model_id = selected_model.id.clone();
+        let provider = selected_model.provider.clone();
+        let config = self.config.clone();
+        let (tx, rx) = async_channel::bounded::<std::result::Result<std::path::PathBuf, String>>(1);
+
+        logging::log(
+            "AI_SCRIPT_GEN",
+            &format!(
+                "state=queued attempted=shift_tab_script_generation model_id={} provider={} prompt_len={}",
+                model_id,
+                provider,
+                prompt_description.len()
+            ),
+        );
+        self.show_hud("Generating script with AI...".to_string(), Some(1500), cx);
+
+        std::thread::spawn(move || {
+            logging::log(
+                "AI_SCRIPT_GEN",
+                &format!(
+                    "state=running attempted=generate_script model_id={} provider={}",
+                    model_id, provider
+                ),
+            );
+
+            let generation_result =
+                generate_script_via_ai_backend(&registry, &model_id, &prompt_description, &config)
+                    .map_err(|error| error.to_string());
+
+            if tx.send_blocking(generation_result).is_err() {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=send_result failure=result_channel_closed",
+                );
+            }
+        });
+
+        cx.spawn(async move |this, cx| {
+            let Ok(result) = rx.recv().await else {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=recv_result failure=result_channel_closed",
+                );
+                return;
+            };
+
+            let _ = cx.update(|cx| {
+                this.update(cx, |app, cx| {
+                    match result {
+                        Ok(script_path) => {
+                            let script_name = script_path
+                                .file_name()
+                                .and_then(|name| name.to_str())
+                                .unwrap_or("generated script");
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!(
+                                    "state=completed attempted=generate_script path={}",
+                                    script_path.display()
+                                ),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::success(
+                                    format!("Generated and opened {}", script_name),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(3500)),
+                            );
+                            app.close_and_reset_window(cx);
+                        }
+                        Err(error) => {
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!("state=failed attempted=generate_script error={}", error),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::error(
+                                    format!("Failed to generate script: {}", error),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(7000)),
+                            );
+                            cx.notify();
+                        }
+                    }
+                })
+            });
+        })
+        .detach();
+    }
+
     /// Rebuild the cached provider registry in a background thread.
     /// Called after config changes (API key saved, Claude Code enabled, etc.)
     pub fn rebuild_provider_registry_async(&mut self, cx: &mut Context<Self>) {
@@ -312,3 +641,44 @@
         .detach();
     }
 }
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_build_ai_script_generation_user_prompt_includes_description() {
+        let prompt = build_ai_script_generation_user_prompt("create a weather script");
+        assert!(prompt.contains("create a weather script"));
+        assert!(prompt.contains("Script Kit script"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_prefers_typescript_fence() {
+        let response = r#"
+Here's your script:
+```typescript
+// Name: Weather
+import "@scriptkit/sdk";
+```
+"#;
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert!(extracted.contains("// Name: Weather"));
+        assert!(!extracted.contains("```"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_falls_back_to_plain_text() {
+        let response = "// Name: Plain\nimport \"@scriptkit/sdk\";";
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert_eq!(extracted, "// Name: Plain\nimport \"@scriptkit/sdk\";\n");
+    }
+
+    #[test]
+    fn test_derive_script_name_from_description_uses_fallback_for_symbols() {
+        let name = derive_script_name_from_description("@@@ !!!");
+        assert_eq!(name, "ai-generated-script");
+    }
+}
diff --git a/src/app_impl/startup.rs b/src/app_impl/startup.rs
index 2e95070751083b8f4c24cd7eae8372d6bec3f6d6..441b4f23bca1b6294025c698eeb11d45815e85e0
--- a/src/app_impl/startup.rs
+++ b/src/app_impl/startup.rs
@@ -555,9 +555,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -720,20 +722,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/startup_new_tab.rs b/src/app_impl/startup_new_tab.rs
index 6c807a195553d9f168087c2ca6d771c76e555c11..7c84270cbe91c0b1ddd460aa29db8c788766d0cf
--- a/src/app_impl/startup_new_tab.rs
+++ b/src/app_impl/startup_new_tab.rs
@@ -6,9 +6,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -171,20 +173,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/tests.rs b/src/app_impl/tests.rs
index d75ba121851d7233c5ae77bc474a64b27e7c31d2..3f0fbd1ccbc2f1e3d515ac1edf9666035f09eaf4
--- a/src/app_impl/tests.rs
+++ b/src/app_impl/tests.rs
@@ -1,6 +1,7 @@
 #[cfg(test)]
 mod app_impl_state_sync_tests {
     use super::{calculate_fallback_error_message, ScriptListApp};
+    use std::fs;
 
     #[test]
     fn test_sync_builtin_query_state_updates_query_and_selection_when_changed() {
@@ -46,4 +47,39 @@
         assert!(message.contains("Could not evaluate expression"));
         assert!(message.contains("Check the syntax and try again"));
     }
+
+    #[test]
+    fn test_shift_tab_routes_to_ai_script_generation_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("if has_shift")
+                && startup_tab.contains("this.generate_script_from_ai_prompt(query, cx);"),
+            "Shift+Tab in ScriptList should route to generate_script_from_ai_prompt. \
+             Missing expected branch in startup_new_tab.rs"
+        );
+    }
+
+    #[test]
+    fn test_tab_still_routes_to_inline_ai_chat_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("this.show_inline_ai_chat(Some(query), cx);"),
+            "Tab in ScriptList should continue routing to show_inline_ai_chat"
+        );
+    }
+
+    #[test]
+    fn test_tab_interceptor_matches_both_tab_key_variants() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("matches!(event.keystroke.key.as_str(), \"tab\" | \"Tab\")"),
+            "Tab interceptor should match both tab key variants: \"tab\" and \"Tab\""
+        );
+    }
 }


thinking
**Filtering inactive agents**
exec
/bin/zsh -lc "~/.claude/bin/swarm.ts status | sed -n '1,40p'" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
Repo: /Users/johnlindquist/dev/script-kit-gpui
Updated: 2026-02-08T02:55:15.919Z

Active agents (1):
codex-script-gen-builtin [in_progress] codex/gpt-5.3-codex/exec
  task: In Script Kit GPUI, there needs to be a built-in feature or action for AI script generation triggered by Shift+Tab. Loo…
  scope: src/builtins/**,src/app_execute/**,src/ai/**,src/kit_store/**
  claims(6): src/builtins/part_001_entries/entries_002.rs, src/builtins/part_000.rs, src/builtins/tests/part_000.rs, src/app_execute/builtin_execution.rs, src/ai/script_gen…
  notes: Claimed builtin/app_execute/ai files; implementing AI script generation command + tests

Inactive agents (254):
- codex-accessibility [done] done. exit=0. final=.ai/logs/codex-accessibility.final.md. See final message file.
- codex-action-labels [failed] failed. exit=1. final=.ai/logs/codex-action-labels.final.md.
- codex-actions-audit [failed] failed. exit=1. final=.ai/logs/codex-actions-audit.final.md. See final message file.
- codex-actions-consistency [failed] failed. exit=1. final=.ai/logs/codex-actions-consistency.final.md.
- codex-actions-dialog [done] done. exit=0. final=.ai/logs/codex-actions-dialog.final.md. See final message file.
- codex-actions-feedback [failed] failed. exit=1. final=.ai/logs/codex-actions-feedback.final.md.
- codex-actions-feedback-v2 [done] done. exit=0. final=.ai/logs/codex-actions-feedback-v2.final.md. See final message file.
- codex-actions-ux [done] done. exit=0. final=.ai/logs/codex-actions-ux.final.md. See final message file.
- codex-agents-md [done] done. exit=0. final=.ai/logs/codex-agents-md.final.md. See final message file.
file update:
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index 98a0c9cd1db5a933992c0f99cb20a3c2ca436b41..c985a5090574816267103019d1b272e1e3151a38
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -1,4 +1,194 @@
 use super::*;
+use anyhow::{anyhow, Context as AnyhowContext, Result};
+
+#[derive(Debug, Clone, Copy)]
+enum AiScriptGenerationStage {
+    SelectModel,
+    ResolveProvider,
+    RequestCompletion,
+    ExtractScript,
+    CreateScriptFile,
+    WriteScriptFile,
+    OpenEditor,
+}
+
+impl AiScriptGenerationStage {
+    fn as_str(self) -> &'static str {
+        match self {
+            Self::SelectModel => "select_model",
+            Self::ResolveProvider => "resolve_provider",
+            Self::RequestCompletion => "request_completion",
+            Self::ExtractScript => "extract_script",
+            Self::CreateScriptFile => "create_script_file",
+            Self::WriteScriptFile => "write_script_file",
+            Self::OpenEditor => "open_editor",
+        }
+    }
+}
+
+const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You are ScriptKitScriptGenerator.
+Generate a complete Script Kit TypeScript script and return code only.
+Requirements:
+- Include metadata comments at the top:
+  // Name: <script name>
+  // Description: <short summary>
+- Include: import "@scriptkit/sdk";
+- Use idiomatic Script Kit APIs (for example await arg(), await div(), await editor()) when useful.
+- Prefer clear async/await flow and practical defaults.
+- Return a full runnable script with no extra explanation."#;
+
+fn build_ai_script_generation_user_prompt(description: &str) -> String {
+    format!(
+        "Generate a complete Script Kit script for this request:\n\n{}\n\nReturn only the TypeScript script source.",
+        description.trim()
+    )
+}
+
+fn select_default_ai_script_model(
+    registry: &crate::ai::ProviderRegistry,
+) -> Option<crate::ai::ModelInfo> {
+    let all_models = registry.get_all_models();
+    all_models
+        .iter()
+        .find(|model| model.provider.eq_ignore_ascii_case("vercel"))
+        .cloned()
+        .or_else(|| all_models.first().cloned())
+}
+
+fn extract_generated_script_source(raw_response: &str) -> Option<String> {
+    let trimmed = raw_response.trim();
+    if trimmed.is_empty() {
+        return None;
+    }
+
+    // Prefer typed code fences first, then generic fences.
+    for fence in ["```typescript", "```ts", "```javascript", "```js", "```"] {
+        if let Some(content) = extract_first_fenced_block(trimmed, fence) {
+            return normalize_generated_script(content);
+        }
+    }
+
+    normalize_generated_script(trimmed.to_string())
+}
+
+fn extract_first_fenced_block(response: &str, fence_start: &str) -> Option<String> {
+    let start_index = response.find(fence_start)?;
+    let after_fence = &response[start_index + fence_start.len()..];
+    let after_newline = after_fence
+        .strip_prefix("\r\n")
+        .or_else(|| after_fence.strip_prefix('\n'))
+        .unwrap_or(after_fence);
+    let end_index = after_newline.find("```")?;
+    Some(after_newline[..end_index].trim().to_string())
+}
+
+fn normalize_generated_script(content: String) -> Option<String> {
+    let trimmed = content.trim();
+    if trimmed.is_empty() {
+        None
+    } else {
+        Some(format!("{trimmed}\n"))
+    }
+}
+
+fn derive_script_name_from_description(description: &str) -> String {
+    let mut slug = String::new();
+    let mut last_was_dash = false;
+
+    for ch in description.chars() {
+        let lower = ch.to_ascii_lowercase();
+        if lower.is_ascii_alphanumeric() {
+            slug.push(lower);
+            last_was_dash = false;
+        } else if !last_was_dash && !slug.is_empty() {
+            slug.push('-');
+            last_was_dash = true;
+        }
+
+        if slug.len() >= 48 {
+            break;
+        }
+    }
+
+    while slug.ends_with('-') {
+        slug.pop();
+    }
+
+    if slug.is_empty() {
+        "ai-generated-script".to_string()
+    } else {
+        slug
+    }
+}
+
+fn generate_script_via_ai_backend(
+    registry: &crate::ai::ProviderRegistry,
+    model_id: &str,
+    prompt_description: &str,
+    config: &crate::config::Config,
+) -> Result<std::path::PathBuf> {
+    let provider = registry
+        .find_provider_for_model(model_id)
+        .cloned()
+        .ok_or_else(|| {
+            anyhow!(
+                "state={} attempted=find_provider_for_model model_id={} failure=provider_not_found",
+                AiScriptGenerationStage::ResolveProvider.as_str(),
+                model_id
+            )
+        })?;
+
+    let request_messages = vec![
+        crate::ai::ProviderMessage::system(AI_SCRIPT_GENERATION_SYSTEM_PROMPT),
+        crate::ai::ProviderMessage::user(build_ai_script_generation_user_prompt(prompt_description)),
+    ];
+
+    let ai_response = provider
+        .send_message(&request_messages, model_id)
+        .with_context(|| {
+            format!(
+                "state={} attempted=send_message model_id={} provider={} failure=provider_call_failed",
+                AiScriptGenerationStage::RequestCompletion.as_str(),
+                model_id,
+                provider.provider_id()
+            )
+        })?;
+
+    let generated_script = extract_generated_script_source(&ai_response).ok_or_else(|| {
+        anyhow!(
+            "state={} attempted=extract_generated_script model_id={} failure=empty_script_response",
+            AiScriptGenerationStage::ExtractScript.as_str(),
+            model_id
+        )
+    })?;
+
+    let script_name = derive_script_name_from_description(prompt_description);
+    let script_path = crate::script_creation::create_new_script(&script_name).with_context(|| {
+        format!(
+            "state={} attempted=create_new_script name={} failure=create_script_failed",
+            AiScriptGenerationStage::CreateScriptFile.as_str(),
+            script_name
+        )
+    })?;
+
+    std::fs::write(&script_path, generated_script).with_context(|| {
+        format!(
+            "state={} attempted=write_script path={} failure=write_failed",
+            AiScriptGenerationStage::WriteScriptFile.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    crate::script_creation::open_in_editor(&script_path, config).with_context(|| {
+        format!(
+            "state={} attempted=open_in_editor path={} failure=open_editor_failed",
+            AiScriptGenerationStage::OpenEditor.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
 
 impl ScriptListApp {
     pub(crate) fn is_in_prompt(&self) -> bool {
@@ -274,6 +464,145 @@
         cx.notify();
     }
 
+    /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
+    /// The generated script is saved to disk and opened in the configured editor.
+    pub fn generate_script_from_ai_prompt(
+        &mut self,
+        prompt_description: String,
+        cx: &mut Context<Self>,
+    ) {
+        let prompt_description = prompt_description.trim().to_string();
+        if prompt_description.is_empty() {
+            return;
+        }
+
+        let registry = self.cached_provider_registry.clone().unwrap_or_else(|| {
+            crate::ai::ProviderRegistry::from_environment_with_config(Some(&self.config))
+        });
+
+        if !registry.has_any_provider() {
+            self.toast_manager.push(
+                components::toast::Toast::error("No AI providers configured for script generation", &self.theme)
+                    .duration_ms(Some(5000)),
+            );
+            cx.notify();
+            return;
+        }
+
+        let selected_model = match select_default_ai_script_model(&registry) {
+            Some(model) => model,
+            None => {
+                let stage = AiScriptGenerationStage::SelectModel.as_str();
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    &format!(
+                        "state={} attempted=select_default_model failure=no_available_models",
+                        stage
+                    ),
+                );
+                self.toast_manager.push(
+                    components::toast::Toast::error(
+                        "No AI models available for script generation",
+                        &self.theme,
+                    )
+                    .duration_ms(Some(5000)),
+                );
+                cx.notify();
+                return;
+            }
+        };
+
+        let model_id = selected_model.id.clone();
+        let provider = selected_model.provider.clone();
+        let config = self.config.clone();
+        let (tx, rx) = async_channel::bounded::<std::result::Result<std::path::PathBuf, String>>(1);
+
+        logging::log(
+            "AI_SCRIPT_GEN",
+            &format!(
+                "state=queued attempted=shift_tab_script_generation model_id={} provider={} prompt_len={}",
+                model_id,
+                provider,
+                prompt_description.len()
+            ),
+        );
+        self.show_hud("Generating script with AI...".to_string(), Some(1500), cx);
+
+        std::thread::spawn(move || {
+            logging::log(
+                "AI_SCRIPT_GEN",
+                &format!(
+                    "state=running attempted=generate_script model_id={} provider={}",
+                    model_id, provider
+                ),
+            );
+
+            let generation_result =
+                generate_script_via_ai_backend(&registry, &model_id, &prompt_description, &config)
+                    .map_err(|error| error.to_string());
+
+            if tx.send_blocking(generation_result).is_err() {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=send_result failure=result_channel_closed",
+                );
+            }
+        });
+
+        cx.spawn(async move |this, cx| {
+            let Ok(result) = rx.recv().await else {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=recv_result failure=result_channel_closed",
+                );
+                return;
+            };
+
+            let _ = cx.update(|cx| {
+                this.update(cx, |app, cx| {
+                    match result {
+                        Ok(script_path) => {
+                            let script_name = script_path
+                                .file_name()
+                                .and_then(|name| name.to_str())
+                                .unwrap_or("generated script");
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!(
+                                    "state=completed attempted=generate_script path={}",
+                                    script_path.display()
+                                ),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::success(
+                                    format!("Generated and opened {}", script_name),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(3500)),
+                            );
+                            app.close_and_reset_window(cx);
+                        }
+                        Err(error) => {
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!("state=failed attempted=generate_script error={}", error),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::error(
+                                    format!("Failed to generate script: {}", error),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(7000)),
+                            );
+                            cx.notify();
+                        }
+                    }
+                })
+            });
+        })
+        .detach();
+    }
+
     /// Rebuild the cached provider registry in a background thread.
     /// Called after config changes (API key saved, Claude Code enabled, etc.)
     pub fn rebuild_provider_registry_async(&mut self, cx: &mut Context<Self>) {
@@ -312,3 +641,44 @@
         .detach();
     }
 }
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_build_ai_script_generation_user_prompt_includes_description() {
+        let prompt = build_ai_script_generation_user_prompt("create a weather script");
+        assert!(prompt.contains("create a weather script"));
+        assert!(prompt.contains("Script Kit script"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_prefers_typescript_fence() {
+        let response = r#"
+Here's your script:
+```typescript
+// Name: Weather
+import "@scriptkit/sdk";
+```
+"#;
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert!(extracted.contains("// Name: Weather"));
+        assert!(!extracted.contains("```"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_falls_back_to_plain_text() {
+        let response = "// Name: Plain\nimport \"@scriptkit/sdk\";";
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert_eq!(extracted, "// Name: Plain\nimport \"@scriptkit/sdk\";\n");
+    }
+
+    #[test]
+    fn test_derive_script_name_from_description_uses_fallback_for_symbols() {
+        let name = derive_script_name_from_description("@@@ !!!");
+        assert_eq!(name, "ai-generated-script");
+    }
+}
diff --git a/src/app_impl/startup.rs b/src/app_impl/startup.rs
index 2e95070751083b8f4c24cd7eae8372d6bec3f6d6..441b4f23bca1b6294025c698eeb11d45815e85e0
--- a/src/app_impl/startup.rs
+++ b/src/app_impl/startup.rs
@@ -555,9 +555,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -720,20 +722,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/startup_new_tab.rs b/src/app_impl/startup_new_tab.rs
index 6c807a195553d9f168087c2ca6d771c76e555c11..7c84270cbe91c0b1ddd460aa29db8c788766d0cf
--- a/src/app_impl/startup_new_tab.rs
+++ b/src/app_impl/startup_new_tab.rs
@@ -6,9 +6,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -171,20 +173,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/tests.rs b/src/app_impl/tests.rs
index d75ba121851d7233c5ae77bc474a64b27e7c31d2..3f0fbd1ccbc2f1e3d515ac1edf9666035f09eaf4
--- a/src/app_impl/tests.rs
+++ b/src/app_impl/tests.rs
@@ -1,6 +1,7 @@
 #[cfg(test)]
 mod app_impl_state_sync_tests {
     use super::{calculate_fallback_error_message, ScriptListApp};
+    use std::fs;
 
     #[test]
     fn test_sync_builtin_query_state_updates_query_and_selection_when_changed() {
@@ -46,4 +47,39 @@
         assert!(message.contains("Could not evaluate expression"));
         assert!(message.contains("Check the syntax and try again"));
     }
+
+    #[test]
+    fn test_shift_tab_routes_to_ai_script_generation_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("if has_shift")
+                && startup_tab.contains("this.generate_script_from_ai_prompt(query, cx);"),
+            "Shift+Tab in ScriptList should route to generate_script_from_ai_prompt. \
+             Missing expected branch in startup_new_tab.rs"
+        );
+    }
+
+    #[test]
+    fn test_tab_still_routes_to_inline_ai_chat_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("this.show_inline_ai_chat(Some(query), cx);"),
+            "Tab in ScriptList should continue routing to show_inline_ai_chat"
+        );
+    }
+
+    #[test]
+    fn test_tab_interceptor_matches_both_tab_key_variants() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("matches!(event.keystroke.key.as_str(), \"tab\" | \"Tab\")"),
+            "Tab interceptor should match both tab key variants: \"tab\" and \"Tab\""
+        );
+    }
 }


thinking
**Summarizing implementation, tests, and risks**
codex
Implemented `Shift+Tab` script-generation flow in parallel with existing `Tab` chat flow.

Changed files:
- `src/app_impl/startup.rs:558`  
  - Tab key matcher now explicitly supports both variants (`"tab" | "Tab"`).
  - In ScriptList, `Shift+Tab` now routes to `generate_script_from_ai_prompt(...)`; plain `Tab` still routes to `show_inline_ai_chat(...)`.
- `src/app_impl/startup_new_tab.rs:9`  
  - Mirrored the same Tab/Shift+Tab interceptor behavior in the new-tab startup path.
- `src/app_impl/prompt_ai.rs:29`  
  - Added AI script-generation prompt/instructions for Script Kit best practices.
  - Added generation pipeline helpers (model selection, fenced-code extraction, script-name derivation, backend call).
  - Added `generate_script_from_ai_prompt(...)` (`src/app_impl/prompt_ai.rs:469`) to:
    - use current input as prompt,
    - call AI provider backend,
    - create/write a new script file,
    - open it in the editor,
    - emit structured logs/toasts.
- `src/app_impl/tests.rs:52`  
  - Added tests asserting:
    - Shift+Tab routes to script generation,
    - Tab still routes to inline AI chat,
    - interceptor matches both key variants (`"tab" | "Tab"`).

Verification run:
- Attempted scoped tests:
  - `cargo test --lib app_impl::prompt_ai::tests::test_build_ai_script_generation_user_prompt_includes_description`
  - `cargo test --lib app_impl::tests::app_impl_state_sync_tests::test_shift_tab_routes_to_ai_script_generation_in_script_list_tab_interceptor`
- Result: blocked by unrelated shared-tree compile failures (duplicate module files and unrelated `src/actions/tests/**` errors).
- Patch hygiene check passed:
  - `git diff --check -- src/app_impl/startup.rs src/app_impl/startup_new_tab.rs src/app_impl/prompt_ai.rs src/app_impl/tests.rs`

How to test:
1. Run app with logs:
   - `echo '{"type":"show"}' | SCRIPT_KIT_AI_LOG=1 ./target/debug/script-kit-gpui 2>&1`
2. In main ScriptList input, type a script request.
3. Press `Shift+Tab`.
4. Verify `AI_SCRIPT_GEN` logs appear, a new script is created, and editor opens it.
5. Press `Tab` with input text and verify inline AI chat still opens.

Risks / known gaps:
- Automated Rust verification for this change is currently blocked by unrelated compile issues in the shared tree.
- Input text is cleared before generation starts; on failure, the prompt is not restored automatically.

Commits:
- None made.
file update:
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index 98a0c9cd1db5a933992c0f99cb20a3c2ca436b41..c985a5090574816267103019d1b272e1e3151a38
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -1,4 +1,194 @@
 use super::*;
+use anyhow::{anyhow, Context as AnyhowContext, Result};
+
+#[derive(Debug, Clone, Copy)]
+enum AiScriptGenerationStage {
+    SelectModel,
+    ResolveProvider,
+    RequestCompletion,
+    ExtractScript,
+    CreateScriptFile,
+    WriteScriptFile,
+    OpenEditor,
+}
+
+impl AiScriptGenerationStage {
+    fn as_str(self) -> &'static str {
+        match self {
+            Self::SelectModel => "select_model",
+            Self::ResolveProvider => "resolve_provider",
+            Self::RequestCompletion => "request_completion",
+            Self::ExtractScript => "extract_script",
+            Self::CreateScriptFile => "create_script_file",
+            Self::WriteScriptFile => "write_script_file",
+            Self::OpenEditor => "open_editor",
+        }
+    }
+}
+
+const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You are ScriptKitScriptGenerator.
+Generate a complete Script Kit TypeScript script and return code only.
+Requirements:
+- Include metadata comments at the top:
+  // Name: <script name>
+  // Description: <short summary>
+- Include: import "@scriptkit/sdk";
+- Use idiomatic Script Kit APIs (for example await arg(), await div(), await editor()) when useful.
+- Prefer clear async/await flow and practical defaults.
+- Return a full runnable script with no extra explanation."#;
+
+fn build_ai_script_generation_user_prompt(description: &str) -> String {
+    format!(
+        "Generate a complete Script Kit script for this request:\n\n{}\n\nReturn only the TypeScript script source.",
+        description.trim()
+    )
+}
+
+fn select_default_ai_script_model(
+    registry: &crate::ai::ProviderRegistry,
+) -> Option<crate::ai::ModelInfo> {
+    let all_models = registry.get_all_models();
+    all_models
+        .iter()
+        .find(|model| model.provider.eq_ignore_ascii_case("vercel"))
+        .cloned()
+        .or_else(|| all_models.first().cloned())
+}
+
+fn extract_generated_script_source(raw_response: &str) -> Option<String> {
+    let trimmed = raw_response.trim();
+    if trimmed.is_empty() {
+        return None;
+    }
+
+    // Prefer typed code fences first, then generic fences.
+    for fence in ["```typescript", "```ts", "```javascript", "```js", "```"] {
+        if let Some(content) = extract_first_fenced_block(trimmed, fence) {
+            return normalize_generated_script(content);
+        }
+    }
+
+    normalize_generated_script(trimmed.to_string())
+}
+
+fn extract_first_fenced_block(response: &str, fence_start: &str) -> Option<String> {
+    let start_index = response.find(fence_start)?;
+    let after_fence = &response[start_index + fence_start.len()..];
+    let after_newline = after_fence
+        .strip_prefix("\r\n")
+        .or_else(|| after_fence.strip_prefix('\n'))
+        .unwrap_or(after_fence);
+    let end_index = after_newline.find("```")?;
+    Some(after_newline[..end_index].trim().to_string())
+}
+
+fn normalize_generated_script(content: String) -> Option<String> {
+    let trimmed = content.trim();
+    if trimmed.is_empty() {
+        None
+    } else {
+        Some(format!("{trimmed}\n"))
+    }
+}
+
+fn derive_script_name_from_description(description: &str) -> String {
+    let mut slug = String::new();
+    let mut last_was_dash = false;
+
+    for ch in description.chars() {
+        let lower = ch.to_ascii_lowercase();
+        if lower.is_ascii_alphanumeric() {
+            slug.push(lower);
+            last_was_dash = false;
+        } else if !last_was_dash && !slug.is_empty() {
+            slug.push('-');
+            last_was_dash = true;
+        }
+
+        if slug.len() >= 48 {
+            break;
+        }
+    }
+
+    while slug.ends_with('-') {
+        slug.pop();
+    }
+
+    if slug.is_empty() {
+        "ai-generated-script".to_string()
+    } else {
+        slug
+    }
+}
+
+fn generate_script_via_ai_backend(
+    registry: &crate::ai::ProviderRegistry,
+    model_id: &str,
+    prompt_description: &str,
+    config: &crate::config::Config,
+) -> Result<std::path::PathBuf> {
+    let provider = registry
+        .find_provider_for_model(model_id)
+        .cloned()
+        .ok_or_else(|| {
+            anyhow!(
+                "state={} attempted=find_provider_for_model model_id={} failure=provider_not_found",
+                AiScriptGenerationStage::ResolveProvider.as_str(),
+                model_id
+            )
+        })?;
+
+    let request_messages = vec![
+        crate::ai::ProviderMessage::system(AI_SCRIPT_GENERATION_SYSTEM_PROMPT),
+        crate::ai::ProviderMessage::user(build_ai_script_generation_user_prompt(prompt_description)),
+    ];
+
+    let ai_response = provider
+        .send_message(&request_messages, model_id)
+        .with_context(|| {
+            format!(
+                "state={} attempted=send_message model_id={} provider={} failure=provider_call_failed",
+                AiScriptGenerationStage::RequestCompletion.as_str(),
+                model_id,
+                provider.provider_id()
+            )
+        })?;
+
+    let generated_script = extract_generated_script_source(&ai_response).ok_or_else(|| {
+        anyhow!(
+            "state={} attempted=extract_generated_script model_id={} failure=empty_script_response",
+            AiScriptGenerationStage::ExtractScript.as_str(),
+            model_id
+        )
+    })?;
+
+    let script_name = derive_script_name_from_description(prompt_description);
+    let script_path = crate::script_creation::create_new_script(&script_name).with_context(|| {
+        format!(
+            "state={} attempted=create_new_script name={} failure=create_script_failed",
+            AiScriptGenerationStage::CreateScriptFile.as_str(),
+            script_name
+        )
+    })?;
+
+    std::fs::write(&script_path, generated_script).with_context(|| {
+        format!(
+            "state={} attempted=write_script path={} failure=write_failed",
+            AiScriptGenerationStage::WriteScriptFile.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    crate::script_creation::open_in_editor(&script_path, config).with_context(|| {
+        format!(
+            "state={} attempted=open_in_editor path={} failure=open_editor_failed",
+            AiScriptGenerationStage::OpenEditor.as_str(),
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
 
 impl ScriptListApp {
     pub(crate) fn is_in_prompt(&self) -> bool {
@@ -274,6 +464,145 @@
         cx.notify();
     }
 
+    /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
+    /// The generated script is saved to disk and opened in the configured editor.
+    pub fn generate_script_from_ai_prompt(
+        &mut self,
+        prompt_description: String,
+        cx: &mut Context<Self>,
+    ) {
+        let prompt_description = prompt_description.trim().to_string();
+        if prompt_description.is_empty() {
+            return;
+        }
+
+        let registry = self.cached_provider_registry.clone().unwrap_or_else(|| {
+            crate::ai::ProviderRegistry::from_environment_with_config(Some(&self.config))
+        });
+
+        if !registry.has_any_provider() {
+            self.toast_manager.push(
+                components::toast::Toast::error("No AI providers configured for script generation", &self.theme)
+                    .duration_ms(Some(5000)),
+            );
+            cx.notify();
+            return;
+        }
+
+        let selected_model = match select_default_ai_script_model(&registry) {
+            Some(model) => model,
+            None => {
+                let stage = AiScriptGenerationStage::SelectModel.as_str();
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    &format!(
+                        "state={} attempted=select_default_model failure=no_available_models",
+                        stage
+                    ),
+                );
+                self.toast_manager.push(
+                    components::toast::Toast::error(
+                        "No AI models available for script generation",
+                        &self.theme,
+                    )
+                    .duration_ms(Some(5000)),
+                );
+                cx.notify();
+                return;
+            }
+        };
+
+        let model_id = selected_model.id.clone();
+        let provider = selected_model.provider.clone();
+        let config = self.config.clone();
+        let (tx, rx) = async_channel::bounded::<std::result::Result<std::path::PathBuf, String>>(1);
+
+        logging::log(
+            "AI_SCRIPT_GEN",
+            &format!(
+                "state=queued attempted=shift_tab_script_generation model_id={} provider={} prompt_len={}",
+                model_id,
+                provider,
+                prompt_description.len()
+            ),
+        );
+        self.show_hud("Generating script with AI...".to_string(), Some(1500), cx);
+
+        std::thread::spawn(move || {
+            logging::log(
+                "AI_SCRIPT_GEN",
+                &format!(
+                    "state=running attempted=generate_script model_id={} provider={}",
+                    model_id, provider
+                ),
+            );
+
+            let generation_result =
+                generate_script_via_ai_backend(&registry, &model_id, &prompt_description, &config)
+                    .map_err(|error| error.to_string());
+
+            if tx.send_blocking(generation_result).is_err() {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=send_result failure=result_channel_closed",
+                );
+            }
+        });
+
+        cx.spawn(async move |this, cx| {
+            let Ok(result) = rx.recv().await else {
+                logging::log(
+                    "AI_SCRIPT_GEN",
+                    "state=aborted attempted=recv_result failure=result_channel_closed",
+                );
+                return;
+            };
+
+            let _ = cx.update(|cx| {
+                this.update(cx, |app, cx| {
+                    match result {
+                        Ok(script_path) => {
+                            let script_name = script_path
+                                .file_name()
+                                .and_then(|name| name.to_str())
+                                .unwrap_or("generated script");
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!(
+                                    "state=completed attempted=generate_script path={}",
+                                    script_path.display()
+                                ),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::success(
+                                    format!("Generated and opened {}", script_name),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(3500)),
+                            );
+                            app.close_and_reset_window(cx);
+                        }
+                        Err(error) => {
+                            logging::log(
+                                "AI_SCRIPT_GEN",
+                                &format!("state=failed attempted=generate_script error={}", error),
+                            );
+                            app.toast_manager.push(
+                                components::toast::Toast::error(
+                                    format!("Failed to generate script: {}", error),
+                                    &app.theme,
+                                )
+                                .duration_ms(Some(7000)),
+                            );
+                            cx.notify();
+                        }
+                    }
+                })
+            });
+        })
+        .detach();
+    }
+
     /// Rebuild the cached provider registry in a background thread.
     /// Called after config changes (API key saved, Claude Code enabled, etc.)
     pub fn rebuild_provider_registry_async(&mut self, cx: &mut Context<Self>) {
@@ -312,3 +641,44 @@
         .detach();
     }
 }
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn test_build_ai_script_generation_user_prompt_includes_description() {
+        let prompt = build_ai_script_generation_user_prompt("create a weather script");
+        assert!(prompt.contains("create a weather script"));
+        assert!(prompt.contains("Script Kit script"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_prefers_typescript_fence() {
+        let response = r#"
+Here's your script:
+```typescript
+// Name: Weather
+import "@scriptkit/sdk";
+```
+"#;
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert!(extracted.contains("// Name: Weather"));
+        assert!(!extracted.contains("```"));
+    }
+
+    #[test]
+    fn test_extract_generated_script_source_falls_back_to_plain_text() {
+        let response = "// Name: Plain\nimport \"@scriptkit/sdk\";";
+        let extracted =
+            extract_generated_script_source(response).expect("script should be extracted");
+        assert_eq!(extracted, "// Name: Plain\nimport \"@scriptkit/sdk\";\n");
+    }
+
+    #[test]
+    fn test_derive_script_name_from_description_uses_fallback_for_symbols() {
+        let name = derive_script_name_from_description("@@@ !!!");
+        assert_eq!(name, "ai-generated-script");
+    }
+}
diff --git a/src/app_impl/startup.rs b/src/app_impl/startup.rs
index 2e95070751083b8f4c24cd7eae8372d6bec3f6d6..441b4f23bca1b6294025c698eeb11d45815e85e0
--- a/src/app_impl/startup.rs
+++ b/src/app_impl/startup.rs
@@ -555,9 +555,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -720,20 +722,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/startup_new_tab.rs b/src/app_impl/startup_new_tab.rs
index 6c807a195553d9f168087c2ca6d771c76e555c11..7c84270cbe91c0b1ddd460aa29db8c788766d0cf
--- a/src/app_impl/startup_new_tab.rs
+++ b/src/app_impl/startup_new_tab.rs
@@ -6,9 +6,11 @@
             let app_entity = app_entity_for_tab;
             move |event, window, cx| {
                 let key = event.keystroke.key.to_lowercase();
+                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
+                    || matches!(key.as_str(), "tab");
                 let has_shift = event.keystroke.modifiers.shift;
                 // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
-                if key == "tab"
+                if is_tab_key
                     && !event.keystroke.modifiers.platform
                     && !event.keystroke.modifiers.alt
                     && !event.keystroke.modifiers.control
@@ -171,20 +173,23 @@
                                 }
                             }
 
-                            // Handle Tab in ScriptList view for Ask AI feature
-                            // Shows inline ChatPrompt with built-in AI (prefers Vercel AI Gateway)
+                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
+                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
-                                && !has_shift
                             {
                                 let query = this.filter_text.clone();
 
                                 // Clear filter text before switching view
                                 this.filter_text.clear();
 
-                                // Show inline AI chat with the query as initial input
-                                this.show_inline_ai_chat(Some(query), cx);
+                                if has_shift {
+                                    this.generate_script_from_ai_prompt(query, cx);
+                                } else {
+                                    // Show inline AI chat with the query as initial input
+                                    this.show_inline_ai_chat(Some(query), cx);
+                                }
 
                                 // Stop propagation so Input doesn't handle it
                                 cx.stop_propagation();
diff --git a/src/app_impl/tests.rs b/src/app_impl/tests.rs
index d75ba121851d7233c5ae77bc474a64b27e7c31d2..3f0fbd1ccbc2f1e3d515ac1edf9666035f09eaf4
--- a/src/app_impl/tests.rs
+++ b/src/app_impl/tests.rs
@@ -1,6 +1,7 @@
 #[cfg(test)]
 mod app_impl_state_sync_tests {
     use super::{calculate_fallback_error_message, ScriptListApp};
+    use std::fs;
 
     #[test]
     fn test_sync_builtin_query_state_updates_query_and_selection_when_changed() {
@@ -46,4 +47,39 @@
         assert!(message.contains("Could not evaluate expression"));
         assert!(message.contains("Check the syntax and try again"));
     }
+
+    #[test]
+    fn test_shift_tab_routes_to_ai_script_generation_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("if has_shift")
+                && startup_tab.contains("this.generate_script_from_ai_prompt(query, cx);"),
+            "Shift+Tab in ScriptList should route to generate_script_from_ai_prompt. \
+             Missing expected branch in startup_new_tab.rs"
+        );
+    }
+
+    #[test]
+    fn test_tab_still_routes_to_inline_ai_chat_in_script_list_tab_interceptor() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("this.show_inline_ai_chat(Some(query), cx);"),
+            "Tab in ScriptList should continue routing to show_inline_ai_chat"
+        );
+    }
+
+    #[test]
+    fn test_tab_interceptor_matches_both_tab_key_variants() {
+        let startup_tab = fs::read_to_string("src/app_impl/startup_new_tab.rs")
+            .expect("Failed to read src/app_impl/startup_new_tab.rs");
+
+        assert!(
+            startup_tab.contains("matches!(event.keystroke.key.as_str(), \"tab\" | \"Tab\")"),
+            "Tab interceptor should match both tab key variants: \"tab\" and \"Tab\""
+        );
+    }
 }

tokens used
292,377
Implemented `Shift+Tab` script-generation flow in parallel with existing `Tab` chat flow.

Changed files:
- `src/app_impl/startup.rs:558`  
  - Tab key matcher now explicitly supports both variants (`"tab" | "Tab"`).
  - In ScriptList, `Shift+Tab` now routes to `generate_script_from_ai_prompt(...)`; plain `Tab` still routes to `show_inline_ai_chat(...)`.
- `src/app_impl/startup_new_tab.rs:9`  
  - Mirrored the same Tab/Shift+Tab interceptor behavior in the new-tab startup path.
- `src/app_impl/prompt_ai.rs:29`  
  - Added AI script-generation prompt/instructions for Script Kit best practices.
  - Added generation pipeline helpers (model selection, fenced-code extraction, script-name derivation, backend call).
  - Added `generate_script_from_ai_prompt(...)` (`src/app_impl/prompt_ai.rs:469`) to:
    - use current input as prompt,
    - call AI provider backend,
    - create/write a new script file,
    - open it in the editor,
    - emit structured logs/toasts.
- `src/app_impl/tests.rs:52`  
  - Added tests asserting:
    - Shift+Tab routes to script generation,
    - Tab still routes to inline AI chat,
    - interceptor matches both key variants (`"tab" | "Tab"`).

Verification run:
- Attempted scoped tests:
  - `cargo test --lib app_impl::prompt_ai::tests::test_build_ai_script_generation_user_prompt_includes_description`
  - `cargo test --lib app_impl::tests::app_impl_state_sync_tests::test_shift_tab_routes_to_ai_script_generation_in_script_list_tab_interceptor`
- Result: blocked by unrelated shared-tree compile failures (duplicate module files and unrelated `src/actions/tests/**` errors).
- Patch hygiene check passed:
  - `git diff --check -- src/app_impl/startup.rs src/app_impl/startup_new_tab.rs src/app_impl/prompt_ai.rs src/app_impl/tests.rs`

How to test:
1. Run app with logs:
   - `echo '{"type":"show"}' | SCRIPT_KIT_AI_LOG=1 ./target/debug/script-kit-gpui 2>&1`
2. In main ScriptList input, type a script request.
3. Press `Shift+Tab`.
4. Verify `AI_SCRIPT_GEN` logs appear, a new script is created, and editor opens it.
5. Press `Tab` with input text and verify inline AI chat still opens.

Risks / known gaps:
- Automated Rust verification for this change is currently blocked by unrelated compile issues in the shared tree.
- Input text is cleared before generation starts; on failure, the prompt is not restored automatically.

Commits:
- None made.


==== exit @ 2026-02-08T02:55:41.624Z code=0 status=done ====

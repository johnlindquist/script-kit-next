==== spawn @ 2026-02-08T03:26:51.445Z ====
cmd: codex "exec" "--cd" "/Users/johnlindquist/dev/script-kit-gpui" "--model" "gpt-5.3-codex" "-c" "model_reasoning_effort=\"xhigh\"" "-c" "model_auto_compact_token_limit=120000" "-c" "developer_instructions=\"You are Codex implementer agent \\\"codex-chat-prompt-script-gen\\\".\\n\\n## Coordination\\nThis repo runs parallel agents in ONE working tree. Before editing any file, you MUST claim it.\\nCommands (run via shell):\\n  - View roster: ~/.claude/bin/swarm.ts status\\n  - Claim files (blocks until free): ~/.claude/bin/swarm.ts claim --wait --id codex-chat-prompt-script-gen path/to/file\\n  - Release: ~/.claude/bin/swarm.ts release --id codex-chat-prompt-script-gen path/to/file\\n  - Update status/notes: ~/.claude/bin/swarm.ts update --id codex-chat-prompt-script-gen --status in_progress --notes \\\"...\\\"\\nRules:\\n  - Never edit a file you haven't claimed.\\n  - Keep claims tight (claim only what you're actively changing).\\n  - If blocked, set status=blocked and say what you're waiting on.\\n  - When finished, set status=done and release claims.\\n\\n## Development practices (this code is maintained by AI agents)\\n  - Log state transitions with enough context to diagnose failures from logs alone.\\n  - Structured errors: include what was attempted, what failed, and current state.\\n  - Name things for grepability — unique identifiers agents can find on the first search.\\n  - Every behavior change gets a test. Untested code is invisible to the next agent.\\n  - Test names describe the scenario: test_X_does_Y_when_Z.\\n  - Keep functions small and single-purpose. 500-line functions burn agent context.\\n  - Use types to encode constraints (enums > strings for errors).\\n\\n## CRITICAL: Parallel-safe verification\\n  - Other agents are modifying other files RIGHT NOW. Full test suites WILL show their failures.\\n  - ONLY run tests that cover YOUR changed files. Scope every test/check command:\\n    Rust: cargo test -p your-crate -- module::test_name | JS: npx jest your/file.test.ts\\n  - Do NOT run: cargo test (unscoped), cargo check (whole workspace), npm test (unscoped).\\n  - If a scoped test fails, it's YOUR bug — fix it. If tests outside your scope fail, ignore them.\\n\\n## Git commit discipline\\n  - NEVER commit unverified work. Run tests/lints/type-checks FIRST. If it fails, fix it before committing.\\n  - A commit is a declaration: \\\"this works and here's the proof.\\\"\\n  - Commit FREQUENTLY — after each meaningful unit of VERIFIED work, not just at the end.\\n  - A \\\"unit\\\" = one logical change that passes verification (new function, bug fix, refactor, test added).\\n  - NEVER batch an entire task into one giant commit. Small commits are searchable; big ones aren't.\\n  - Commit message format:\\n      Line 1: <type>(<scope>): <what changed> (imperative, max 72 chars)\\n      Line 3+: WHY this change was made, WHAT was verified, and HOW to test it.\\n    Types: feat, fix, refactor, test, docs, chore\\n    Example:\\n      feat(auth): add refresh token rotation\\n      \\n      Tokens now rotate on each refresh call to prevent replay attacks.\\n      The old token is invalidated immediately on rotation.\\n      \\n      Verified: cargo test --lib -- auth::refresh (4 tests pass)\\n      Verified: manual smoke test via curl — token rotates, old token returns 401\\n  - The \\\"Verified:\\\" lines are REQUIRED. Future agents will read git log to understand\\n    what was tested and how to re-verify. This is the most valuable part of the message.\\n  - In your final message, list all commits you made with their hashes.\\n\\n## Time budget\\nYou have approximately 10 minutes. If your task is too broad to complete in time:\\n(1) Commit any verified progress.\\n(2) Run: ~/.claude/bin/swarm.ts update --id YOUR_ID --status needs_split --notes 'suggest: 1) sub-task-desc scope:files 2) sub-task-desc scope:files'.\\n(3) Exit cleanly. The dispatcher will read your suggestions and spawn narrower workers.\"" "--output-last-message" "/Users/johnlindquist/dev/script-kit-gpui/.ai/logs/codex-chat-prompt-script-gen.final.md" "--yolo" "You are agent codex-chat-prompt-script-gen.\n\nCurrent parallel-agent roster (read this first):\n```\nSWARM SNAPSHOT @ 2026-02-08T03:26:51.437Z\n- codex-chat-prompt-script-gen [starting] scope=src/app_impl/prompt_ai.rs,src/app_impl/startup.rs,src/app_impl/startup_new_tab.rs,src/prompts/chat/**,src/ai/script_generation.rs task=Refactor the Shift+Tab AI script generation flow. Currently Shift+Tab in the main prompt calls generate_script_from_ai_…\n- codex-wire-remaining-v3 [in_progress] scope=src/emoji/**,src/quicklinks/**,src/builtins/part_000.rs,src/builtins/part_001_entries/**,src/builtins/tests/**,src/app_execute/builtin_execution.rs,src/main.rs,src/lib.rs task=Previous workers wired Favorites and Clipboard Exclusions into the app. They also partially wired EmojiPicker and Quick…\n  notes: spawned codex (exec)\n```\n\nYour assignment:\n- Task: Refactor the Shift+Tab AI script generation flow. Currently Shift+Tab in the main prompt calls generate_script_from_ai_prompt() which silently generates a script in the background. Instead, Shift+Tab should open the ChatPrompt UI (similar to how plain Tab opens show_inline_ai_chat) but with a special 'script generation' mode. The flow should be: 1) User types a description, presses Shift+Tab 2) The ChatPrompt opens and streams the AI response (the generated script) visibly to the user 3) After generation completes, the chat UI shows action buttons: 'Save' (saves to ~/.kenv/scripts/), 'Run' (saves and executes), and 'Save and Run' (same as Run). Look at how show_inline_ai_chat works in src/app_impl/prompt_ai.rs - it creates a ChatPrompt with with_builtin_ai() and with_pending_submit(). The script generation system prompt is in src/ai/script_generation.rs (AI_SCRIPT_GENERATION_SYSTEM_PROMPT). Modify the Shift+Tab path in startup.rs/startup_new_tab.rs to call a new method like show_script_generation_chat() that opens a ChatPrompt configured with the script generation system prompt. The ChatPrompt needs save/run/save-and-run buttons added to its UI after the AI finishes responding. The save logic should use the existing script_creation module and the slug/convention logic from ai::script_generation.\n- Scope: src/app_impl/prompt_ai.rs,src/app_impl/startup.rs,src/app_impl/startup_new_tab.rs,src/prompts/chat/**,src/ai/script_generation.rs\n\nOperating constraints:\n- Single working tree (no worktrees, no patches).\n- You MUST claim files before editing (~/.claude/bin/swarm.ts claim --wait ...).\n\nDeliverables:\n- Make the code changes required by the task.\n- Run the most relevant tests/linters.\n- In your FINAL message, include: changed files, how to test, and any risks/known gaps."

OpenAI Codex v0.98.0 (research preview)
--------
workdir: /Users/johnlindquist/dev/script-kit-gpui
model: gpt-5.3-codex
provider: openai
approval: never
sandbox: danger-full-access
reasoning effort: xhigh
reasoning summaries: auto
session id: 019c3b49-b28f-7483-8818-0d80ae9cf077
--------
user
You are agent codex-chat-prompt-script-gen.

Current parallel-agent roster (read this first):
```
SWARM SNAPSHOT @ 2026-02-08T03:26:51.437Z
- codex-chat-prompt-script-gen [starting] scope=src/app_impl/prompt_ai.rs,src/app_impl/startup.rs,src/app_impl/startup_new_tab.rs,src/prompts/chat/**,src/ai/script_generation.rs task=Refactor the Shift+Tab AI script generation flow. Currently Shift+Tab in the main prompt calls generate_script_from_ai_…
- codex-wire-remaining-v3 [in_progress] scope=src/emoji/**,src/quicklinks/**,src/builtins/part_000.rs,src/builtins/part_001_entries/**,src/builtins/tests/**,src/app_execute/builtin_execution.rs,src/main.rs,src/lib.rs task=Previous workers wired Favorites and Clipboard Exclusions into the app. They also partially wired EmojiPicker and Quick…
  notes: spawned codex (exec)
```

Your assignment:
- Task: Refactor the Shift+Tab AI script generation flow. Currently Shift+Tab in the main prompt calls generate_script_from_ai_prompt() which silently generates a script in the background. Instead, Shift+Tab should open the ChatPrompt UI (similar to how plain Tab opens show_inline_ai_chat) but with a special 'script generation' mode. The flow should be: 1) User types a description, presses Shift+Tab 2) The ChatPrompt opens and streams the AI response (the generated script) visibly to the user 3) After generation completes, the chat UI shows action buttons: 'Save' (saves to ~/.kenv/scripts/), 'Run' (saves and executes), and 'Save and Run' (same as Run). Look at how show_inline_ai_chat works in src/app_impl/prompt_ai.rs - it creates a ChatPrompt with with_builtin_ai() and with_pending_submit(). The script generation system prompt is in src/ai/script_generation.rs (AI_SCRIPT_GENERATION_SYSTEM_PROMPT). Modify the Shift+Tab path in startup.rs/startup_new_tab.rs to call a new method like show_script_generation_chat() that opens a ChatPrompt configured with the script generation system prompt. The ChatPrompt needs save/run/save-and-run buttons added to its UI after the AI finishes responding. The save logic should use the existing script_creation module and the slug/convention logic from ai::script_generation.
- Scope: src/app_impl/prompt_ai.rs,src/app_impl/startup.rs,src/app_impl/startup_new_tab.rs,src/prompts/chat/**,src/ai/script_generation.rs

Operating constraints:
- Single working tree (no worktrees, no patches).
- You MUST claim files before editing (~/.claude/bin/swarm.ts claim --wait ...).

Deliverables:
- Make the code changes required by the task.
- Run the most relevant tests/linters.
- In your FINAL message, include: changed files, how to test, and any risks/known gaps.
warning: Under-development features enabled: responses_websockets. Under-development features are incomplete and may behave unpredictably. To suppress this warning, set `suppress_unstable_features_warning = true` in /Users/johnlindquist/.codex/config.toml.
mcp: deepwiki starting
mcp: deepwiki ready
mcp startup: ready: deepwiki

thinking
**Confirming task approach**
codex
I’ll implement the Shift+Tab refactor so it opens `ChatPrompt` in a script-generation mode, then wire post-generation `Save`/`Run` actions using the existing script creation flow. I’m starting by syncing swarm status and reading the current `prompt_ai`, startup key handling, and chat prompt internals before editing.

thinking
**Planning parallel command execution**
exec
/bin/zsh -lc '~/.claude/bin/swarm.ts status && ~/.claude/bin/swarm.ts update --id codex-chat-prompt-script-gen --status in_progress --notes "Inspecting current Shift+Tab and ChatPrompt flow for script-generation mode"' in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
Repo: /Users/johnlindquist/dev/script-kit-gpui
Updated: 2026-02-08T03:27:03.851Z

Active agents (2):
codex-chat-prompt-script-gen [in_progress] codex/gpt-5.3-codex/exec
  task: Refactor the Shift+Tab AI script generation flow. Currently Shift+Tab in the main prompt calls generate_script_from_ai_…
  scope: src/app_impl/prompt_ai.rs,src/app_impl/startup.rs,src/app_impl/startup_new_tab.rs,src/prompts/chat/**,src/ai/script_gen…
  claims: (none)
  notes: spawned codex (exec)

codex-wire-remaining-v3 [in_progress] codex/gpt-5.3-codex/exec
  task: Previous workers wired Favorites and Clipboard Exclusions into the app. They also partially wired EmojiPicker and Quick…
  scope: src/emoji/**,src/quicklinks/**,src/builtins/part_000.rs,src/builtins/part_001_entries/**,src/builtins/tests/**,src/app_…
  claims: (none)
  notes: spawned codex (exec)

Inactive agents (262):
- codex-accessibility [done] done. exit=0. final=.ai/logs/codex-accessibility.final.md. See final message file.
- codex-action-labels [failed] failed. exit=1. final=.ai/logs/codex-action-labels.final.md.
- codex-actions-audit [failed] failed. exit=1. final=.ai/logs/codex-actions-audit.final.md. See final message file.

thinking
**Planning selective claim file reads**
codex
I’ve set this worker to `in_progress`; next I’m mapping the current key handlers and chat prompt lifecycle to isolate where Shift+Tab should branch into script-generation chat mode. I’ll read the relevant files now and then claim only the files that need edits.

thinking
**Planning parallel read strategy**
exec
/bin/zsh -lc "rg -n \"show_inline_ai_chat|generate_script_from_ai_prompt|Shift\\+Tab|shift\\+tab|show_script_generation|pending_submit|with_builtin_ai|with_pending_submit\" src/app_impl/prompt_ai.rs src/app_impl/startup.rs src/app_impl/startup_new_tab.rs src/prompts/chat src/ai/script_generation.rs" in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc "sed -n '1,280p' src/app_impl/startup.rs" in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc "sed -n '1,260p' src/app_impl/prompt_ai.rs" in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc "sed -n '1,320p' src/app_impl/startup_new_tab.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
src/app_impl/prompt_ai.rs:348:    pub fn show_inline_ai_chat(&mut self, initial_query: Option<String>, cx: &mut Context<Self>) {
src/app_impl/prompt_ai.rs:448:        .with_builtin_ai(registry, true); // true = prefer Vercel AI Gateway
src/app_impl/prompt_ai.rs:453:            chat_prompt = chat_prompt.with_pending_submit(true);
src/app_impl/prompt_ai.rs:469:    pub fn generate_script_from_ai_prompt(
src/app_impl/startup_new_tab.rs:31:                            // Handle Tab/Shift+Tab in FileSearchView for directory/file navigation
src/app_impl/startup_new_tab.rs:32:                            // CRITICAL: ALWAYS consume Tab/Shift+Tab to prevent focus traversal
src/app_impl/startup_new_tab.rs:38:                                // ALWAYS stop propagation for Tab/Shift+Tab in FileSearchView
src/app_impl/startup_new_tab.rs:43:                                    // Shift+Tab: Go up one directory level using parent_dir_display helper
src/app_impl/startup_new_tab.rs:51:                                                "Shift+Tab: Navigating up from '{}' to '{}'",
src/app_impl/startup_new_tab.rs:76:                                                "Shift+Tab: Already at root '{}', no-op",
src/app_impl/startup_new_tab.rs:164:                            // Handle Tab/Shift+Tab in ChatPrompt setup mode
src/app_impl/startup_new_tab.rs:176:                            // Handle Tab/Shift+Tab in ScriptList view for AI actions.
src/app_impl/startup_new_tab.rs:177:                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
src/app_impl/startup_new_tab.rs:188:                                    this.generate_script_from_ai_prompt(query, cx);
src/app_impl/startup_new_tab.rs:191:                                    this.show_inline_ai_chat(Some(query), cx);
src/app_impl/startup.rs:580:                            // Handle Tab/Shift+Tab in FileSearchView for directory/file navigation
src/app_impl/startup.rs:581:                            // CRITICAL: ALWAYS consume Tab/Shift+Tab to prevent focus traversal
src/app_impl/startup.rs:587:                                // ALWAYS stop propagation for Tab/Shift+Tab in FileSearchView
src/app_impl/startup.rs:592:                                    // Shift+Tab: Go up one directory level using parent_dir_display helper
src/app_impl/startup.rs:600:                                                "Shift+Tab: Navigating up from '{}' to '{}'",
 succeeded in 51ms:
use super::*;

pub(super) fn calculate_fallback_error_message(expression: &str) -> String {
    format!(
        "Could not evaluate expression \"{}\". Check the syntax and try again.",
        expression
    )
}

impl ScriptListApp {
    pub(crate) fn new(
        config: config::Config,
        bun_available: bool,
        window: &mut Window,
        cx: &mut Context<Self>,
    ) -> Self {
        // PERF: Parallelize script + scriptlet loading to reduce startup wall time.
        let load_start = std::time::Instant::now();
        let (scripts, scriptlets, scripts_elapsed, scriptlets_elapsed) = std::thread::scope(
            |scope| {
 succeeded in 51ms:
        // Add Tab key interceptor for "Ask AI" feature and file search directory navigation
        // This fires BEFORE normal key handling, allowing us to intercept Tab
        // even when the Input component has focus
        let app_entity_for_tab = cx.entity().downgrade();
        let tab_interceptor = cx.intercept_keystrokes({
            let app_entity = app_entity_for_tab;
            move |event, window, cx| {
                let key = event.keystroke.key.to_lowercase();
                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
                    || matches!(key.as_str(), "tab");
                let has_shift = event.keystroke.modifiers.shift;
                // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
                if is_tab_key
                    && !event.keystroke.modifiers.platform
                    && !event.keystroke.modifiers.alt
                    && !event.keystroke.modifiers.control
                {
                    if let Some(app) = app_entity.upgrade() {
                        app.update(cx, |this, cx| {
                            // FIRST: If confirm dialog is open, route Tab to it for button switching
 succeeded in 51ms:
use super::*;
use anyhow::{anyhow, Context as AnyhowContext, Result};

#[derive(Debug, Clone, Copy)]
enum AiScriptGenerationStage {
    SelectModel,
    ResolveProvider,
    RequestCompletion,
    ExtractScript,
    CreateScriptFile,
    WriteScriptFile,
    OpenEditor,
}

impl AiScriptGenerationStage {
    fn as_str(self) -> &'static str {
        match self {
            Self::SelectModel => "select_model",
            Self::ResolveProvider => "resolve_provider",
            Self::RequestCompletion => "request_completion",

thinking
**Inspecting chat prompt files**
codex
I’ve confirmed Shift+Tab currently routes to `generate_script_from_ai_prompt` in both startup paths. I’m now inspecting `ChatPrompt` internals to add a dedicated mode and post-response action buttons without affecting normal Ask-AI behavior.

thinking

exec
/bin/zsh -lc 'ls -R src/prompts/chat' in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc "sed -n '1,420p' src/prompts/chat/prompt.rs" in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc "sed -n '1,280p' src/prompts/chat/render_core.rs" in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc "sed -n '1,320p' src/prompts/chat/layout.rs" in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc "sed -n '1,340p' src/prompts/chat/provider_streaming.rs" in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc "sed -n '1,320p' src/prompts/chat/render_response.rs" in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc "sed -n '1,320p' src/prompts/chat/render_setup.rs" in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc "sed -n '1,320p' src/prompts/chat/events.rs" in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc "sed -n '1,320p' src/prompts/chat/state_machine.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 50ms:
actions.rs
mod.rs
prompt.rs
render_core.rs
render_input.rs
render_setup.rs
render_turns.rs
state.rs
streaming.rs
tests.rs
types.rs
 succeeded in 50ms:
use super::*;

impl ChatPrompt {
    fn render_footer(&self, _cx: &mut Context<Self>) -> impl IntoElement {
        // Use standard PromptFooter colors from theme
        let footer_colors = PromptFooterColors::from_theme(&self.theme);

        // Build model display text (show model name if available)
        let model_text = self.model.clone().unwrap_or_else(|| "Select Model".into());

        // Configure footer with chat-specific labels
        let footer_config = PromptFooterConfig::new()
            .primary_label("Continue in Chat")
            .primary_shortcut("⌘↵")
            .secondary_label("Actions")
            .secondary_shortcut("⌘K")
            .show_logo(true)
            .show_secondary(true)
            .helper_text(model_text) // Show model name next to logo
            .info_label("Shift+Enter newline");
 succeeded in 51ms:
use super::*;
pub struct ChatPrompt {
    pub id: String,
    pub messages: Vec<ChatPromptMessage>,
    pub placeholder: Option<String>,
    pub hint: Option<String>,
    pub footer: Option<String>,
    pub model: Option<String>,
    pub models: Vec<ChatModel>,
    pub title: Option<String>,
    pub focus_handle: FocusHandle,
    pub input: TextInputState,
    pub on_submit: ChatSubmitCallback,
    pub on_escape: Option<ChatEscapeCallback>,
    pub on_continue: Option<ChatContinueCallback>,
    pub on_retry: Option<ChatRetryCallback>,
    pub theme: Arc<theme::Theme>,
    pub turns_list_state: ListState,
    pub(super) prompt_colors: theme::PromptColors,
    pub(super) conversation_turns_cache: Arc<Vec<ConversationTurn>>,
 exited 1 in 50ms:
sed: src/prompts/chat/layout.rs: No such file or directory
 exited 1 in 50ms:
sed: src/prompts/chat/provider_streaming.rs: No such file or directory
 succeeded in 50ms:
use super::*;

impl ChatPrompt {
    pub(super) fn render_setup_card(&self, cx: &Context<Self>) -> impl IntoElement {
        let colors = &self.prompt_colors;

        let accent_full = rgba((colors.accent_color << 8) | 0xFF);
        let accent_25 = rgba((colors.accent_color << 8) | 0x40);
        let muted_bg = rgba((colors.code_bg << 8) | 0x60);
        let muted_bg_hover = rgba((colors.code_bg << 8) | 0x90);
        let ring_color = rgba((colors.accent_color << 8) | 0x80);
        let kbd_bg = rgba((colors.code_bg << 8) | 0x50);
        let accent_text = rgb(self.theme.colors.text.on_accent);

        let on_configure = self.on_configure.clone();
        let on_claude_code = self.on_claude_code.clone();
        let is_configure_focused = self.setup_focus_index == 0;
        let is_claude_focused = self.setup_focus_index == 1;

        div()
 exited 1 in 50ms:
sed: src/prompts/chat/render_response.rs: No such file or directory
 exited 1 in 51ms:
sed: src/prompts/chat/events.rs: No such file or directory
 exited 1 in 51ms:
sed: src/prompts/chat/state_machine.rs: No such file or directory

thinking
**Inspecting existing action files**
exec
/bin/zsh -lc "sed -n '1,360p' src/prompts/chat/render_input.rs" in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc "sed -n '1,360p' src/prompts/chat/actions.rs" in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc "sed -n '1,360p' src/prompts/chat/state.rs" in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc "sed -n '1,420p' src/prompts/chat/render_turns.rs" in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc "sed -n '1,360p' src/prompts/chat/tests.rs" in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc "sed -n '1,280p' src/prompts/chat/mod.rs" in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc "sed -n '1,420p' src/prompts/chat/streaming.rs" in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc "sed -n '1,380p' src/prompts/chat/types.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
use super::*;

impl ChatPrompt {
    pub(super) fn handle_escape(&mut self, _cx: &mut Context<Self>) {
        logging::log("CHAT", "Escape pressed - closing chat");

        // Save conversation to database if save_history is enabled
        if self.save_history {
            self.save_to_database();
        }

        if let Some(ref callback) = self.on_escape {
            callback(self.id.clone());
        }
    }

    /// Save the current conversation to the AI chats database
    pub(super) fn save_to_database(&self) {
        // Only save if we have messages
        if self.messages.is_empty() {
 succeeded in 51ms:
use super::*;

impl ChatPrompt {
    pub(super) fn render_input(&self, is_focused: bool) -> impl IntoElement {
        let colors = &self.prompt_colors;
        let theme_colors = &self.theme.colors;
        let text = self.input.text();
        let cursor_pos = self.input.cursor();
        let chars: Vec<char> = text.chars().collect();
        let cursor_visible = self.cursor_visible && is_focused;

        let mut input_content = div()
            .flex()
            .flex_row()
            .items_center()
            .w_full()
            .text_size(px(14.0));

        // Text before cursor
        if cursor_pos > 0 {
 succeeded in 51ms:
use super::*;

impl ChatPrompt {
    pub(super) fn mark_conversation_turns_dirty(&mut self) {
        self.conversation_turns_dirty = true;
    }

    pub(super) fn sync_turns_list_state(&mut self) {
        let item_count = self.conversation_turns_cache.len();
        let old_count = self.turns_list_state.item_count();
        if old_count != item_count {
            self.turns_list_state.splice(0..old_count, item_count);
        }
    }

    pub(super) fn ensure_conversation_turns_cache(&mut self) {
        if !self.conversation_turns_dirty {
            return;
        }
        self.conversation_turns_cache = Arc::new(build_conversation_turns(
 succeeded in 51ms:
use super::*;

impl ChatPrompt {
    pub(super) fn render_turn(
        &self,
        turn: &ConversationTurn,
        turn_index: usize,
        cx: &Context<Self>,
    ) -> impl IntoElement {
        let colors = &self.prompt_colors;

        // VIBRANCY: Use theme-aware overlay for subtle lift that lets blur show through
        // Dark mode: white overlay brightens; Light mode: much subtler black overlay
        let container_bg = if self.theme.is_dark_mode() {
            theme::hover_overlay_bg(&self.theme, 0x15) // ~8% white overlay for dark mode
        } else {
            theme::hover_overlay_bg(&self.theme, 0x08) // ~3% black overlay for light mode
        };
        let copy_hover_bg = theme::hover_overlay_bg(&self.theme, 0x28); // ~16% for hover
        let error_color = self.theme.colors.ui.error;
 succeeded in 50ms:
use super::*;
#[cfg(test)]
#[allow(clippy::module_inception)]
mod tests {
    use std::collections::HashMap;

    use crate::protocol::ChatPromptMessage;

    use super::{
        next_chat_scroll_follow_state, next_reveal_boundary, resolve_chat_input_key_action,
        resolve_setup_card_key, should_ignore_stream_reveal_update, ChatInputKeyAction,
        ChatScrollDirection, SetupCardAction,
    };

    #[test]
    fn resolve_setup_card_key_cycles_focus_for_tab_and_arrows() {
        assert_eq!(
            resolve_setup_card_key("tab", false, 0),
            (1, SetupCardAction::None, true)
        );
 succeeded in 50ms:
//! ChatPrompt - Raycast-style chat interface
//!
//! Features:
//! - Input at TOP (not bottom)
//! - Messages bundled as conversation turns (user prompt + AI response in same container)
//! - Full-width containers (not bubbles)
//! - Footer with model selector and "Continue in Chat"
//! - Actions menu (⌘+K) with model picker

use crate::components::prompt_footer::{PromptFooter, PromptFooterColors, PromptFooterConfig};
use crate::components::TextInputState;
use crate::designs::icon_variations::IconName;
use gpui::{
    div, img, list, prelude::*, px, rgb, rgba, svg, App, Context, ExternalPaths, FocusHandle,
    Focusable, Hsla, KeyDownEvent, ListAlignment, ListSizingBehavior, ListState, Render,
    RenderImage, ScrollWheelEvent, Timer, Window,
};
use gpui_component::{scroll::ScrollableElement, theme::ActiveTheme};
use std::collections::HashMap;
use std::sync::atomic::{AtomicBool, Ordering};
 succeeded in 50ms:
use super::*;

impl ChatPrompt {
    pub(super) fn handle_submit(&mut self, cx: &mut Context<Self>) {
        let text = self.input.text().to_string();
        let pending_image = self.pending_image.take();
        let pending_render = self.pending_image_render.take();

        if text.trim().is_empty() && pending_image.is_none() {
            return;
        }
        logging::log("CHAT", &format!("User submitted: {}", text));
        self.input.clear();

        // If built-in AI mode is enabled, handle the AI call directly
        if self.has_builtin_ai() {
            // Cache the render image for conversation history display
            // We need the user message ID, which will be generated in handle_builtin_ai_submit
            self.handle_builtin_ai_submit(text, pending_image, pending_render, cx);
        } else {
 succeeded in 50ms:
use super::*;
/// Available AI models for the chat
#[derive(Clone, Debug, PartialEq)]
pub struct ChatModel {
    pub id: String,
    pub name: String,
    pub provider: String,
}

impl ChatModel {
    pub fn new(
        id: impl Into<String>,
        name: impl Into<String>,
        provider: impl Into<String>,
    ) -> Self {
        Self {
            id: id.into(),
            name: name.into(),
            provider: provider.into(),
        }

thinking
**Inspecting AI prompt methods**
exec
/bin/zsh -lc "sed -n '300,620p' src/app_impl/prompt_ai.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
    pub(crate) fn create_box_shadows(&self) -> Vec<BoxShadow> {
        let shadow_config = self.theme.get_drop_shadow();

        if !shadow_config.enabled {
            return vec![];
        }

        // Convert hex color to HSLA
        // For black (0x000000), we use h=0, s=0, l=0
        let r = ((shadow_config.color >> 16) & 0xFF) as f32 / 255.0;
        let g = ((shadow_config.color >> 8) & 0xFF) as f32 / 255.0;
        let b = (shadow_config.color & 0xFF) as f32 / 255.0;

        // Simple RGB to HSL conversion for shadow color
        let max = r.max(g).max(b);
        let min = r.min(g).min(b);
        let l = (max + min) / 2.0;

        let (h, s) = if max == min {
            (0.0, 0.0) // achromatic

thinking
**Inspecting script_generation module**
exec
/bin/zsh -lc "sed -n '1,360p' src/ai/script_generation.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 52ms:
use anyhow::{Context, Result};
use std::fs::{self, OpenOptions};
use std::io::{ErrorKind, Write};
use std::path::{Path, PathBuf};
use std::sync::Arc;

use super::config::ModelInfo;
use super::providers::{AiProvider, ProviderMessage, ProviderRegistry};

const AI_SCRIPT_OUTPUT_DIR: &str = "~/.kenv/scripts";
const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;

const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.

Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.

Required output conventions:
1) Include metadata comments at the top:
   // Name: <clear title>

thinking
**Evaluating code reuse options**
exec
/bin/zsh -lc "sed -n '360,760p' src/ai/script_generation.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 50ms:
```
"#;
        assert_eq!(extract_script_code(response), "await div(\"hello\");");
    }

    #[test]
    fn test_extract_script_code_falls_back_to_first_fenced_block() {
        let response = r#"
```python
print("hello")
```
"#;
        assert_eq!(extract_script_code(response), "print(\"hello\")");
    }

    #[test]
    fn test_extract_script_code_returns_trimmed_response_when_no_fence_exists() {
        let response = "const answer = 42;";
        assert_eq!(extract_script_code(response), "const answer = 42;");
    }

thinking
**Inspecting key handling area**
exec
/bin/zsh -lc "sed -n '660,780p' src/app_impl/startup.rs && sed -n '120,230p' src/app_impl/startup_new_tab.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
                                        if file.file_type == crate::file_search::FileType::Directory
                                        {
                                            // Directory: Enter it (append /)
                                            let shortened =
                                                crate::file_search::shorten_path(&file.path);
                                            let new_path = format!("{}/", shortened);
                                            crate::logging::log(
                                                "KEY",
                                                &format!("Tab: Entering directory: {}", new_path),
                                            );

                                            // Update the input - handle_filter_input_change handles the rest
                                            this.gpui_input_state.update(cx, |state, cx| {
                                                state.set_value(new_path.clone(), window, cx);
                                                // Ensure cursor is at end with no selection after programmatic set_value
                                                let len = new_path.len();
                                                state.set_selection(len, len, window, cx);
                                            });

                                            cx.notify();

thinking
**Inspecting possible duplicated code segment**
exec
/bin/zsh -lc "sed -n '1,320p' src/app_impl/startup_new_tab.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
        // Add Tab key interceptor for "Ask AI" feature and file search directory navigation
        // This fires BEFORE normal key handling, allowing us to intercept Tab
        // even when the Input component has focus
        let app_entity_for_tab = cx.entity().downgrade();
        let tab_interceptor = cx.intercept_keystrokes({
            let app_entity = app_entity_for_tab;
            move |event, window, cx| {
                let key = event.keystroke.key.to_lowercase();
                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
                    || matches!(key.as_str(), "tab");
                let has_shift = event.keystroke.modifiers.shift;
                // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
                if is_tab_key
                    && !event.keystroke.modifiers.platform
                    && !event.keystroke.modifiers.alt
                    && !event.keystroke.modifiers.control
                {
                    if let Some(app) = app_entity.upgrade() {
                        app.update(cx, |this, cx| {
                            // FIRST: If confirm dialog is open, route Tab to it for button switching

thinking
**Reviewing script creation module**
exec
/bin/zsh -lc "sed -n '1,340p' src/script_creation.rs" in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc 'rg -n "create_new_script|open_in_editor|run_script|execute_script|script_creation" src | head -n 200' in /Users/johnlindquist/dev/script-kit-gpui exited 1 in 51ms:
sed: src/script_creation.rs: No such file or directory
 succeeded in 51ms:
src/app_actions/handle_action.rs:562:            "run_script" => {
src/app_actions/handle_action.rs:2506:                                        match executor::run_scriptlet(&action_scriptlet, options) {
src/app_actions/handle_action/main_actions_06.rs:240:                                        match executor::run_scriptlet(&action_scriptlet, options) {
src/app_actions/handle_action/main_actions_01.rs:18:            "run_script" => {
src/builtins/part_000.rs:81:// SDK tileWindow() function still works via protocol messages in execute_script.rs.
src/mcp_streaming/part_001.rs:110:                "tools/run_script",
src/mcp_streaming/part_001.rs:151:            .log_success("tools/run_script", params.clone(), 250)
src/mcp_streaming/part_001.rs:165:        assert_eq!(entry.method, "tools/run_script", "method must match");
src/builtins/part_001_entries/entries_001.rs:280:    // SDK tileWindow() function still works via protocol messages in execute_script.rs
src/app_impl/filter_input_change.rs:435:                            self.execute_scriptlet(&scriptlet, cx);
src/app_impl/selection_fallback.rs:177:                        self.execute_scriptlet(&scriptlet_match.scriptlet, cx);
src/executor_tests/chunk_10.rs:14:    let result = run_scriptlet(&scriptlet, ScriptletExecOptions::default());
src/executor_tests/chunk_10.rs:28:    let result = run_scriptlet(&scriptlet, ScriptletExecOptions::default());
src/executor_tests/chunk_10.rs:42:    let result = run_scriptlet(&scriptlet, ScriptletExecOptions::default());
src/executor_tests/chunk_10.rs:55:    let result = run_scriptlet(&scriptlet, ScriptletExecOptions::default());
src/executor_tests/chunk_10.rs:68:    let result = run_scriptlet(&scriptlet, ScriptletExecOptions::default());
src/executor_tests/chunk_10.rs:81:    let result = run_scriptlet(&scriptlet, ScriptletExecOptions::default());
src/executor_tests/chunk_10.rs:94:    let result = run_scriptlet(&scriptlet, ScriptletExecOptions::default());
src/executor_tests/chunk_10.rs:108:    let result = run_scriptlet(&scriptlet, ScriptletExecOptions::default());
src/executor_tests/chunk_10.rs:122:    let result = run_scriptlet(&scriptlet, ScriptletExecOptions::default());

thinking
**Inspecting module split files**
exec
/bin/zsh -lc "sed -n '1,220p' src/script_creation/mod.rs" in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc "sed -n '220,520p' src/script_creation/part_000.rs" in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc 'rg -n "pub fn (create_new_script|open_in_editor|create_new_extension|sanitize|slug|scripts_dir)" src/script_creation/part_000.rs src/script_creation/part_001.rs' in /Users/johnlindquist/dev/script-kit-gpui succeeded in 50ms:
//! Script and Extension Creation Module
//!
//! This module provides functions to create new scripts and extensions
//! in the Script Kit environment, as well as opening files in the configured editor.
//!
//! # Usage
//!
//! ```rust,ignore
//! use script_kit_gpui::script_creation::{create_new_script, create_new_extension, open_in_editor};
//! use script_kit_gpui::config::Config;
//!
//! // Create a new script
//! let script_path = create_new_script("my-script")?;
//!
//! // Create a new extension
//! let extension_path = create_new_extension("my-extension")?;
//!
//! // Open in editor
//! let config = Config::default();
//! open_in_editor(&script_path, &config)?;
 succeeded in 50ms:
  Learn more about creating extensions: ~/.scriptkit/kit/GUIDE.md
-->

# {title}

Scriptlets in this bundle.

## Hello World

```bash
echo "Hello from {title}!"
```
"#
    )
}
/// Create a new script file in ~/.scriptkit/scripts/
///
/// # Arguments
///
/// * `name` - The name of the script (will be sanitized for filename)
 succeeded in 51ms:
src/script_creation/part_000.rs:252:pub fn create_new_script(name: &str) -> Result<PathBuf> {
src/script_creation/part_000.rs:302:pub fn create_new_extension(name: &str) -> Result<PathBuf> {
src/script_creation/part_000.rs:403:pub fn open_in_editor(path: &Path, config: &Config) -> Result<()> {

thinking
**Inspecting script execution method**
exec
/bin/zsh -lc "sed -n '240,360p' src/app_impl/execution_scripts.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 52ms:
                            &self.theme,
                        )
                        .duration_ms(Some(5000)),
                    );
                    cx.notify();
                }
            }
            Err(e) => {
                logging::log(
                    "ERROR",
                    &format!("Failed to execute scriptlet '{}': {}", scriptlet.name, e),
                );

                self.toast_manager.push(
                    components::toast::Toast::error(
                        format!("Failed to execute: {}", e),
                        &self.theme,
                    )
                    .duration_ms(Some(5000)),
                );

thinking
**Assessing script execution flow**
exec
/bin/zsh -lc "sed -n '1,260p' src/app_impl/prompt_ai.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 52ms:
use super::*;
use anyhow::{anyhow, Context as AnyhowContext, Result};

#[derive(Debug, Clone, Copy)]
enum AiScriptGenerationStage {
    SelectModel,
    ResolveProvider,
    RequestCompletion,
    ExtractScript,
    CreateScriptFile,
    WriteScriptFile,
    OpenEditor,
}

impl AiScriptGenerationStage {
    fn as_str(self) -> &'static str {
        match self {
            Self::SelectModel => "select_model",
            Self::ResolveProvider => "resolve_provider",
            Self::RequestCompletion => "request_completion",

thinking
**Assessing script generation integration**
codex
I found duplicated script-generation helpers inside `prompt_ai.rs`; I’m now tracing `ScriptListApp` state and existing chat callbacks so the new script-generation chat can run save/run actions without introducing cross-view regressions.

thinking
**Exploring app state fields**
exec
/bin/zsh -lc 'rg -n "inline_chat_|ChatPrompt|current_view = AppView::ChatPrompt|opened_from_main_menu|pending_focus|show_inline_ai_chat|with_escape_callback" src/app_impl src | head -n 220' in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc 'rg -n "struct ScriptListApp" src | head -n 20' in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc "sed -n '1,320p' src/app_state.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
src/app_impl/alias_input.rs:49:            self.pending_focus = Some(FocusTarget::MainFilter);
src/app_impl/startup_new_actions.rs:94:                                AppView::ChatPrompt { .. } => {
src/app_impl/startup_new_actions.rs:96:                                    logging::log("KEY", "Interceptor: Cmd+K -> toggle_chat_actions (ChatPrompt)");
src/app_impl/startup_new_actions.rs:202:                                AppView::ChatPrompt { .. } => Some(ActionsDialogHost::ChatPrompt),
src/app_impl/startup_new_actions.rs:228:                                            ActionsDialogHost::ChatPrompt => {
src/app_impl/tests.rs:70:            startup_tab.contains("this.show_inline_ai_chat(Some(query), cx);"),
src/app_impl/tests.rs:71:            "Tab in ScriptList should continue routing to show_inline_ai_chat"
src/app_impl/actions_dialog.rs:266:        // pop_focus_overlay sets pending_focus to the saved target (e.g. ChatPrompt).
src/app_impl/actions_dialog.rs:268:        if !self.apply_pending_focus(window, cx) {
src/app_impl/startup_new_arrow.rs:25:                            // This ensures ALL views (ChatPrompt, ArgPrompt, etc.) route
src/app_impl/ui_window.rs:57:            AppView::ChatPrompt { .. } => Some((ViewType::DivPrompt, 0)), // Chat prompt uses div size
src/app_impl/actions_toggle.rs:359:        use crate::actions::{ChatModelInfo, ChatPromptInfo};
src/app_impl/actions_toggle.rs:371:            self.close_actions_popup(ActionsDialogHost::ChatPrompt, window, cx);
src/app_impl/actions_toggle.rs:373:            // Get chat info from current ChatPrompt entity
src/app_impl/actions_toggle.rs:374:            let chat_info = if let AppView::ChatPrompt { entity, .. } = &self.current_view {
src/app_impl/actions_toggle.rs:376:                ChatPromptInfo {
src/app_impl/actions_toggle.rs:396:                    "toggle_chat_actions called but current view is not ChatPrompt",
src/app_impl/actions_toggle.rs:406:            // CRITICAL: Transfer focus from ChatPrompt to main focus_handle
src/app_impl/actions_toggle.rs:407:            // This prevents the ChatPrompt from receiving text input while
src/app_impl/prompt_ai.rs:346:    /// This switches to the ChatPrompt view with direct AI integration (no SDK needed).
 succeeded in 50ms:
src/main_sections/app_state.rs:1:struct ScriptListApp {
 exited 1 in 50ms:
sed: src/app_state.rs: No such file or directory

thinking
**Inspecting app state struct**
exec
/bin/zsh -lc "sed -n '1,360p' src/main_sections/app_state.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
struct ScriptListApp {
    /// H1 Optimization: Arc-wrapped scripts for cheap cloning during filter operations
    scripts: Vec<std::sync::Arc<scripts::Script>>,
    /// H1 Optimization: Arc-wrapped scriptlets for cheap cloning during filter operations
    scriptlets: Vec<std::sync::Arc<scripts::Scriptlet>>,
    builtin_entries: Vec<builtins::BuiltInEntry>,
    /// Cached list of installed applications for main search and AppLauncherView
    apps: Vec<app_launcher::AppInfo>,
    /// P0 FIX: Cached clipboard entries for ClipboardHistoryView (avoids cloning per frame)
    cached_clipboard_entries: Vec<clipboard_history::ClipboardEntryMeta>,
    /// Focused clipboard entry ID for action handling in ClipboardHistoryView
    #[allow(dead_code)]
    focused_clipboard_entry_id: Option<String>,
    /// P0 FIX: Cached windows for WindowSwitcherView (avoids cloning per frame)
    cached_windows: Vec<window_control::WindowInfo>,
    /// Cached file results for FileSearchView (avoids cloning per frame)
    cached_file_results: Vec<file_search::FileResult>,
    selected_index: usize,
    /// Main menu filter text (mirrors gpui-component input state)
    filter_text: String,

thinking
**Evaluating script action integration**
exec
/bin/zsh -lc 'rg -n "generate_script_from_prompt|GeneratedScriptOutput|ai-script-generation|AI_SCRIPT_GENERATION_SYSTEM_PROMPT|slugify_script_name|script generation" src | head -n 200' in /Users/johnlindquist/dev/script-kit-gpui succeeded in 52ms:
src/ai/mod.rs:48:pub use self::script_generation::{generate_script_from_prompt, GeneratedScriptOutput};
src/ai/script_generation.rs:14:const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
src/ai/script_generation.rs:28:pub struct GeneratedScriptOutput {
src/ai/script_generation.rs:35:pub fn generate_script_from_prompt(
src/ai/script_generation.rs:38:) -> Result<GeneratedScriptOutput> {
src/ai/script_generation.rs:41:        anyhow::bail!("AI script generation requires a non-empty prompt");
src/ai/script_generation.rs:55:            "correlation_id=ai-script-generation state=provider_ready model_id={} provider_id={} prompt_len={}",
src/ai/script_generation.rs:63:        ProviderMessage::system(AI_SCRIPT_GENERATION_SYSTEM_PROMPT),
src/ai/script_generation.rs:74:                "AI script generation failed (attempted=send_message, model_id={}, provider_id={})",
src/ai/script_generation.rs:81:        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
src/ai/script_generation.rs:84:    let slug = slugify_script_name(normalized_prompt);
src/ai/script_generation.rs:96:            "correlation_id=ai-script-generation state=script_written path={} slug={}",
src/ai/script_generation.rs:102:    Ok(GeneratedScriptOutput {
src/ai/script_generation.rs:189:fn slugify_script_name(prompt: &str) -> String {
src/ai/script_generation.rs:343:    fn test_slugify_script_name_handles_spaces_and_symbols() {
src/ai/script_generation.rs:345:            slugify_script_name("Build: API Client!"),
src/ai/script_generation.rs:348:        assert_eq!(slugify_script_name("  ___  "), "ai-script");
src/app_execute/builtin_execution.rs:704:                                    "No prompt available for AI script generation",
src/app_execute/builtin_execution.rs:715:                            std::result::Result<ai::GeneratedScriptOutput, String>,
src/app_execute/builtin_execution.rs:728:                                ai::generate_script_from_prompt(&prompt_description, Some(&config))

thinking
**Inspecting builtin execution**
exec
/bin/zsh -lc "sed -n '660,830p' src/app_execute/builtin_execution.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 52ms:
                                    self.toast_manager.push(
                                        components::toast::Toast::error(
                                            format!(
                                                "AI history cleared, but failed to open AI: {}",
                                                e
                                            ),
                                            &self.theme,
                                        )
                                        .duration_ms(Some(5000)),
                                    );
                                    cx.notify();
                                } else {
                                    self.show_hud(
                                        "Cleared AI conversations".to_string(),
                                        Some(2000),
                                        cx,
                                    );
                                }
                            }
                            Err(e) => {

thinking
**Reviewing chat integration approach**
exec
/bin/zsh -lc "sed -n '1,240p' src/main_sections/render_impl.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 52ms:
impl Focusable for ScriptListApp {
    fn focus_handle(&self, _cx: &App) -> FocusHandle {
        self.focus_handle.clone()
    }
}

impl Render for ScriptListApp {
    fn render(&mut self, window: &mut Window, cx: &mut Context<Self>) -> impl IntoElement {
        // Track render timing for filter perf analysis
        let render_start = std::time::Instant::now();
        let filter_snapshot = self.filter_text.clone();

        // Always log render start for "gr" prefix filters to debug the issue
        if filter_snapshot.starts_with("gr") {
            crate::logging::log(
                "FILTER_PERF",
                &format!(
                    "[FRAME_START] filter='{}' selected_idx={} view={:?}",
                    filter_snapshot,
                    self.selected_index,

thinking
**Evaluating callback handling strategies**
exec
/bin/zsh -lc "sed -n '1,240p' src/render_prompts/other.rs" in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc "sed -n '1,220p' src/app_impl/chat_actions.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
mod __render_prompts_other_docs {
    //! Render methods for non-arg prompts: select, env, drop, template, chat, and webcam.
    //! Key APIs are the `render_*_prompt` methods and shared key-routing helpers for actions dialogs.
    //! This fragment depends on prompt entities, action routing, and shell container components from `main.rs`.
}

// Other prompt render methods - extracted from render_prompts.rs
// Contains: select, env, drop, template prompts
// This file is included via include!() macro in main.rs

impl ScriptListApp {
    #[inline]
    fn other_prompt_shell_radius_lg(&self) -> f32 {
        get_tokens(self.current_design).visual().radius_lg
    }

    #[inline]
    fn other_prompt_shell_handle_key_default(
        &mut self,
        event: &gpui::KeyDownEvent,
 succeeded in 51ms:
use super::*;

impl ScriptListApp {
    pub fn execute_chat_action(&mut self, action_id: &str, cx: &mut Context<Self>) {
        logging::log("ACTIONS", &format!("execute_chat_action: {}", action_id));

        let action_id = action_id.strip_prefix("chat:").unwrap_or(action_id);

        // Handle model selection (action_id starts with "select_model_")
        if let Some(model_id) = action_id.strip_prefix("select_model_") {
            let mut selected_model_name: Option<String> = None;
            if let AppView::ChatPrompt { entity, .. } = &self.current_view {
                let model_id_owned = model_id.to_string();
                entity.update(cx, |chat, cx| {
                    // Find model by ID and set it
                    if let Some(model) = chat.models.iter().find(|m| m.id == model_id_owned) {
                        chat.model = Some(model.name.clone());
                        selected_model_name = Some(model.name.clone());
                        logging::log("CHAT", &format!("Model changed to: {}", model.name));
                        cx.notify();

thinking
**Designing script generation UI and callbacks**
exec
/bin/zsh -lc "sed -n '540,760p' src/executor/runner.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
        logging::log("EXEC", &format!("Script exited with code: {}", code));
        Ok(code)
    }

    /// Kill the child process and its process group
    pub fn kill(&mut self) -> Result<(), String> {
        logging::log(
            "EXEC",
            &format!("ScriptSession::kill() for PID {}", self.process_handle.pid),
        );
        self.process_handle.kill();
        // Also try the standard kill for good measure
        let _ = self.child.kill();
        Ok(())
    }

    /// Get the process ID
    pub fn pid(&self) -> u32 {
        self.process_handle.pid
    }

thinking
**Checking resource cleanup behavior**
exec
/bin/zsh -lc "rg -n \"impl Drop for ScriptSession|struct ScriptSession\" src/executor/runner.rs && sed -n '420,560p' src/executor/runner.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
408:pub struct ScriptSession {
    pub stdin: ChildStdin,
    pub stdout_reader: JsonlReader<BufReader<ChildStdout>>,
    /// Captured stderr for error reporting
    pub stderr: Option<ChildStderr>,
    pub child: Child,
    /// Process handle for cleanup - kills process group on drop
    /// IMPORTANT: This MUST be kept alive until the script completes!
    /// Dropping it will kill the process group via the Drop impl.
    pub process_handle: ProcessHandle,
}

impl ScriptSession {
    /// Split the session into separate read/write components
    /// This allows using separate threads for reading and writing
    pub fn split(self) -> SplitSession {
        logging::log(
            "EXEC",
            &format!(
                "Splitting ScriptSession for PID {}",

thinking
**Inspecting app startup initialization**
exec
/bin/zsh -lc "sed -n '250,560p' src/app_impl/startup.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
                InputEvent::Blur => {
                    this.gpui_input_focused = false;
                    if this.focused_input == FocusedInput::MainFilter {
                        this.focused_input = FocusedInput::None;
                    }
                    cx.notify();
                }
                InputEvent::Change => {
                    let input_received_at = std::time::Instant::now();
                    // Read the current input value to see what we're processing
                    let current_value = this.gpui_input_state.read(cx).value().to_string();
                    logging::log(
                        "FILTER_PERF",
                        &format!(
                            "[1/5] INPUT_CHANGE value='{}' len={} at {:?}",
                            current_value,
                            current_value.len(),
                            input_received_at
                        ),
                    );

thinking
**Checking alternate startup state**
exec
/bin/zsh -lc "sed -n '1,280p' src/app_impl/startup_new_state.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 52ms:
        // Create channel for API key configuration completion signals
        // Small buffer (4) prevents blocking, more than enough for normal use
        let (api_key_tx, api_key_rx) = mpsc::sync_channel(4);

        // Create channel for builtin confirmation modal signals
        // When a dangerous action (Quit, Shut Down, etc.) requires confirmation,
        // the modal callback sends (entry_id, confirmed) through this channel
        let (builtin_confirm_tx, builtin_confirm_rx) = async_channel::bounded(4);

        // Create channel for inline chat escape signals
        let (inline_chat_escape_tx, inline_chat_escape_rx) = mpsc::sync_channel(4);
        // Create channel for inline chat configure signals (when user wants to set up API key)
        let (inline_chat_configure_tx, inline_chat_configure_rx) = mpsc::sync_channel(4);
        // Create channel for inline chat Claude Code signals (when user wants to enable Claude Code)
        let (inline_chat_claude_code_tx, inline_chat_claude_code_rx) = mpsc::sync_channel(4);
        let mut app = ScriptListApp {
            scripts,
            scriptlets,
            builtin_entries,
            apps,

thinking
**Investigating module relationships**
exec
/bin/zsh -lc "sed -n '1,260p' src/app_impl/mod.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
#[path = "startup.rs"]
mod startup;
#[path = "theme_focus.rs"]
mod theme_focus;
#[path = "refresh_scriptlets.rs"]
mod refresh_scriptlets;
#[path = "filtering_cache.rs"]
mod filtering_cache;
#[path = "selection_fallback.rs"]
mod selection_fallback;
#[path = "filter_input_core.rs"]
mod filter_input_core;
#[path = "filter_input_change.rs"]
mod filter_input_change;
#[path = "filter_input_updates.rs"]
mod filter_input_updates;
#[path = "ui_window.rs"]
mod ui_window;
#[path = "actions_toggle.rs"]
mod actions_toggle;

thinking
**Inspecting startup includes**
exec
/bin/zsh -lc "sed -n '1,220p' src/app_impl/startup.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
use super::*;

pub(super) fn calculate_fallback_error_message(expression: &str) -> String {
    format!(
        "Could not evaluate expression \"{}\". Check the syntax and try again.",
        expression
    )
}

impl ScriptListApp {
    pub(crate) fn new(
        config: config::Config,
        bun_available: bool,
        window: &mut Window,
        cx: &mut Context<Self>,
    ) -> Self {
        // PERF: Parallelize script + scriptlet loading to reduce startup wall time.
        let load_start = std::time::Instant::now();
        let (scripts, scriptlets, scripts_elapsed, scriptlets_elapsed) = std::thread::scope(
            |scope| {

thinking
**Checking module inclusion logic**
exec
/bin/zsh -lc 'rg -n "startup_new_tab|include!'"\\(\" src/app_impl/startup.rs src/app_impl/startup_new_tab.rs src/app_impl/startup_new_state.rs" in /Users/johnlindquist/dev/script-kit-gpui exited 1 in 51ms:


thinking
**Planning dual-file updates**
exec
/bin/zsh -lc "sed -n '1,220p' src/app_impl/tests.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
#[cfg(test)]
mod app_impl_state_sync_tests {
    use super::{calculate_fallback_error_message, ScriptListApp};
    use std::fs;

    #[test]
    fn test_sync_builtin_query_state_updates_query_and_selection_when_changed() {
        let mut query = String::from("old");
        let mut selected_index = 3;

        let changed =
            ScriptListApp::sync_builtin_query_state(&mut query, &mut selected_index, "new");

        assert!(changed);
        assert_eq!(query, "new");
        assert_eq!(selected_index, 0);
    }

    #[test]
    fn test_sync_builtin_query_state_noop_when_query_is_unchanged() {

thinking
**Scoping tests and callbacks**
exec
/bin/zsh -lc "sed -n '1,340p' src/protocol/types/chat.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
use serde::{Deserialize, Serialize};

use super::ProtocolAction;

// ============================================================
// CHAT PROMPT TYPES (AI SDK Compatible)
// ============================================================

/// Role of a chat message - compatible with AI SDK's CoreMessage
#[derive(Clone, Debug, Serialize, Deserialize, PartialEq, Default)]
#[serde(rename_all = "lowercase")]
pub enum ChatMessageRole {
    /// System prompt/instructions
    System,
    /// User message
    User,
    /// Assistant response
    #[default]
    Assistant,
    /// Tool/function call result

thinking
**Inspecting button placement options**
exec
/bin/zsh -lc "sed -n '280,620p' src/prompts/chat/render_core.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 50ms:
                    }
                })
            })
            .with_sizing_behavior(ListSizingBehavior::Infer)
            .size_full()
            .px(px(12.0))
            .py(px(12.0));

            div()
                .id("chat-messages")
                .relative()
                .flex_1()
                .min_h(px(0.))
                .on_scroll_wheel(
                    cx.listener(move |this, event: &ScrollWheelEvent, _window, cx| {
                        let delta_y = event.delta.pixel_delta(px(1.0)).y;
                        let direction = if delta_y > px(0.) {
                            ChatScrollDirection::Up
                        } else if delta_y < px(0.) {
                            ChatScrollDirection::Down

thinking
**Scoping UI insertion point**
exec
/bin/zsh -lc "sed -n '1,180p' src/ai/mod.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
//! AI Chat Module
//!
//! This module provides the data layer for the AI chat window feature.
//! It includes data models, SQLite storage with FTS5 search support,
//! and provider abstraction for BYOK (Bring Your Own Key) AI integration.
//!
//! # Architecture
//!
//! ```text
//! src/ai/
//! ├── mod.rs       - Module exports and documentation
//! ├── model.rs     - Data models (Chat, Message, ChatId, MessageRole)
//! ├── storage.rs   - SQLite persistence layer
//! ├── config.rs    - Environment variable detection and model configuration
//! └── providers.rs - Provider trait and implementations (OpenAI, Anthropic, etc.)
//! ```
//!
//! # Database Location
//!
//! The AI chats database is stored at `~/.scriptkit/ai-chats.db`.

thinking
**Designing script generation UI and logic**
exec
/bin/zsh -lc "sed -n '1,220p' src/script_creation/part_000.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 50ms:
use crate::config::Config;
use anyhow::{Context, Result};
use std::fs::{self, OpenOptions};
use std::io::{ErrorKind, Write};
use std::path::{Path, PathBuf};
use std::process::Command;
use tracing::{info, instrument, warn};
/// Scripts directory under ~/.scriptkit/kit/main/
const SCRIPTS_DIR: &str = "~/.scriptkit/kit/main/scripts";
/// Extensions directory under ~/.scriptkit/kit/main/
const EXTENSIONS_DIR: &str = "~/.scriptkit/kit/main/extensions";
/// Maximum filename size on most filesystems (bytes, not chars).
const MAX_FILENAME_BYTES: usize = 255;
/// Reserved filenames on Windows that are invalid even with an extension.
const WINDOWS_RESERVED_FILENAMES: [&str; 22] = [
    "con", "prn", "aux", "nul", "com1", "com2", "com3", "com4", "com5", "com6", "com7", "com8",
    "com9", "lpt1", "lpt2", "lpt3", "lpt4", "lpt5", "lpt6", "lpt7", "lpt8", "lpt9",
];
fn scripts_dir() -> PathBuf {
    PathBuf::from(shellexpand::tilde(SCRIPTS_DIR).as_ref())

thinking
**Designing script generation integration**
exec
/bin/zsh -lc "sed -n '1,120p' src/executor/mod.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 52ms:
//! Script execution module
//!
//! This module handles all aspects of script execution including:
//! - Interactive script sessions with bidirectional JSONL communication
//! - Scriptlet execution (embedded scripts in markdown)
//! - Error parsing and suggestions
//! - Selected text operations
//! - Auto-submit mode for autonomous testing

mod auto_submit;
mod errors;
mod runner;
mod scriptlet;
mod selected_text;
mod stderr_buffer;

// Re-export public items for external use and backwards compatibility
// Allow unused imports - these are public API exports that may be used by external code
// or will be used in the future (marked #[allow(dead_code)] in their source files)
#[allow(unused_imports)]

thinking
**Designing script run integration**
exec
/bin/zsh -lc "rg -n \"generate_script_from_ai_prompt\\(\" src | head -n 50" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
src/app_impl/tests.rs:58:                && startup_tab.contains("this.generate_script_from_ai_prompt(query, cx);"),
src/app_impl/prompt_ai.rs:469:    pub fn generate_script_from_ai_prompt(
src/app_impl/startup.rs:737:                                    this.generate_script_from_ai_prompt(query, cx);
src/app_impl/startup_new_tab.rs:188:                                    this.generate_script_from_ai_prompt(query, cx);

thinking
**Designing script generation helpers**
exec
/bin/zsh -lc 'rg -n "anyhow|with_context|AnyhowContext|Result<" src/app_impl/prompt_ai.rs' in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
2:use anyhow::{anyhow, Context as AnyhowContext, Result};
129:) -> Result<std::path::PathBuf> {
134:            anyhow!(
148:        .with_context(|| {
158:        anyhow!(
166:    let script_path = crate::script_creation::create_new_script(&script_name).with_context(|| {
174:    std::fs::write(&script_path, generated_script).with_context(|| {
182:    crate::script_creation::open_in_editor(&script_path, config).with_context(|| {
518:        let (tx, rx) = async_channel::bounded::<std::result::Result<std::path::PathBuf, String>>(1);

thinking
**Planning import cleanup and method review**
exec
/bin/zsh -lc "sed -n '620,980p' src/app_impl/prompt_ai.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 50ms:
                );
            }
        });

        cx.spawn(async move |this, cx| {
            let Ok(registry) = rx.recv().await else {
                logging::log("APP", "Provider registry rebuild failed: channel closed");
                return;
            };

            let provider_count = registry.provider_ids().len();
            let _ = cx.update(|cx| {
                this.update(cx, |app, _cx| {
                    app.cached_provider_registry = Some(registry);
                    logging::log(
                        "APP",
                        &format!("Provider registry rebuilt: {} providers", provider_count),
                    );
                })
            });

thinking
**Scoping and planning focused test updates**
codex
I have enough context and I’m ready to implement. I’m claiming the targeted files now, then I’ll wire a new `show_script_generation_chat` flow, add script-generation mode to `ChatPrompt`, and connect post-stream Save/Run actions to `ai::script_generation` save helpers.

thinking
**Running claim command on paths**
exec
/bin/zsh -lc '~/.claude/bin/swarm.ts claim --wait --id codex-chat-prompt-script-gen src/app_impl/prompt_ai.rs src/app_impl/startup.rs src/app_impl/startup_new_tab.rs src/prompts/chat/mod.rs src/prompts/chat/prompt.rs src/prompts/chat/render_core.rs src/prompts/chat/streaming.rs src/prompts/chat/types.rs src/prompts/chat/tests.rs src/ai/script_generation.rs' in /Users/johnlindquist/dev/script-kit-gpui succeeded in 50ms:
Claimed for codex-chat-prompt-script-gen: src/app_impl/prompt_ai.rs, src/app_impl/startup.rs, src/app_impl/startup_new_tab.rs, src/prompts/chat/mod.rs, src/prompts/chat/prompt.rs, src/prompts/chat/render_core.rs, src/prompts/chat/streaming.rs, src/prompts/chat/types.rs, src/prompts/chat/tests.rs, src/ai/script_generation.rs

thinking
**Designing AI script generation helpers**
file update
M /Users/johnlindquist/dev/script-kit-gpui/src/ai/script_generation.rs
@@ -13,3 +13,3 @@
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
@@ -78,9 +78,3 @@
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
@@ -109,2 +103,73 @@
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
@@ -427,2 +492,29 @@
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
apply_patch(auto_approved=true) exited 0 in 12ms:
Success. Updated the following files:
M src/ai/script_generation.rs
file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }

file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }


thinking
**Designing script generation types**
exec
/bin/zsh -lc "sed -n '1,520p' src/prompts/chat/prompt.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 52ms:
use super::*;
pub struct ChatPrompt {
    pub id: String,
    pub messages: Vec<ChatPromptMessage>,
    pub placeholder: Option<String>,
    pub hint: Option<String>,
    pub footer: Option<String>,
    pub model: Option<String>,
    pub models: Vec<ChatModel>,
    pub title: Option<String>,
    pub focus_handle: FocusHandle,
    pub input: TextInputState,
    pub on_submit: ChatSubmitCallback,
    pub on_escape: Option<ChatEscapeCallback>,
    pub on_continue: Option<ChatContinueCallback>,
    pub on_retry: Option<ChatRetryCallback>,
    pub theme: Arc<theme::Theme>,
    pub turns_list_state: ListState,
    pub(super) prompt_colors: theme::PromptColors,
    pub(super) conversation_turns_cache: Arc<Vec<ConversationTurn>>,
file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }


thinking
**Extending prompt struct with script generation**
file update
M /Users/johnlindquist/dev/script-kit-gpui/src/prompts/chat/prompt.rs
@@ -29,2 +29,3 @@
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
@@ -50,2 +51,6 @@
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
@@ -111,2 +116,3 @@
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
@@ -123,2 +129,5 @@
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
@@ -309,2 +318,14 @@
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
@@ -441,2 +462,48 @@
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+    }
 }
apply_patch(auto_approved=true) exited 0 in 17ms:
Success. Updated the following files:
M src/prompts/chat/prompt.rs
file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..f38da4434a11c5693a2baaca2ad71f17d6e04ad8
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,50 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+    }
 }

file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..f38da4434a11c5693a2baaca2ad71f17d6e04ad8
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,50 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+    }
 }


thinking
**Adding built-in system prompt and status clearing**
file update
M /Users/johnlindquist/dev/script-kit-gpui/src/prompts/chat/streaming.rs
@@ -13,2 +13,3 @@
         self.input.clear();
+        self.clear_script_generation_status();
 
@@ -113,2 +114,6 @@
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
@@ -236,3 +241,7 @@
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
@@ -247,4 +256,3 @@
                 }
-            })
-            .collect();
+            }));
 
apply_patch(auto_approved=true) exited 0 in 22ms:
Success. Updated the following files:
M src/prompts/chat/streaming.rs
file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..f38da4434a11c5693a2baaca2ad71f17d6e04ad8
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,50 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;

file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..f38da4434a11c5693a2baaca2ad71f17d6e04ad8
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,50 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;


thinking
**Inspecting syntax near message extension**
exec
/bin/zsh -lc "sed -n '1,220p' src/prompts/chat/streaming.rs && sed -n '220,420p' src/prompts/chat/streaming.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
use super::*;

impl ChatPrompt {
    pub(super) fn handle_submit(&mut self, cx: &mut Context<Self>) {
        let text = self.input.text().to_string();
        let pending_image = self.pending_image.take();
        let pending_render = self.pending_image_render.take();

        if text.trim().is_empty() && pending_image.is_none() {
            return;
        }
        logging::log("CHAT", &format!("User submitted: {}", text));
        self.input.clear();
        self.clear_script_generation_status();

        // If built-in AI mode is enabled, handle the AI call directly
        if self.has_builtin_ai() {
            // Cache the render image for conversation history display
            // We need the user message ID, which will be generated in handle_builtin_ai_submit
            self.handle_builtin_ai_submit(text, pending_image, pending_render, cx);
file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..f38da4434a11c5693a2baaca2ad71f17d6e04ad8
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,50 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;

context compacted
Reconnecting... 1/5 (stream disconnected before completion: An error occurred while processing your request. You can retry your request, or contact us through our help center at help.openai.com if the error persists. Please include the request ID b33d22c2-a674-4600-8533-0f7d4bd1b151 in your message.)

thinking
**Checking claim status**
codex
I’ve resumed from the partial implementation and will first inspect the current file states, fix the known `streaming.rs` syntax issue, then finish wiring Shift+Tab to open ChatPrompt in script-generation mode with Save/Run actions. After that I’ll run scoped verification on the changed modules.

thinking
**Planning parallel command execution**
exec
/bin/zsh -lc 'git diff -- src/app_impl/prompt_ai.rs src/app_impl/startup.rs src/app_impl/startup_new_tab.rs src/prompts/chat/mod.rs src/prompts/chat/prompt.rs src/prompts/chat/render_core.rs src/prompts/chat/streaming.rs src/prompts/chat/types.rs src/prompts/chat/tests.rs src/ai/script_generation.rs' in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc "~/.claude/bin/swarm.ts status && echo '---' && git status --short" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 52ms:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9c..5cbd63a 100644
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@ const AI_SCRIPT_OUTPUT_DIR: &str = "~/.kenv/scripts";
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@ pub fn generate_script_from_prompt(
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
 succeeded in 58ms:
Repo: /Users/johnlindquist/dev/script-kit-gpui
Updated: 2026-02-08T03:32:49.923Z

Active agents (3):
codex-chat-prompt-script-gen [in_progress] codex/gpt-5.3-codex/exec
  task: Refactor the Shift+Tab AI script generation flow. Currently Shift+Tab in the main prompt calls generate_script_from_ai_…
  scope: src/app_impl/prompt_ai.rs,src/app_impl/startup.rs,src/app_impl/startup_new_tab.rs,src/prompts/chat/**,src/ai/script_gen…
  claims(10): src/app_impl/prompt_ai.rs, src/app_impl/startup.rs, src/app_impl/startup_new_tab.rs, src/prompts/chat/mod.rs, src/prompts/chat/prompt.rs, src/prompts/chat/rend…
  notes: Inspecting current Shift+Tab and ChatPrompt flow for script-generation mode

codex-fallback-generate-script [in_progress] codex/gpt-5.3-codex/exec
  task: Make 'Generate Script with AI' the top fallback built-in command. In Script Kit, when the user types text that doesn't …
  scope: src/app_impl/selection_fallback.rs,src/builtins/part_001.rs,src/builtins/part_001_entries/**,src/builtins/part_000.rs
  claims(3): src/app_impl/selection_fallback.rs, src/fallbacks/builtins.rs, src/main_sections/fallbacks.rs
  notes: Inspecting fallback ordering and built-in entry priority for Generate Script with AI

codex-wire-remaining-v3 [in_progress] codex/gpt-5.3-codex/exec
  task: Previous workers wired Favorites and Clipboard Exclusions into the app. They also partially wired EmojiPicker and Quick…
  scope: src/emoji/**,src/quicklinks/**,src/builtins/part_000.rs,src/builtins/part_001_entries/**,src/builtins/tests/**,src/app_…
  claims(4): src/builtins/part_000.rs, src/builtins/part_001_entries/entries_000.rs, src/app_execute/builtin_execution.rs, src/builtins/tests/part_000.rs
file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..f38da4434a11c5693a2baaca2ad71f17d6e04ad8
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,50 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;


thinking
**Preparing comprehensive file inspection**
codex
I found the partial edits applied in `script_generation.rs`, `chat/prompt.rs`, and `chat/streaming.rs`. Next I’m reading the startup and chat rendering paths to wire the new mode and action buttons cleanly, then I’ll patch and compile-test.
exec
/bin/zsh -lc "sed -n '1,260p' src/app_impl/prompt_ai.rs" in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc "sed -n '1,260p' src/app_impl/startup_new_tab.rs" in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc "sed -n '1,260p' src/app_impl/startup.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
use super::*;
use anyhow::{anyhow, Context as AnyhowContext, Result};

#[derive(Debug, Clone, Copy)]
enum AiScriptGenerationStage {
    SelectModel,
    ResolveProvider,
    RequestCompletion,
    ExtractScript,
    CreateScriptFile,
    WriteScriptFile,
    OpenEditor,
}

impl AiScriptGenerationStage {
    fn as_str(self) -> &'static str {
        match self {
            Self::SelectModel => "select_model",
            Self::ResolveProvider => "resolve_provider",
            Self::RequestCompletion => "request_completion",
 succeeded in 51ms:
        // Add Tab key interceptor for "Ask AI" feature and file search directory navigation
        // This fires BEFORE normal key handling, allowing us to intercept Tab
        // even when the Input component has focus
        let app_entity_for_tab = cx.entity().downgrade();
        let tab_interceptor = cx.intercept_keystrokes({
            let app_entity = app_entity_for_tab;
            move |event, window, cx| {
                let key = event.keystroke.key.to_lowercase();
                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
                    || matches!(key.as_str(), "tab");
                let has_shift = event.keystroke.modifiers.shift;
                // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
                if is_tab_key
                    && !event.keystroke.modifiers.platform
                    && !event.keystroke.modifiers.alt
                    && !event.keystroke.modifiers.control
                {
                    if let Some(app) = app_entity.upgrade() {
                        app.update(cx, |this, cx| {
                            // FIRST: If confirm dialog is open, route Tab to it for button switching
 succeeded in 51ms:
use super::*;

pub(super) fn calculate_fallback_error_message(expression: &str) -> String {
    format!(
        "Could not evaluate expression \"{}\". Check the syntax and try again.",
        expression
    )
}

impl ScriptListApp {
    pub(crate) fn new(
        config: config::Config,
        bun_available: bool,
        window: &mut Window,
        cx: &mut Context<Self>,
    ) -> Self {
        // PERF: Parallelize script + scriptlet loading to reduce startup wall time.
        let load_start = std::time::Instant::now();
        let (scripts, scriptlets, scripts_elapsed, scriptlets_elapsed) = std::thread::scope(
            |scope| {
file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..f38da4434a11c5693a2baaca2ad71f17d6e04ad8
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,50 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;


thinking
**Inspecting chat modules**
exec
/bin/zsh -lc "sed -n '1,260p' src/prompts/chat/mod.rs" in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc "sed -n '1,340p' src/prompts/chat/render_core.rs" in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc "sed -n '1,380p' src/prompts/chat/tests.rs" in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc "sed -n '1,320p' src/prompts/chat/types.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 50ms:
//! ChatPrompt - Raycast-style chat interface
//!
//! Features:
//! - Input at TOP (not bottom)
//! - Messages bundled as conversation turns (user prompt + AI response in same container)
//! - Full-width containers (not bubbles)
//! - Footer with model selector and "Continue in Chat"
//! - Actions menu (⌘+K) with model picker

use crate::components::prompt_footer::{PromptFooter, PromptFooterColors, PromptFooterConfig};
use crate::components::TextInputState;
use crate::designs::icon_variations::IconName;
use gpui::{
    div, img, list, prelude::*, px, rgb, rgba, svg, App, Context, ExternalPaths, FocusHandle,
    Focusable, Hsla, KeyDownEvent, ListAlignment, ListSizingBehavior, ListState, Render,
    RenderImage, ScrollWheelEvent, Timer, Window,
};
use gpui_component::{scroll::ScrollableElement, theme::ActiveTheme};
use std::collections::HashMap;
use std::sync::atomic::{AtomicBool, Ordering};
 succeeded in 50ms:
use super::*;
#[cfg(test)]
#[allow(clippy::module_inception)]
mod tests {
    use std::collections::HashMap;

    use crate::protocol::ChatPromptMessage;

    use super::{
        next_chat_scroll_follow_state, next_reveal_boundary, resolve_chat_input_key_action,
        resolve_setup_card_key, should_ignore_stream_reveal_update, ChatInputKeyAction,
        ChatScrollDirection, SetupCardAction,
    };

    #[test]
    fn resolve_setup_card_key_cycles_focus_for_tab_and_arrows() {
        assert_eq!(
            resolve_setup_card_key("tab", false, 0),
            (1, SetupCardAction::None, true)
        );
 succeeded in 50ms:
use super::*;
/// Available AI models for the chat
#[derive(Clone, Debug, PartialEq)]
pub struct ChatModel {
    pub id: String,
    pub name: String,
    pub provider: String,
}

impl ChatModel {
    pub fn new(
        id: impl Into<String>,
        name: impl Into<String>,
        provider: impl Into<String>,
    ) -> Self {
        Self {
            id: id.into(),
            name: name.into(),
            provider: provider.into(),
        }
 succeeded in 50ms:
use super::*;

impl ChatPrompt {
    fn render_footer(&self, _cx: &mut Context<Self>) -> impl IntoElement {
        // Use standard PromptFooter colors from theme
        let footer_colors = PromptFooterColors::from_theme(&self.theme);

        // Build model display text (show model name if available)
        let model_text = self.model.clone().unwrap_or_else(|| "Select Model".into());

        // Configure footer with chat-specific labels
        let footer_config = PromptFooterConfig::new()
            .primary_label("Continue in Chat")
            .primary_shortcut("⌘↵")
            .secondary_label("Actions")
            .secondary_shortcut("⌘K")
            .show_logo(true)
            .show_secondary(true)
            .helper_text(model_text) // Show model name next to logo
            .info_label("Shift+Enter newline");
file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..f38da4434a11c5693a2baaca2ad71f17d6e04ad8
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,50 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;


thinking
**Inspecting streaming and action rendering**
exec
/bin/zsh -lc "sed -n '320,760p' src/prompts/chat/render_core.rs" in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc "sed -n '1,320p' src/prompts/chat/prompt.rs" in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc "sed -n '1,420p' src/prompts/chat/streaming.rs" in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc "sed -n '1,420p' src/prompts/chat/actions.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
                            cx.notify();
                        }
                    }),
                )
                .child(turns_list)
                .vertical_scrollbar(&self.turns_list_state)
                .when(show_scroll_to_latest, |el| {
                    el.child(
                        div()
                            .id("chat-scroll-to-latest-pill")
                            .absolute()
                            .bottom(px(12.0))
                            .left_0()
                            .right_0()
                            .flex()
                            .justify_center()
                            .child(
                                div()
                                    .id("chat-scroll-to-latest-button")
                                    .px(px(10.0))
 succeeded in 51ms:
use super::*;

impl ChatPrompt {
    pub(super) fn handle_escape(&mut self, _cx: &mut Context<Self>) {
        logging::log("CHAT", "Escape pressed - closing chat");

        // Save conversation to database if save_history is enabled
        if self.save_history {
            self.save_to_database();
        }

        if let Some(ref callback) = self.on_escape {
            callback(self.id.clone());
        }
    }

    /// Save the current conversation to the AI chats database
    pub(super) fn save_to_database(&self) {
        // Only save if we have messages
        if self.messages.is_empty() {
 succeeded in 51ms:
use super::*;
pub struct ChatPrompt {
    pub id: String,
    pub messages: Vec<ChatPromptMessage>,
    pub placeholder: Option<String>,
    pub hint: Option<String>,
    pub footer: Option<String>,
    pub model: Option<String>,
    pub models: Vec<ChatModel>,
    pub title: Option<String>,
    pub focus_handle: FocusHandle,
    pub input: TextInputState,
    pub on_submit: ChatSubmitCallback,
    pub on_escape: Option<ChatEscapeCallback>,
    pub on_continue: Option<ChatContinueCallback>,
    pub on_retry: Option<ChatRetryCallback>,
    pub theme: Arc<theme::Theme>,
    pub turns_list_state: ListState,
    pub(super) prompt_colors: theme::PromptColors,
    pub(super) conversation_turns_cache: Arc<Vec<ConversationTurn>>,
 succeeded in 51ms:
use super::*;

impl ChatPrompt {
    pub(super) fn handle_submit(&mut self, cx: &mut Context<Self>) {
        let text = self.input.text().to_string();
        let pending_image = self.pending_image.take();
        let pending_render = self.pending_image_render.take();

        if text.trim().is_empty() && pending_image.is_none() {
            return;
        }
        logging::log("CHAT", &format!("User submitted: {}", text));
        self.input.clear();
        self.clear_script_generation_status();

        // If built-in AI mode is enabled, handle the AI call directly
        if self.has_builtin_ai() {
            // Cache the render image for conversation history display
            // We need the user message ID, which will be generated in handle_builtin_ai_submit
            self.handle_builtin_ai_submit(text, pending_image, pending_render, cx);
file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..f38da4434a11c5693a2baaca2ad71f17d6e04ad8
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,50 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;


thinking
**Inspecting remaining prompt.rs content**
exec
/bin/zsh -lc "sed -n '320,620p' src/prompts/chat/prompt.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 50ms:
    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
        self.builtin_system_prompt = Some(prompt.into());
        self
    }

    /// Enable script generation mode, which shows Save/Run actions after responses complete.
    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
        self.script_generation_mode = enabled;
        self
    }

    /// Set pending_submit flag - when true, auto-submit input on first render
    /// Used for Tab from main menu to immediately send the query to AI
    pub fn with_pending_submit(mut self, submit: bool) -> Self {
        self.pending_submit = submit;
        self
    }

    /// Set needs_initial_response flag - when true, auto-respond to initial messages on first render
    /// Used for scriptlets that call chat() with pre-populated messages
file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..f38da4434a11c5693a2baaca2ad71f17d6e04ad8
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,50 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;


thinking
**Investigating button component usage**
exec
/bin/zsh -lc "sed -n '1,320p' src/prompts/chat/render_input.rs" in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc "sed -n '1,320p' src/prompts/chat/render_turns.rs" in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc "rg -n \"on_click\\(|render_.*button|button\" src/prompts/chat -g'*.rs'" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 50ms:
use super::*;

impl ChatPrompt {
    pub(super) fn render_input(&self, is_focused: bool) -> impl IntoElement {
        let colors = &self.prompt_colors;
        let theme_colors = &self.theme.colors;
        let text = self.input.text();
        let cursor_pos = self.input.cursor();
        let chars: Vec<char> = text.chars().collect();
        let cursor_visible = self.cursor_visible && is_focused;

        let mut input_content = div()
            .flex()
            .flex_row()
            .items_center()
            .w_full()
            .text_size(px(14.0));

        // Text before cursor
        if cursor_pos > 0 {
 succeeded in 50ms:
use super::*;

impl ChatPrompt {
    pub(super) fn render_turn(
        &self,
        turn: &ConversationTurn,
        turn_index: usize,
        cx: &Context<Self>,
    ) -> impl IntoElement {
        let colors = &self.prompt_colors;

        // VIBRANCY: Use theme-aware overlay for subtle lift that lets blur show through
        // Dark mode: white overlay brightens; Light mode: much subtler black overlay
        let container_bg = if self.theme.is_dark_mode() {
            theme::hover_overlay_bg(&self.theme, 0x15) // ~8% white overlay for dark mode
        } else {
            theme::hover_overlay_bg(&self.theme, 0x08) // ~3% black overlay for light mode
        };
        let copy_hover_bg = theme::hover_overlay_bg(&self.theme, 0x28); // ~16% for hover
        let error_color = self.theme.colors.ui.error;
 succeeded in 51ms:
src/prompts/chat/render_input.rs:84:    /// Render the header with back button and title
src/prompts/chat/render_turns.rs:52:        // Error state - show error message with optional retry button
src/prompts/chat/render_turns.rs:65:            // Add retry button if applicable
src/prompts/chat/render_turns.rs:81:                        .on_click(cx.listener(move |this, _, _window, _cx| {
src/prompts/chat/render_turns.rs:143:        // Copy button (appears on right side) - copies assistant response
src/prompts/chat/render_turns.rs:144:        let copy_button = div()
src/prompts/chat/render_turns.rs:161:            .on_click(cx.listener(move |this, _, _window, cx| {
src/prompts/chat/render_turns.rs:165:        // The full-width container with copy button
src/prompts/chat/render_turns.rs:176:            .child(copy_button)
src/prompts/chat/render_setup.rs:65:                    .id("setup-buttons-container")
src/prompts/chat/render_setup.rs:74:                            .id("configure-button")
src/prompts/chat/render_setup.rs:103:                            .on_click(cx.listener(move |_this, _event, _window, _cx| {
src/prompts/chat/render_setup.rs:161:                            .on_click(cx.listener(move |_this, _event, _window, _cx| {
src/prompts/chat/tests.rs:53:    fn resolve_setup_card_key_activates_buttons_and_escape() {
src/prompts/chat/render_core.rs:96:            // Setup mode: keyboard navigation for Configure / Claude Code buttons
src/prompts/chat/render_core.rs:199:                // Header with back button and title
src/prompts/chat/render_core.rs:338:                                    .id("chat-scroll-to-latest-button")
src/prompts/chat/render_core.rs:347:                                    .on_click(cx.listener(|this, _event, _window, cx| {
src/prompts/chat/render_core.rs:384:            // Header with back button and title
src/prompts/chat/state.rs:275:                            .on_click(cx.listener(move |this, _, _window, cx| {
file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..f38da4434a11c5693a2baaca2ad71f17d6e04ad8
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,50 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;


thinking
**Reviewing UI reuse and AI integration**
exec
/bin/zsh -lc "rg -n \"generate_script_from_ai_prompt|show_inline_ai_chat|Shift\\+Tab|with_pending_submit|ChatPrompt\" src/app_impl/startup.rs src/app_impl/startup_new_tab.rs src/app_impl/prompt_ai.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
src/app_impl/prompt_ai.rs:346:    /// This switches to the ChatPrompt view with direct AI integration (no SDK needed).
src/app_impl/prompt_ai.rs:348:    pub fn show_inline_ai_chat(&mut self, initial_query: Option<String>, cx: &mut Context<Self>) {
src/app_impl/prompt_ai.rs:350:        use crate::prompts::{ChatEscapeCallback, ChatPrompt, ChatSubmitCallback};
src/app_impl/prompt_ai.rs:391:            let chat_prompt = ChatPrompt::new(
src/app_impl/prompt_ai.rs:409:            self.current_view = AppView::ChatPrompt {
src/app_impl/prompt_ai.rs:414:            self.pending_focus = Some(FocusTarget::ChatPrompt);
src/app_impl/prompt_ai.rs:435:        let mut chat_prompt = ChatPrompt::new(
src/app_impl/prompt_ai.rs:453:            chat_prompt = chat_prompt.with_pending_submit(true);
src/app_impl/prompt_ai.rs:457:        self.current_view = AppView::ChatPrompt {
src/app_impl/prompt_ai.rs:462:        self.pending_focus = Some(FocusTarget::ChatPrompt);
src/app_impl/prompt_ai.rs:469:    pub fn generate_script_from_ai_prompt(
src/app_impl/startup.rs:480:            // Inline chat escape channel - for ChatPrompt escape callback to signal return to main menu
src/app_impl/startup.rs:483:            // Inline chat configure channel - for ChatPrompt configure callback to trigger API key setup
src/app_impl/startup.rs:486:            // Inline chat Claude Code channel - for ChatPrompt Claude Code callback to enable Claude Code
src/app_impl/startup.rs:580:                            // Handle Tab/Shift+Tab in FileSearchView for directory/file navigation
src/app_impl/startup.rs:581:                            // CRITICAL: ALWAYS consume Tab/Shift+Tab to prevent focus traversal
src/app_impl/startup.rs:587:                                // ALWAYS stop propagation for Tab/Shift+Tab in FileSearchView
src/app_impl/startup.rs:592:                                    // Shift+Tab: Go up one directory level using parent_dir_display helper
src/app_impl/startup.rs:600:                                                "Shift+Tab: Navigating up from '{}' to '{}'",
src/app_impl/startup.rs:625:                                                "Shift+Tab: Already at root '{}', no-op",
file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..f38da4434a11c5693a2baaca2ad71f17d6e04ad8
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,50 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;


thinking
**Inspecting AI prompt handling**
exec
/bin/zsh -lc "sed -n '300,620p' src/app_impl/prompt_ai.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 53ms:
    pub(crate) fn create_box_shadows(&self) -> Vec<BoxShadow> {
        let shadow_config = self.theme.get_drop_shadow();

        if !shadow_config.enabled {
            return vec![];
        }

        // Convert hex color to HSLA
        // For black (0x000000), we use h=0, s=0, l=0
        let r = ((shadow_config.color >> 16) & 0xFF) as f32 / 255.0;
        let g = ((shadow_config.color >> 8) & 0xFF) as f32 / 255.0;
        let b = (shadow_config.color & 0xFF) as f32 / 255.0;

        // Simple RGB to HSL conversion for shadow color
        let max = r.max(g).max(b);
        let min = r.min(g).min(b);
        let l = (max + min) / 2.0;

        let (h, s) = if max == min {
            (0.0, 0.0) // achromatic
file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..f38da4434a11c5693a2baaca2ad71f17d6e04ad8
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,50 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;


thinking
**Assessing removal of deprecated helpers**
exec
/bin/zsh -lc "rg -n \"generate_script_from_ai_prompt\\(\" src | head -n 20" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
src/app_impl/prompt_ai.rs:469:    pub fn generate_script_from_ai_prompt(
src/app_impl/tests.rs:58:                && startup_tab.contains("this.generate_script_from_ai_prompt(query, cx);"),
src/app_impl/startup.rs:737:                                    this.generate_script_from_ai_prompt(query, cx);
src/app_impl/startup_new_tab.rs:188:                                    this.generate_script_from_ai_prompt(query, cx);
file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..f38da4434a11c5693a2baaca2ad71f17d6e04ad8
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,50 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;


thinking
**Reviewing tests for impact**
exec
/bin/zsh -lc "sed -n '1,220p' src/app_impl/tests.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 52ms:
#[cfg(test)]
mod app_impl_state_sync_tests {
    use super::{calculate_fallback_error_message, ScriptListApp};
    use std::fs;

    #[test]
    fn test_sync_builtin_query_state_updates_query_and_selection_when_changed() {
        let mut query = String::from("old");
        let mut selected_index = 3;

        let changed =
            ScriptListApp::sync_builtin_query_state(&mut query, &mut selected_index, "new");

        assert!(changed);
        assert_eq!(query, "new");
        assert_eq!(selected_index, 0);
    }

    #[test]
    fn test_sync_builtin_query_state_noop_when_query_is_unchanged() {
file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..f38da4434a11c5693a2baaca2ad71f17d6e04ad8
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,50 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;


thinking
**Weighing test update scope**
exec
/bin/zsh -lc "sed -n '420,840p' src/prompts/chat/streaming.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 52ms:
                                chat.builtin_streaming_content = full_text.clone();
                                chat.builtin_accumulated_content = full_text.clone();
                                chat.mark_conversation_turns_dirty();
                                chat.scroll_turns_to_bottom();
                                cx.notify();
                            } else if let Some(new_offset) =
                                next_reveal_boundary(&full_text, current_offset)
                            {
                                if new_offset > current_offset {
                                    chat.builtin_reveal_offset = new_offset;
                                    let revealed = &full_text[..new_offset];

                                    if let Some(msg) = chat
                                        .messages
                                        .iter_mut()
                                        .find(|m| m.id.as_deref() == Some(&msg_id))
                                    {
                                        msg.set_content(revealed);
                                    }
                                    chat.builtin_streaming_content = revealed.to_string();
file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..f38da4434a11c5693a2baaca2ad71f17d6e04ad8
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,50 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;


thinking
**Confirming no syntax errors**
exec
/bin/zsh -lc "sed -n '1,300p' src/app_impl/startup_new_tab.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 50ms:
        // Add Tab key interceptor for "Ask AI" feature and file search directory navigation
        // This fires BEFORE normal key handling, allowing us to intercept Tab
        // even when the Input component has focus
        let app_entity_for_tab = cx.entity().downgrade();
        let tab_interceptor = cx.intercept_keystrokes({
            let app_entity = app_entity_for_tab;
            move |event, window, cx| {
                let key = event.keystroke.key.to_lowercase();
                let is_tab_key = matches!(event.keystroke.key.as_str(), "tab" | "Tab")
                    || matches!(key.as_str(), "tab");
                let has_shift = event.keystroke.modifiers.shift;
                // Check for Tab key (no cmd/alt/ctrl modifiers, but shift is allowed)
                if is_tab_key
                    && !event.keystroke.modifiers.platform
                    && !event.keystroke.modifiers.alt
                    && !event.keystroke.modifiers.control
                {
                    if let Some(app) = app_entity.upgrade() {
                        app.update(cx, |this, cx| {
                            // FIRST: If confirm dialog is open, route Tab to it for button switching
file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..f38da4434a11c5693a2baaca2ad71f17d6e04ad8
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,50 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;


thinking
**Planning script generation integration**
codex
I’ve identified where to hook the new flow: `show_inline_ai_chat`/`generate_script_from_ai_prompt` in `prompt_ai.rs`, Shift+Tab interceptors in both startup files, and `render_core` + `actions` in `prompts/chat` for post-generation buttons. I’m now checking `script_creation` execution helpers so Save/Run can reuse existing behavior instead of duplicating it.
exec
/bin/zsh -lc "rg -n \"pub fn create_new_script|open_in_editor|run|execute\" src/script_creation* src -g'*script_creation*.rs'" in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc "sed -n '1,320p' src/script_creation.rs" in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc 'rg -n "run_saved_script_detached|save_generated_script_from_response|prepare_script_from_ai_response" src' in /Users/johnlindquist/dev/script-kit-gpui exited 1 in 51ms:

 exited 1 in 52ms:
sed: src/script_creation.rs: No such file or directory
 succeeded in 52ms:
src/ai/script_generation.rs:79:    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
src/ai/script_generation.rs:104:pub(crate) fn prepare_script_from_ai_response(
src/ai/script_generation.rs:123:pub(crate) fn save_generated_script_from_response(
src/ai/script_generation.rs:127:    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
src/ai/script_generation.rs:145:pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
src/ai/script_generation.rs:495:    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
src/ai/script_generation.rs:499:        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
src/ai/script_generation.rs:508:    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
src/ai/script_generation.rs:516:        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..f38da4434a11c5693a2baaca2ad71f17d6e04ad8
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,50 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;


thinking
**Searching for script file**
exec
/bin/zsh -lc 'rg --files src | rg "script_creation|create_new_script|open_in_editor"' in /Users/johnlindquist/dev/script-kit-gpui succeeded in 50ms:
src/script_creation/part_000.rs
src/script_creation/part_001.rs
src/script_creation/mod.rs
file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..f38da4434a11c5693a2baaca2ad71f17d6e04ad8
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,50 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;


thinking
**Planning module inspection**
exec
/bin/zsh -lc "sed -n '1,260p' src/script_creation/mod.rs" in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc "sed -n '1,360p' src/script_creation/part_000.rs" in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc "sed -n '1,360p' src/script_creation/part_001.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 50ms:
//! Script and Extension Creation Module
//!
//! This module provides functions to create new scripts and extensions
//! in the Script Kit environment, as well as opening files in the configured editor.
//!
//! # Usage
//!
//! ```rust,ignore
//! use script_kit_gpui::script_creation::{create_new_script, create_new_extension, open_in_editor};
//! use script_kit_gpui::config::Config;
//!
//! // Create a new script
//! let script_path = create_new_script("my-script")?;
//!
//! // Create a new extension
//! let extension_path = create_new_extension("my-extension")?;
//!
//! // Open in editor
//! let config = Config::default();
//! open_in_editor(&script_path, &config)?;
 succeeded in 53ms:
#[cfg(test)]
mod tests {
    use super::*;
    use std::env;
    use tempfile::tempdir;

    #[test]
    fn test_sanitize_name_basic() {
        assert_eq!(sanitize_name("hello"), "hello");
        assert_eq!(sanitize_name("Hello World"), "hello-world");
        assert_eq!(sanitize_name("my_script_name"), "my-script-name");
    }

    #[test]
    fn test_sanitize_name_special_chars() {
        assert_eq!(sanitize_name("hello@world!"), "helloworld");
        assert_eq!(sanitize_name("test#$%script"), "testscript");
        assert_eq!(sanitize_name("foo & bar"), "foo-bar");
    }

 succeeded in 53ms:
use crate::config::Config;
use anyhow::{Context, Result};
use std::fs::{self, OpenOptions};
use std::io::{ErrorKind, Write};
use std::path::{Path, PathBuf};
use std::process::Command;
use tracing::{info, instrument, warn};
/// Scripts directory under ~/.scriptkit/kit/main/
const SCRIPTS_DIR: &str = "~/.scriptkit/kit/main/scripts";
/// Extensions directory under ~/.scriptkit/kit/main/
const EXTENSIONS_DIR: &str = "~/.scriptkit/kit/main/extensions";
/// Maximum filename size on most filesystems (bytes, not chars).
const MAX_FILENAME_BYTES: usize = 255;
/// Reserved filenames on Windows that are invalid even with an extension.
const WINDOWS_RESERVED_FILENAMES: [&str; 22] = [
    "con", "prn", "aux", "nul", "com1", "com2", "com3", "com4", "com5", "com6", "com7", "com8",
    "com9", "lpt1", "lpt2", "lpt3", "lpt4", "lpt5", "lpt6", "lpt7", "lpt8", "lpt9",
];
fn scripts_dir() -> PathBuf {
    PathBuf::from(shellexpand::tilde(SCRIPTS_DIR).as_ref())
file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..f38da4434a11c5693a2baaca2ad71f17d6e04ad8
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,50 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;


thinking
**Inspecting script execution methods**
exec
/bin/zsh -lc "rg -n \"open_in_editor\\(|run.*script|kit run|bun run|execute.*script|spawn\\(\\).*script\" src | head -n 200" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 53ms:
src/builtins/part_000.rs:81:// SDK tileWindow() function still works via protocol messages in execute_script.rs.
src/prompt_handler/part_001.rs:541:                self.execute_interactive(&script, cx);
src/builtins/part_001_entries/entries_001.rs:280:    // SDK tileWindow() function still works via protocol messages in execute_script.rs
src/builtins/part_001_entries/entries_003.rs:123:        "Inspect running scripts and copy their process details",
src/builtins/part_001_entries/entries_003.rs:125:            "process", "running", "scripts", "jobs", "pid", "inspect", "manage", "kill",
src/prompt_handler/part_001_arms/arms_001.rs:143:                self.execute_interactive(&script, cx);
src/hotkeys/part_000.rs:31:    /// Script shortcut - run the script at this path
src/builtins/tests/part_001.rs:54:            "Stop all running scripts builtin should exist in the main menu"
src/app_actions/handle_action.rs:562:            "run_script" => {
src/app_actions/handle_action.rs:2506:                                        match executor::run_scriptlet(&action_scriptlet, options) {
src/app_actions/handle_action.rs:2546:                                                        "Failed to execute scriptlet action '{}': {}",
src/app_actions/handle_action/main_actions_06.rs:240:                                        match executor::run_scriptlet(&action_scriptlet, options) {
src/app_actions/handle_action/main_actions_06.rs:280:                                                        "Failed to execute scriptlet action '{}': {}",
src/app_actions/handle_action/main_actions_01.rs:18:            "run_script" => {
src/mcp_streaming/part_001.rs:110:                "tools/run_script",
src/mcp_streaming/part_001.rs:151:            .log_success("tools/run_script", params.clone(), 250)
src/mcp_streaming/part_001.rs:165:        assert_eq!(entry.method, "tools/run_script", "method must match");
src/executor_tests/chunk_10.rs:14:    let result = run_scriptlet(&scriptlet, ScriptletExecOptions::default());
src/executor_tests/chunk_10.rs:28:    let result = run_scriptlet(&scriptlet, ScriptletExecOptions::default());
src/executor_tests/chunk_10.rs:42:    let result = run_scriptlet(&scriptlet, ScriptletExecOptions::default());
file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..f38da4434a11c5693a2baaca2ad71f17d6e04ad8
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,50 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;


thinking
**Reviewing script execution and imports**
exec
/bin/zsh -lc "sed -n '1,260p' src/ai/script_generation.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
use anyhow::{Context, Result};
use std::fs::{self, OpenOptions};
use std::io::{ErrorKind, Write};
use std::path::{Path, PathBuf};
use std::sync::Arc;

use super::config::ModelInfo;
use super::providers::{AiProvider, ProviderMessage, ProviderRegistry};

const AI_SCRIPT_OUTPUT_DIR: &str = "~/.kenv/scripts";
const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;

pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.

Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.

Required output conventions:
1) Include metadata comments at the top:
   // Name: <clear title>
file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..f38da4434a11c5693a2baaca2ad71f17d6e04ad8
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,50 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;

exec
/bin/zsh -lc "sed -n '260,560p' src/ai/script_generation.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 52ms:
            slug.push(character);
            last_was_hyphen = false;
        } else if matches!(character, ' ' | '_' | '-') {
            if !slug.is_empty() && !last_was_hyphen {
                slug.push('-');
                last_was_hyphen = true;
            }
        }
    }

    while slug.ends_with('-') {
        slug.pop();
    }

    if slug.len() > AI_SCRIPT_MAX_SLUG_LEN {
        slug.truncate(AI_SCRIPT_MAX_SLUG_LEN);
        while slug.ends_with('-') {
            slug.pop();
        }
    }
file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..f38da4434a11c5693a2baaca2ad71f17d6e04ad8
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,50 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;


thinking
**Scoping UI and action integration**
exec
/bin/zsh -lc "sed -n '1,280p' src/prompts/chat/render_setup.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
use super::*;

impl ChatPrompt {
    pub(super) fn render_setup_card(&self, cx: &Context<Self>) -> impl IntoElement {
        let colors = &self.prompt_colors;

        let accent_full = rgba((colors.accent_color << 8) | 0xFF);
        let accent_25 = rgba((colors.accent_color << 8) | 0x40);
        let muted_bg = rgba((colors.code_bg << 8) | 0x60);
        let muted_bg_hover = rgba((colors.code_bg << 8) | 0x90);
        let ring_color = rgba((colors.accent_color << 8) | 0x80);
        let kbd_bg = rgba((colors.code_bg << 8) | 0x50);
        let accent_text = rgb(self.theme.colors.text.on_accent);

        let on_configure = self.on_configure.clone();
        let on_claude_code = self.on_claude_code.clone();
        let is_configure_focused = self.setup_focus_index == 0;
        let is_claude_focused = self.setup_focus_index == 1;

        div()
file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..f38da4434a11c5693a2baaca2ad71f17d6e04ad8
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,50 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;


thinking
**Planning action button integration**
exec
/bin/zsh -lc 'rg -n "inline_chat_.*receiver|inline_chat_.*sender|show_inline_ai_chat|ChatPrompt" src/app_impl/startup.rs' in /Users/johnlindquist/dev/script-kit-gpui succeeded in 52ms:
480:            // Inline chat escape channel - for ChatPrompt escape callback to signal return to main menu
481:            inline_chat_escape_sender: inline_chat_escape_tx,
482:            inline_chat_escape_receiver: inline_chat_escape_rx,
483:            // Inline chat configure channel - for ChatPrompt configure callback to trigger API key setup
484:            inline_chat_configure_sender: inline_chat_configure_tx,
485:            inline_chat_configure_receiver: inline_chat_configure_rx,
486:            // Inline chat Claude Code channel - for ChatPrompt Claude Code callback to enable Claude Code
487:            inline_chat_claude_code_sender: inline_chat_claude_code_tx,
488:            inline_chat_claude_code_receiver: inline_chat_claude_code_rx,
713:                            // Handle Tab/Shift+Tab in ChatPrompt setup mode
715:                            if let AppView::ChatPrompt { entity, .. } = &this.current_view {
740:                                    this.show_inline_ai_chat(Some(query), cx);
777:                            // This ensures ALL views (ChatPrompt, ArgPrompt, etc.) route
1213:                                AppView::ChatPrompt { .. } => {
1215:                                    logging::log("KEY", "Interceptor: Cmd+K -> toggle_chat_actions (ChatPrompt)");
1321:                                AppView::ChatPrompt { .. } => Some(ActionsDialogHost::ChatPrompt),
1347:                                            ActionsDialogHost::ChatPrompt => {
file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..f38da4434a11c5693a2baaca2ad71f17d6e04ad8
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,50 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;


thinking
**Searching receiver processing**
exec
/bin/zsh -lc 'rg -n "inline_chat_escape_receiver|inline_chat_configure_receiver|inline_chat_claude_code_receiver" src' in /Users/johnlindquist/dev/script-kit-gpui succeeded in 52ms:
src/main_sections/render_impl.rs:48:        if self.inline_chat_escape_receiver.try_recv().is_ok() {
src/main_sections/render_impl.rs:58:        if self.inline_chat_configure_receiver.try_recv().is_ok() {
src/main_sections/render_impl.rs:76:        if self.inline_chat_claude_code_receiver.try_recv().is_ok() {
src/app_impl/startup.rs:482:            inline_chat_escape_receiver: inline_chat_escape_rx,
src/app_impl/startup.rs:485:            inline_chat_configure_receiver: inline_chat_configure_rx,
src/app_impl/startup.rs:488:            inline_chat_claude_code_receiver: inline_chat_claude_code_rx,
src/main_sections/app_state.rs:263:    inline_chat_escape_receiver: mpsc::Receiver<()>,
src/main_sections/app_state.rs:269:    inline_chat_configure_receiver: mpsc::Receiver<()>,
src/main_sections/app_state.rs:275:    inline_chat_claude_code_receiver: mpsc::Receiver<()>,
src/app_impl/startup_new_state.rs:196:            inline_chat_escape_receiver: inline_chat_escape_rx,
src/app_impl/startup_new_state.rs:199:            inline_chat_configure_receiver: inline_chat_configure_rx,
src/app_impl/startup_new_state.rs:202:            inline_chat_claude_code_receiver: inline_chat_claude_code_rx,
file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..f38da4434a11c5693a2baaca2ad71f17d6e04ad8
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,50 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;


thinking
**Inspecting state clearing logic**
exec
/bin/zsh -lc "sed -n '1,340p' src/prompts/chat/state.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 50ms:
use super::*;

impl ChatPrompt {
    pub(super) fn mark_conversation_turns_dirty(&mut self) {
        self.conversation_turns_dirty = true;
    }

    pub(super) fn sync_turns_list_state(&mut self) {
        let item_count = self.conversation_turns_cache.len();
        let old_count = self.turns_list_state.item_count();
        if old_count != item_count {
            self.turns_list_state.splice(0..old_count, item_count);
        }
    }

    pub(super) fn ensure_conversation_turns_cache(&mut self) {
        if !self.conversation_turns_dirty {
            return;
        }
        self.conversation_turns_cache = Arc::new(build_conversation_turns(
file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..f38da4434a11c5693a2baaca2ad71f17d6e04ad8
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,50 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;


thinking
**Reviewing cleanup and error handling**
exec
/bin/zsh -lc "sed -n '1,220p' src/app_impl/prompt_ai.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 52ms:
use super::*;
use anyhow::{anyhow, Context as AnyhowContext, Result};

#[derive(Debug, Clone, Copy)]
enum AiScriptGenerationStage {
    SelectModel,
    ResolveProvider,
    RequestCompletion,
    ExtractScript,
    CreateScriptFile,
    WriteScriptFile,
    OpenEditor,
}

impl AiScriptGenerationStage {
    fn as_str(self) -> &'static str {
        match self {
            Self::SelectModel => "select_model",
            Self::ResolveProvider => "resolve_provider",
            Self::RequestCompletion => "request_completion",
file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..f38da4434a11c5693a2baaca2ad71f17d6e04ad8
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,50 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;


thinking
**Assessing helper removal and save path conflict**
exec
/bin/zsh -lc "rg -n \"~/.kenv/scripts|kenv/scripts|create_new_script\\(\" src | head -n 80" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 50ms:
src/setup/part_002.rs:215:/// - Moves ~/.kenv/scripts to ~/.scriptkit/kit/main/scripts
src/setup/part_002.rs:255:    // Move scripts from ~/.kenv/scripts to ~/.scriptkit/kit/main/scripts
src/script_creation/part_000.rs:252:pub fn create_new_script(name: &str) -> Result<PathBuf> {
src/script_creation/part_001.rs:86:        let result = create_new_script("");
src/script_creation/part_001.rs:96:        let result = create_new_script("@#$%^&*");
src/script_creation/mod.rs:13://! let script_path = create_new_script("my-script")?;
src/app_execute/builtin_execution.rs:1412:                        (script_creation::create_new_script("untitled"), "script")
src/app_execute/builtin_execution/feature_02.rs:310:                        (script_creation::create_new_script("untitled"), "script")
src/designs/core/tests/part_03.rs:286:        s.path = std::path::PathBuf::from("/Users/john/.kenv/scripts/secret-helper.ts");
src/ai/script_generation.rs:10:const AI_SCRIPT_OUTPUT_DIR: &str = "~/.kenv/scripts";
src/ai/script_generation.rs:128:    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
src/app_impl/prompt_ai.rs:166:    let script_path = crate::script_creation::create_new_script(&script_name).with_context(|| {
file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..f38da4434a11c5693a2baaca2ad71f17d6e04ad8
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,50 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;


thinking
**Reviewing script creation and chat callback integration**
exec
/bin/zsh -lc "rg -n \"set_on_show_actions\\(|with_builtin_system_prompt|with_script_generation_mode|on_show_actions\" src/app_impl src/prompts -g'*.rs'" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
src/prompts/path/prompt.rs:41:            on_show_actions: None,
src/prompts/path/prompt.rs:51:        self.on_show_actions = Some(callback);
src/prompts/path/prompt.rs:57:        self.on_show_actions = Some(callback);
src/prompts/path/prompt.rs:192:            if let Some(ref callback) = self.on_show_actions {
src/prompts/path/types.rs:79:    pub on_show_actions: Option<ShowActionsCallback>,
src/prompts/chat/prompt.rs:62:    pub(super) on_show_actions: Option<ChatShowActionsCallback>,
src/prompts/chat/prompt.rs:136:            on_show_actions: None,
src/prompts/chat/prompt.rs:145:    pub fn set_on_show_actions(&mut self, callback: ChatShowActionsCallback) {
src/prompts/chat/prompt.rs:146:        self.on_show_actions = Some(callback);
src/prompts/chat/prompt.rs:320:    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
src/prompts/chat/prompt.rs:326:    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
src/prompts/chat/render_core.rs:392:            // Note: Actions menu is now handled by parent via on_show_actions callback
src/prompts/chat/actions.rs:138:        if let Some(ref callback) = self.on_show_actions {
src/prompts/chat/actions.rs:142:            logging::log("CHAT", "No on_show_actions callback set");
file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..f38da4434a11c5693a2baaca2ad71f17d6e04ad8
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,50 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;


thinking
**Checking prompt creation callbacks**
exec
/bin/zsh -lc "rg -n \"ChatPrompt::new\\(|AppView::ChatPrompt|set_on_show_actions\\(\" src/app_impl src/prompt_handler src/main_sections -g'*.rs'" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
src/prompt_handler/part_001.rs:825:                    AppView::ChatPrompt { id, .. } => (
src/prompt_handler/part_001.rs:1678:                let mut chat_prompt = prompts::ChatPrompt::new(
src/prompt_handler/part_001.rs:1764:                self.current_view = AppView::ChatPrompt { id, entity };
src/prompt_handler/part_001.rs:1777:                if let AppView::ChatPrompt {
src/prompt_handler/part_001.rs:1798:                if let AppView::ChatPrompt {
src/prompt_handler/part_001.rs:1815:                if let AppView::ChatPrompt {
src/prompt_handler/part_001.rs:1832:                if let AppView::ChatPrompt {
src/prompt_handler/part_001.rs:1846:                if let AppView::ChatPrompt {
src/prompt_handler/part_001.rs:1867:                if let AppView::ChatPrompt {
src/prompt_handler/part_001.rs:1884:                if let AppView::ChatPrompt {
src/app_impl/startup_new_actions.rs:94:                                AppView::ChatPrompt { .. } => {
src/app_impl/startup_new_actions.rs:202:                                AppView::ChatPrompt { .. } => Some(ActionsDialogHost::ChatPrompt),
src/main_sections/render_impl.rs:197:            AppView::ChatPrompt { entity, .. } => {
src/prompt_handler/part_001_arms/arms_005.rs:3:                if let AppView::ChatPrompt {
src/prompt_handler/part_001_arms/arms_005.rs:24:                if let AppView::ChatPrompt {
src/prompt_handler/part_001_arms/arms_005.rs:41:                if let AppView::ChatPrompt {
src/prompt_handler/part_001_arms/arms_005.rs:58:                if let AppView::ChatPrompt {
src/prompt_handler/part_001_arms/arms_005.rs:72:                if let AppView::ChatPrompt {
src/prompt_handler/part_001_arms/arms_005.rs:93:                if let AppView::ChatPrompt {
src/prompt_handler/part_001_arms/arms_005.rs:110:                if let AppView::ChatPrompt {
file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..f38da4434a11c5693a2baaca2ad71f17d6e04ad8
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,50 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;


thinking
**Identifying action callback handling**
exec
/bin/zsh -lc "sed -n '1,220p' src/app_impl/chat_actions.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
use super::*;

impl ScriptListApp {
    pub fn execute_chat_action(&mut self, action_id: &str, cx: &mut Context<Self>) {
        logging::log("ACTIONS", &format!("execute_chat_action: {}", action_id));

        let action_id = action_id.strip_prefix("chat:").unwrap_or(action_id);

        // Handle model selection (action_id starts with "select_model_")
        if let Some(model_id) = action_id.strip_prefix("select_model_") {
            let mut selected_model_name: Option<String> = None;
            if let AppView::ChatPrompt { entity, .. } = &self.current_view {
                let model_id_owned = model_id.to_string();
                entity.update(cx, |chat, cx| {
                    // Find model by ID and set it
                    if let Some(model) = chat.models.iter().find(|m| m.id == model_id_owned) {
                        chat.model = Some(model.name.clone());
                        selected_model_name = Some(model.name.clone());
                        logging::log("CHAT", &format!("Model changed to: {}", model.name));
                        cx.notify();
file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..f38da4434a11c5693a2baaca2ad71f17d6e04ad8
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,50 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;


thinking
**Identifying UI callback location**
exec
/bin/zsh -lc "sed -n '150,300p' src/main_sections/render_impl.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:

        // P0-4: Clone current_view only for dispatch (needed to call &mut self methods)
        // The clone is unavoidable due to borrow checker: we need &mut self for render methods
        // but also need to match on self.current_view. Future optimization: refactor render
        // methods to take &str/&[T] references instead of owned values.
        //
        // HUD is now handled by hud_manager as a separate floating window
        // No need to render it as part of this view
        let current_view = self.current_view.clone();
        let main_content: AnyElement = match current_view {
            AppView::ScriptList => self.render_script_list(cx).into_any_element(),
            AppView::ActionsDialog => self.render_actions_dialog(cx),
            AppView::ArgPrompt {
                id,
                placeholder,
                choices,
                actions,
            } => self
                .render_arg_prompt(id, placeholder, choices, actions, cx)
                .into_any_element(),
file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..f38da4434a11c5693a2baaca2ad71f17d6e04ad8
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,50 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;


thinking
**Investigating prompt rendering location**
exec
/bin/zsh -lc "rg -n \"render_chat_prompt\\(|set_on_show_actions\" src/main_sections src/app_impl src/prompt_handler -g'*.rs'" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
src/main_sections/render_impl.rs:198:                self.render_chat_prompt(entity, cx).into_any_element()
file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..f38da4434a11c5693a2baaca2ad71f17d6e04ad8
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,50 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;


thinking
**Locating render method globally**
exec
/bin/zsh -lc "rg -n \"fn render_chat_prompt\" src -g'*.rs'" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
src/render_prompts/other.rs:179:    fn render_chat_prompt(
file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..f38da4434a11c5693a2baaca2ad71f17d6e04ad8
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,50 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;

exec
/bin/zsh -lc "sed -n '140,300p' src/render_prompts/other.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 50ms:
            .into_any_element()
    }

    fn render_drop_prompt(
        &mut self,
        entity: Entity<DropPrompt>,
        cx: &mut Context<Self>,
    ) -> AnyElement {
        let shell_radius = self.other_prompt_shell_radius_lg();
        let handle_key = cx.listener(Self::other_prompt_shell_handle_key_default);

        let vibrancy_bg = get_vibrancy_background(&self.theme);

        // DropPrompt entity has its own track_focus and on_key_down in its render method.
        // We wrap with our own handler to intercept Cmd+W and ESC first.
        crate::components::prompt_shell_container(shell_radius, vibrancy_bg)
            .on_key_down(handle_key)
            .child(crate::components::prompt_shell_content(entity))
            .into_any_element()
    }
file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..f38da4434a11c5693a2baaca2ad71f17d6e04ad8
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,50 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;


thinking
**Designing script generation integration**
exec
/bin/zsh -lc "rg -n \"generate_script_from_ai_prompt\\(|show_script_generation_chat\\(\" src --glob '*tests*.rs'" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 53ms:
src/app_impl/tests.rs:58:                && startup_tab.contains("this.generate_script_from_ai_prompt(query, cx);"),
file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..f38da4434a11c5693a2baaca2ad71f17d6e04ad8
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,50 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;


thinking
**Claiming additional source files**
codex
I’m moving to edits now: adding a dedicated `show_script_generation_chat` flow, wiring Shift+Tab to it, and implementing ChatPrompt Save/Run actions. I also need to update one app-level test that currently hardcodes the old Shift+Tab function call so verification reflects the new behavior.
exec
/bin/zsh -lc '~/.claude/bin/swarm.ts claim --wait --id codex-chat-prompt-script-gen src/app_impl/tests.rs' in /Users/johnlindquist/dev/script-kit-gpuiexec
/bin/zsh -lc '~/.claude/bin/swarm.ts claim --wait --id codex-chat-prompt-script-gen src/prompts/chat/actions.rs' in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
Claimed for codex-chat-prompt-script-gen: src/app_impl/tests.rs
 succeeded in 52ms:
Claimed for codex-chat-prompt-script-gen: src/prompts/chat/actions.rs
file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..f38da4434a11c5693a2baaca2ad71f17d6e04ad8
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,50 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;


thinking
**Planning multi-file patch application**
file update
M /Users/johnlindquist/dev/script-kit-gpui/src/app_impl/prompt_ai.rs
@@ -466,2 +466,141 @@
 
+    /// Show script generation chat prompt with built-in AI provider support.
+    /// This opens ChatPrompt in script-generation mode with the script generation system prompt.
+    pub fn show_script_generation_chat(
+        &mut self,
+        initial_query: Option<String>,
+        cx: &mut Context<Self>,
+    ) {
+        use crate::ai::ProviderRegistry;
+        use crate::prompts::{ChatEscapeCallback, ChatPrompt, ChatSubmitCallback};
+
+        // Mark as opened from main menu so ESC returns to main menu
+        self.opened_from_main_menu = true;
+
+        // Create escape callback that signals via channel
+        let escape_sender = self.inline_chat_escape_sender.clone();
+        let escape_callback: ChatEscapeCallback = std::sync::Arc::new(move |_id| {
+            let _ = escape_sender.try_send(());
+        });
+
+        // Use cached registry if available, otherwise build synchronously as fallback
+        let registry = self
+            .cached_provider_registry
+            .clone()
+            .unwrap_or_else(|| ProviderRegistry::from_environment_with_config(Some(&self.config)));
+
+        if !registry.has_any_provider() {
+            crate::logging::log(
+                "CHAT_SCRIPT_GEN",
+                "No AI providers configured - showing setup card",
+            );
+
+            // Create configure callback that signals via channel
+            let configure_sender = self.inline_chat_configure_sender.clone();
+            let configure_callback: crate::prompts::ChatConfigureCallback =
+                std::sync::Arc::new(move || {
+                    crate::logging::log(
+                        "CHAT_SCRIPT_GEN",
+                        "Configure callback triggered - sending signal",
+                    );
+                    let _ = configure_sender.try_send(());
+                });
+
+            // Create Claude Code callback that signals via channel
+            let claude_code_sender = self.inline_chat_claude_code_sender.clone();
+            let claude_code_callback: crate::prompts::ChatClaudeCodeCallback =
+                std::sync::Arc::new(move || {
+                    crate::logging::log(
+                        "CHAT_SCRIPT_GEN",
+                        "Claude Code callback triggered - sending signal",
+                    );
+                    let _ = claude_code_sender.try_send(());
+                });
+
+            // Create a no-op submit callback since we're in setup mode
+            let noop_callback: ChatSubmitCallback = std::sync::Arc::new(|_id, _text| {
+                crate::logging::log(
+                    "CHAT_SCRIPT_GEN",
+                    "No providers - submission ignored (setup mode)",
+                );
+            });
+
+            let chat_prompt = ChatPrompt::new(
+                "script-generation-setup".to_string(),
+                Some("Configure API key to generate scripts...".to_string()),
+                vec![],
+                None, // No hint needed - setup card is the UI
+                None,
+                self.focus_handle.clone(),
+                noop_callback,
+                std::sync::Arc::clone(&self.theme),
+            )
+            .with_title("Generate Script with AI")
+            .with_save_history(false) // Don't save setup state to history
+            .with_escape_callback(escape_callback.clone())
+            .with_needs_setup(true)
+            .with_configure_callback(configure_callback)
+            .with_claude_code_callback(claude_code_callback);
+
+            let entity = cx.new(|_| chat_prompt);
+            self.current_view = AppView::ChatPrompt {
+                id: "script-generation-setup".to_string(),
+                entity,
+            };
+            self.focused_input = FocusedInput::None;
+            self.pending_focus = Some(FocusTarget::ChatPrompt);
+            resize_to_view_sync(ViewType::DivPrompt, 0);
+            cx.notify();
+            return;
+        }
+
+        crate::logging::log(
+            "CHAT_SCRIPT_GEN",
+            &format!(
+                "Showing script generation chat with {} providers",
+                registry.provider_ids().len()
+            ),
+        );
+
+        // Create a no-op callback since built-in AI handles submissions internally
+        let noop_callback: ChatSubmitCallback = std::sync::Arc::new(|_id, _text| {
+            // Built-in AI mode handles this internally
+        });
+
+        let placeholder = Some("Describe the script you want to generate...".to_string());
+
+        let mut chat_prompt = ChatPrompt::new(
+            "script-generation".to_string(),
+            placeholder,
+            vec![],
+            None,
+            None,
+            self.focus_handle.clone(),
+            noop_callback,
+            std::sync::Arc::clone(&self.theme),
+        )
+        .with_title("Generate Script with AI")
+        .with_save_history(false)
+        .with_escape_callback(escape_callback)
+        .with_builtin_ai(registry, true) // true = prefer Vercel AI Gateway
+        .with_builtin_system_prompt(crate::ai::script_generation::AI_SCRIPT_GENERATION_SYSTEM_PROMPT)
+        .with_script_generation_mode(true);
+
+        // If there's an initial query, set it in the input and auto-submit
+        if let Some(query) = initial_query {
+            chat_prompt.input.set_text(&query);
+            chat_prompt = chat_prompt.with_pending_submit(true);
+        }
+
+        let entity = cx.new(|_| chat_prompt);
+        self.current_view = AppView::ChatPrompt {
+            id: "script-generation".to_string(),
+            entity,
+        };
+        self.focused_input = FocusedInput::None;
+        self.pending_focus = Some(FocusTarget::ChatPrompt);
+        resize_to_view_sync(ViewType::DivPrompt, 0);
+        cx.notify();
+    }
+
     /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
apply_patch(auto_approved=true) exited 0 in 26ms:
Success. Updated the following files:
M /Users/johnlindquist/dev/script-kit-gpui/src/app_impl/prompt_ai.rs
file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index c985a5090574816267103019d1b272e1e3151a38..7f8acfae4036d0f77ae1eca73994c8d188bfb44e
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -464,6 +464,145 @@
         cx.notify();
     }
 
+    /// Show script generation chat prompt with built-in AI provider support.
+    /// This opens ChatPrompt in script-generation mode with the script generation system prompt.
+    pub fn show_script_generation_chat(
+        &mut self,
+        initial_query: Option<String>,
+        cx: &mut Context<Self>,
+    ) {
+        use crate::ai::ProviderRegistry;
+        use crate::prompts::{ChatEscapeCallback, ChatPrompt, ChatSubmitCallback};
+
+        // Mark as opened from main menu so ESC returns to main menu
+        self.opened_from_main_menu = true;
+
+        // Create escape callback that signals via channel
+        let escape_sender = self.inline_chat_escape_sender.clone();
+        let escape_callback: ChatEscapeCallback = std::sync::Arc::new(move |_id| {
+            let _ = escape_sender.try_send(());
+        });
+
+        // Use cached registry if available, otherwise build synchronously as fallback
+        let registry = self
+            .cached_provider_registry
+            .clone()
+            .unwrap_or_else(|| ProviderRegistry::from_environment_with_config(Some(&self.config)));
+
+        if !registry.has_any_provider() {
+            crate::logging::log(
+                "CHAT_SCRIPT_GEN",
+                "No AI providers configured - showing setup card",
+            );
+
+            // Create configure callback that signals via channel
+            let configure_sender = self.inline_chat_configure_sender.clone();
+            let configure_callback: crate::prompts::ChatConfigureCallback =
+                std::sync::Arc::new(move || {
+                    crate::logging::log(
+                        "CHAT_SCRIPT_GEN",
+                        "Configure callback triggered - sending signal",
+                    );
+                    let _ = configure_sender.try_send(());
+                });
+
+            // Create Claude Code callback that signals via channel
+            let claude_code_sender = self.inline_chat_claude_code_sender.clone();
+            let claude_code_callback: crate::prompts::ChatClaudeCodeCallback =
+                std::sync::Arc::new(move || {
+                    crate::logging::log(
+                        "CHAT_SCRIPT_GEN",
+                        "Claude Code callback triggered - sending signal",
+                    );
+                    let _ = claude_code_sender.try_send(());
+                });
+
+            // Create a no-op submit callback since we're in setup mode
+            let noop_callback: ChatSubmitCallback = std::sync::Arc::new(|_id, _text| {
+                crate::logging::log(
+                    "CHAT_SCRIPT_GEN",
+                    "No providers - submission ignored (setup mode)",
+                );
+            });
+
+            let chat_prompt = ChatPrompt::new(
+                "script-generation-setup".to_string(),
+                Some("Configure API key to generate scripts...".to_string()),
+                vec![],
+                None, // No hint needed - setup card is the UI
+                None,
+                self.focus_handle.clone(),
+                noop_callback,
+                std::sync::Arc::clone(&self.theme),
+            )
+            .with_title("Generate Script with AI")
+            .with_save_history(false) // Don't save setup state to history
+            .with_escape_callback(escape_callback.clone())
+            .with_needs_setup(true)
+            .with_configure_callback(configure_callback)
+            .with_claude_code_callback(claude_code_callback);
+
+            let entity = cx.new(|_| chat_prompt);
+            self.current_view = AppView::ChatPrompt {
+                id: "script-generation-setup".to_string(),
+                entity,
+            };
+            self.focused_input = FocusedInput::None;
+            self.pending_focus = Some(FocusTarget::ChatPrompt);
+            resize_to_view_sync(ViewType::DivPrompt, 0);
+            cx.notify();
+            return;
+        }
+
+        crate::logging::log(
+            "CHAT_SCRIPT_GEN",
+            &format!(
+                "Showing script generation chat with {} providers",
+                registry.provider_ids().len()
+            ),
+        );
+
+        // Create a no-op callback since built-in AI handles submissions internally
+        let noop_callback: ChatSubmitCallback = std::sync::Arc::new(|_id, _text| {
+            // Built-in AI mode handles this internally
+        });
+
+        let placeholder = Some("Describe the script you want to generate...".to_string());
+
+        let mut chat_prompt = ChatPrompt::new(
+            "script-generation".to_string(),
+            placeholder,
+            vec![],
+            None,
+            None,
+            self.focus_handle.clone(),
+            noop_callback,
+            std::sync::Arc::clone(&self.theme),
+        )
+        .with_title("Generate Script with AI")
+        .with_save_history(false)
+        .with_escape_callback(escape_callback)
+        .with_builtin_ai(registry, true) // true = prefer Vercel AI Gateway
+        .with_builtin_system_prompt(crate::ai::script_generation::AI_SCRIPT_GENERATION_SYSTEM_PROMPT)
+        .with_script_generation_mode(true);
+
+        // If there's an initial query, set it in the input and auto-submit
+        if let Some(query) = initial_query {
+            chat_prompt.input.set_text(&query);
+            chat_prompt = chat_prompt.with_pending_submit(true);
+        }
+
+        let entity = cx.new(|_| chat_prompt);
+        self.current_view = AppView::ChatPrompt {
+            id: "script-generation".to_string(),
+            entity,
+        };
+        self.focused_input = FocusedInput::None;
+        self.pending_focus = Some(FocusTarget::ChatPrompt);
+        resize_to_view_sync(ViewType::DivPrompt, 0);
+        cx.notify();
+    }
+
     /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
     /// The generated script is saved to disk and opened in the configured editor.
     pub fn generate_script_from_ai_prompt(
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..f38da4434a11c5693a2baaca2ad71f17d6e04ad8
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,50 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;

file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index c985a5090574816267103019d1b272e1e3151a38..7f8acfae4036d0f77ae1eca73994c8d188bfb44e
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -464,6 +464,145 @@
         cx.notify();
     }
 
+    /// Show script generation chat prompt with built-in AI provider support.
+    /// This opens ChatPrompt in script-generation mode with the script generation system prompt.
+    pub fn show_script_generation_chat(
+        &mut self,
+        initial_query: Option<String>,
+        cx: &mut Context<Self>,
+    ) {
+        use crate::ai::ProviderRegistry;
+        use crate::prompts::{ChatEscapeCallback, ChatPrompt, ChatSubmitCallback};
+
+        // Mark as opened from main menu so ESC returns to main menu
+        self.opened_from_main_menu = true;
+
+        // Create escape callback that signals via channel
+        let escape_sender = self.inline_chat_escape_sender.clone();
+        let escape_callback: ChatEscapeCallback = std::sync::Arc::new(move |_id| {
+            let _ = escape_sender.try_send(());
+        });
+
+        // Use cached registry if available, otherwise build synchronously as fallback
+        let registry = self
+            .cached_provider_registry
+            .clone()
+            .unwrap_or_else(|| ProviderRegistry::from_environment_with_config(Some(&self.config)));
+
+        if !registry.has_any_provider() {
+            crate::logging::log(
+                "CHAT_SCRIPT_GEN",
+                "No AI providers configured - showing setup card",
+            );
+
+            // Create configure callback that signals via channel
+            let configure_sender = self.inline_chat_configure_sender.clone();
+            let configure_callback: crate::prompts::ChatConfigureCallback =
+                std::sync::Arc::new(move || {
+                    crate::logging::log(
+                        "CHAT_SCRIPT_GEN",
+                        "Configure callback triggered - sending signal",
+                    );
+                    let _ = configure_sender.try_send(());
+                });
+
+            // Create Claude Code callback that signals via channel
+            let claude_code_sender = self.inline_chat_claude_code_sender.clone();
+            let claude_code_callback: crate::prompts::ChatClaudeCodeCallback =
+                std::sync::Arc::new(move || {
+                    crate::logging::log(
+                        "CHAT_SCRIPT_GEN",
+                        "Claude Code callback triggered - sending signal",
+                    );
+                    let _ = claude_code_sender.try_send(());
+                });
+
+            // Create a no-op submit callback since we're in setup mode
+            let noop_callback: ChatSubmitCallback = std::sync::Arc::new(|_id, _text| {
+                crate::logging::log(
+                    "CHAT_SCRIPT_GEN",
+                    "No providers - submission ignored (setup mode)",
+                );
+            });
+
+            let chat_prompt = ChatPrompt::new(
+                "script-generation-setup".to_string(),
+                Some("Configure API key to generate scripts...".to_string()),
+                vec![],
+                None, // No hint needed - setup card is the UI
+                None,
+                self.focus_handle.clone(),
+                noop_callback,
+                std::sync::Arc::clone(&self.theme),
+            )
+            .with_title("Generate Script with AI")
+            .with_save_history(false) // Don't save setup state to history
+            .with_escape_callback(escape_callback.clone())
+            .with_needs_setup(true)
+            .with_configure_callback(configure_callback)
+            .with_claude_code_callback(claude_code_callback);
+
+            let entity = cx.new(|_| chat_prompt);
+            self.current_view = AppView::ChatPrompt {
+                id: "script-generation-setup".to_string(),
+                entity,
+            };
+            self.focused_input = FocusedInput::None;
+            self.pending_focus = Some(FocusTarget::ChatPrompt);
+            resize_to_view_sync(ViewType::DivPrompt, 0);
+            cx.notify();
+            return;
+        }
+
+        crate::logging::log(
+            "CHAT_SCRIPT_GEN",
+            &format!(
+                "Showing script generation chat with {} providers",
+                registry.provider_ids().len()
+            ),
+        );
+
+        // Create a no-op callback since built-in AI handles submissions internally
+        let noop_callback: ChatSubmitCallback = std::sync::Arc::new(|_id, _text| {
+            // Built-in AI mode handles this internally
+        });
+
+        let placeholder = Some("Describe the script you want to generate...".to_string());
+
+        let mut chat_prompt = ChatPrompt::new(
+            "script-generation".to_string(),
+            placeholder,
+            vec![],
+            None,
+            None,
+            self.focus_handle.clone(),
+            noop_callback,
+            std::sync::Arc::clone(&self.theme),
+        )
+        .with_title("Generate Script with AI")
+        .with_save_history(false)
+        .with_escape_callback(escape_callback)
+        .with_builtin_ai(registry, true) // true = prefer Vercel AI Gateway
+        .with_builtin_system_prompt(crate::ai::script_generation::AI_SCRIPT_GENERATION_SYSTEM_PROMPT)
+        .with_script_generation_mode(true);
+
+        // If there's an initial query, set it in the input and auto-submit
+        if let Some(query) = initial_query {
+            chat_prompt.input.set_text(&query);
+            chat_prompt = chat_prompt.with_pending_submit(true);
+        }
+
+        let entity = cx.new(|_| chat_prompt);
+        self.current_view = AppView::ChatPrompt {
+            id: "script-generation".to_string(),
+            entity,
+        };
+        self.focused_input = FocusedInput::None;
+        self.pending_focus = Some(FocusTarget::ChatPrompt);
+        resize_to_view_sync(ViewType::DivPrompt, 0);
+        cx.notify();
+    }
+
     /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
     /// The generated script is saved to disk and opened in the configured editor.
     pub fn generate_script_from_ai_prompt(
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..f38da4434a11c5693a2baaca2ad71f17d6e04ad8
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,50 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;


thinking
**Adding startup comments and calls**
file update
M /Users/johnlindquist/dev/script-kit-gpui/src/app_impl/startup.rs
@@ -725,3 +725,3 @@
                             // Handle Tab/Shift+Tab in ScriptList view for AI actions.
-                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
+                            // Tab opens Ask AI chat, Shift+Tab opens script generation chat mode.
                             if matches!(this.current_view, AppView::ScriptList)
@@ -736,3 +736,3 @@
                                 if has_shift {
-                                    this.generate_script_from_ai_prompt(query, cx);
+                                    this.show_script_generation_chat(Some(query), cx);
                                 } else {
apply_patch(auto_approved=true) exited 0 in 32ms:
Success. Updated the following files:
M /Users/johnlindquist/dev/script-kit-gpui/src/app_impl/startup.rs
file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index c985a5090574816267103019d1b272e1e3151a38..7f8acfae4036d0f77ae1eca73994c8d188bfb44e
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -464,6 +464,145 @@
         cx.notify();
     }
 
+    /// Show script generation chat prompt with built-in AI provider support.
+    /// This opens ChatPrompt in script-generation mode with the script generation system prompt.
+    pub fn show_script_generation_chat(
+        &mut self,
+        initial_query: Option<String>,
+        cx: &mut Context<Self>,
+    ) {
+        use crate::ai::ProviderRegistry;
+        use crate::prompts::{ChatEscapeCallback, ChatPrompt, ChatSubmitCallback};
+
+        // Mark as opened from main menu so ESC returns to main menu
+        self.opened_from_main_menu = true;
+
+        // Create escape callback that signals via channel
+        let escape_sender = self.inline_chat_escape_sender.clone();
+        let escape_callback: ChatEscapeCallback = std::sync::Arc::new(move |_id| {
+            let _ = escape_sender.try_send(());
+        });
+
+        // Use cached registry if available, otherwise build synchronously as fallback
+        let registry = self
+            .cached_provider_registry
+            .clone()
+            .unwrap_or_else(|| ProviderRegistry::from_environment_with_config(Some(&self.config)));
+
+        if !registry.has_any_provider() {
+            crate::logging::log(
+                "CHAT_SCRIPT_GEN",
+                "No AI providers configured - showing setup card",
+            );
+
+            // Create configure callback that signals via channel
+            let configure_sender = self.inline_chat_configure_sender.clone();
+            let configure_callback: crate::prompts::ChatConfigureCallback =
+                std::sync::Arc::new(move || {
+                    crate::logging::log(
+                        "CHAT_SCRIPT_GEN",
+                        "Configure callback triggered - sending signal",
+                    );
+                    let _ = configure_sender.try_send(());
+                });
+
+            // Create Claude Code callback that signals via channel
+            let claude_code_sender = self.inline_chat_claude_code_sender.clone();
+            let claude_code_callback: crate::prompts::ChatClaudeCodeCallback =
+                std::sync::Arc::new(move || {
+                    crate::logging::log(
+                        "CHAT_SCRIPT_GEN",
+                        "Claude Code callback triggered - sending signal",
+                    );
+                    let _ = claude_code_sender.try_send(());
+                });
+
+            // Create a no-op submit callback since we're in setup mode
+            let noop_callback: ChatSubmitCallback = std::sync::Arc::new(|_id, _text| {
+                crate::logging::log(
+                    "CHAT_SCRIPT_GEN",
+                    "No providers - submission ignored (setup mode)",
+                );
+            });
+
+            let chat_prompt = ChatPrompt::new(
+                "script-generation-setup".to_string(),
+                Some("Configure API key to generate scripts...".to_string()),
+                vec![],
+                None, // No hint needed - setup card is the UI
+                None,
+                self.focus_handle.clone(),
+                noop_callback,
+                std::sync::Arc::clone(&self.theme),
+            )
+            .with_title("Generate Script with AI")
+            .with_save_history(false) // Don't save setup state to history
+            .with_escape_callback(escape_callback.clone())
+            .with_needs_setup(true)
+            .with_configure_callback(configure_callback)
+            .with_claude_code_callback(claude_code_callback);
+
+            let entity = cx.new(|_| chat_prompt);
+            self.current_view = AppView::ChatPrompt {
+                id: "script-generation-setup".to_string(),
+                entity,
+            };
+            self.focused_input = FocusedInput::None;
+            self.pending_focus = Some(FocusTarget::ChatPrompt);
+            resize_to_view_sync(ViewType::DivPrompt, 0);
+            cx.notify();
+            return;
+        }
+
+        crate::logging::log(
+            "CHAT_SCRIPT_GEN",
+            &format!(
+                "Showing script generation chat with {} providers",
+                registry.provider_ids().len()
+            ),
+        );
+
+        // Create a no-op callback since built-in AI handles submissions internally
+        let noop_callback: ChatSubmitCallback = std::sync::Arc::new(|_id, _text| {
+            // Built-in AI mode handles this internally
+        });
+
+        let placeholder = Some("Describe the script you want to generate...".to_string());
+
+        let mut chat_prompt = ChatPrompt::new(
+            "script-generation".to_string(),
+            placeholder,
+            vec![],
+            None,
+            None,
+            self.focus_handle.clone(),
+            noop_callback,
+            std::sync::Arc::clone(&self.theme),
+        )
+        .with_title("Generate Script with AI")
+        .with_save_history(false)
+        .with_escape_callback(escape_callback)
+        .with_builtin_ai(registry, true) // true = prefer Vercel AI Gateway
+        .with_builtin_system_prompt(crate::ai::script_generation::AI_SCRIPT_GENERATION_SYSTEM_PROMPT)
+        .with_script_generation_mode(true);
+
+        // If there's an initial query, set it in the input and auto-submit
+        if let Some(query) = initial_query {
+            chat_prompt.input.set_text(&query);
+            chat_prompt = chat_prompt.with_pending_submit(true);
+        }
+
+        let entity = cx.new(|_| chat_prompt);
+        self.current_view = AppView::ChatPrompt {
+            id: "script-generation".to_string(),
+            entity,
+        };
+        self.focused_input = FocusedInput::None;
+        self.pending_focus = Some(FocusTarget::ChatPrompt);
+        resize_to_view_sync(ViewType::DivPrompt, 0);
+        cx.notify();
+    }
+
     /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
     /// The generated script is saved to disk and opened in the configured editor.
     pub fn generate_script_from_ai_prompt(
diff --git a/src/app_impl/startup.rs b/src/app_impl/startup.rs
index 441b4f23bca1b6294025c698eeb11d45815e85e0..4fd4dc8c23d8df925c657b8db59f888b855ea6a2
--- a/src/app_impl/startup.rs
+++ b/src/app_impl/startup.rs
@@ -723,7 +723,7 @@
                             }
 
                             // Handle Tab/Shift+Tab in ScriptList view for AI actions.
-                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
+                            // Tab opens Ask AI chat, Shift+Tab opens script generation chat mode.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
@@ -734,7 +734,7 @@
                                 this.filter_text.clear();
 
                                 if has_shift {
-                                    this.generate_script_from_ai_prompt(query, cx);
+                                    this.show_script_generation_chat(Some(query), cx);
                                 } else {
                                     // Show inline AI chat with the query as initial input
                                     this.show_inline_ai_chat(Some(query), cx);
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..f38da4434a11c5693a2baaca2ad71f17d6e04ad8
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,50 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;

file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index c985a5090574816267103019d1b272e1e3151a38..7f8acfae4036d0f77ae1eca73994c8d188bfb44e
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -464,6 +464,145 @@
         cx.notify();
     }
 
+    /// Show script generation chat prompt with built-in AI provider support.
+    /// This opens ChatPrompt in script-generation mode with the script generation system prompt.
+    pub fn show_script_generation_chat(
+        &mut self,
+        initial_query: Option<String>,
+        cx: &mut Context<Self>,
+    ) {
+        use crate::ai::ProviderRegistry;
+        use crate::prompts::{ChatEscapeCallback, ChatPrompt, ChatSubmitCallback};
+
+        // Mark as opened from main menu so ESC returns to main menu
+        self.opened_from_main_menu = true;
+
+        // Create escape callback that signals via channel
+        let escape_sender = self.inline_chat_escape_sender.clone();
+        let escape_callback: ChatEscapeCallback = std::sync::Arc::new(move |_id| {
+            let _ = escape_sender.try_send(());
+        });
+
+        // Use cached registry if available, otherwise build synchronously as fallback
+        let registry = self
+            .cached_provider_registry
+            .clone()
+            .unwrap_or_else(|| ProviderRegistry::from_environment_with_config(Some(&self.config)));
+
+        if !registry.has_any_provider() {
+            crate::logging::log(
+                "CHAT_SCRIPT_GEN",
+                "No AI providers configured - showing setup card",
+            );
+
+            // Create configure callback that signals via channel
+            let configure_sender = self.inline_chat_configure_sender.clone();
+            let configure_callback: crate::prompts::ChatConfigureCallback =
+                std::sync::Arc::new(move || {
+                    crate::logging::log(
+                        "CHAT_SCRIPT_GEN",
+                        "Configure callback triggered - sending signal",
+                    );
+                    let _ = configure_sender.try_send(());
+                });
+
+            // Create Claude Code callback that signals via channel
+            let claude_code_sender = self.inline_chat_claude_code_sender.clone();
+            let claude_code_callback: crate::prompts::ChatClaudeCodeCallback =
+                std::sync::Arc::new(move || {
+                    crate::logging::log(
+                        "CHAT_SCRIPT_GEN",
+                        "Claude Code callback triggered - sending signal",
+                    );
+                    let _ = claude_code_sender.try_send(());
+                });
+
+            // Create a no-op submit callback since we're in setup mode
+            let noop_callback: ChatSubmitCallback = std::sync::Arc::new(|_id, _text| {
+                crate::logging::log(
+                    "CHAT_SCRIPT_GEN",
+                    "No providers - submission ignored (setup mode)",
+                );
+            });
+
+            let chat_prompt = ChatPrompt::new(
+                "script-generation-setup".to_string(),
+                Some("Configure API key to generate scripts...".to_string()),
+                vec![],
+                None, // No hint needed - setup card is the UI
+                None,
+                self.focus_handle.clone(),
+                noop_callback,
+                std::sync::Arc::clone(&self.theme),
+            )
+            .with_title("Generate Script with AI")
+            .with_save_history(false) // Don't save setup state to history
+            .with_escape_callback(escape_callback.clone())
+            .with_needs_setup(true)
+            .with_configure_callback(configure_callback)
+            .with_claude_code_callback(claude_code_callback);
+
+            let entity = cx.new(|_| chat_prompt);
+            self.current_view = AppView::ChatPrompt {
+                id: "script-generation-setup".to_string(),
+                entity,
+            };
+            self.focused_input = FocusedInput::None;
+            self.pending_focus = Some(FocusTarget::ChatPrompt);
+            resize_to_view_sync(ViewType::DivPrompt, 0);
+            cx.notify();
+            return;
+        }
+
+        crate::logging::log(
+            "CHAT_SCRIPT_GEN",
+            &format!(
+                "Showing script generation chat with {} providers",
+                registry.provider_ids().len()
+            ),
+        );
+
+        // Create a no-op callback since built-in AI handles submissions internally
+        let noop_callback: ChatSubmitCallback = std::sync::Arc::new(|_id, _text| {
+            // Built-in AI mode handles this internally
+        });
+
+        let placeholder = Some("Describe the script you want to generate...".to_string());
+
+        let mut chat_prompt = ChatPrompt::new(
+            "script-generation".to_string(),
+            placeholder,
+            vec![],
+            None,
+            None,
+            self.focus_handle.clone(),
+            noop_callback,
+            std::sync::Arc::clone(&self.theme),
+        )
+        .with_title("Generate Script with AI")
+        .with_save_history(false)
+        .with_escape_callback(escape_callback)
+        .with_builtin_ai(registry, true) // true = prefer Vercel AI Gateway
+        .with_builtin_system_prompt(crate::ai::script_generation::AI_SCRIPT_GENERATION_SYSTEM_PROMPT)
+        .with_script_generation_mode(true);
+
+        // If there's an initial query, set it in the input and auto-submit
+        if let Some(query) = initial_query {
+            chat_prompt.input.set_text(&query);
+            chat_prompt = chat_prompt.with_pending_submit(true);
+        }
+
+        let entity = cx.new(|_| chat_prompt);
+        self.current_view = AppView::ChatPrompt {
+            id: "script-generation".to_string(),
+            entity,
+        };
+        self.focused_input = FocusedInput::None;
+        self.pending_focus = Some(FocusTarget::ChatPrompt);
+        resize_to_view_sync(ViewType::DivPrompt, 0);
+        cx.notify();
+    }
+
     /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
     /// The generated script is saved to disk and opened in the configured editor.
     pub fn generate_script_from_ai_prompt(
diff --git a/src/app_impl/startup.rs b/src/app_impl/startup.rs
index 441b4f23bca1b6294025c698eeb11d45815e85e0..4fd4dc8c23d8df925c657b8db59f888b855ea6a2
--- a/src/app_impl/startup.rs
+++ b/src/app_impl/startup.rs
@@ -723,7 +723,7 @@
                             }
 
                             // Handle Tab/Shift+Tab in ScriptList view for AI actions.
-                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
+                            // Tab opens Ask AI chat, Shift+Tab opens script generation chat mode.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
@@ -734,7 +734,7 @@
                                 this.filter_text.clear();
 
                                 if has_shift {
-                                    this.generate_script_from_ai_prompt(query, cx);
+                                    this.show_script_generation_chat(Some(query), cx);
                                 } else {
                                     // Show inline AI chat with the query as initial input
                                     this.show_inline_ai_chat(Some(query), cx);
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..f38da4434a11c5693a2baaca2ad71f17d6e04ad8
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,50 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;

file update
M /Users/johnlindquist/dev/script-kit-gpui/src/app_impl/startup_new_tab.rs
@@ -176,3 +176,3 @@
                             // Handle Tab/Shift+Tab in ScriptList view for AI actions.
-                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
+                            // Tab opens Ask AI chat, Shift+Tab opens script generation chat mode.
                             if matches!(this.current_view, AppView::ScriptList)
@@ -187,3 +187,3 @@
                                 if has_shift {
-                                    this.generate_script_from_ai_prompt(query, cx);
+                                    this.show_script_generation_chat(Some(query), cx);
                                 } else {
apply_patch(auto_approved=true) exited 0 in 36ms:
Success. Updated the following files:
M /Users/johnlindquist/dev/script-kit-gpui/src/app_impl/startup_new_tab.rs
file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index c985a5090574816267103019d1b272e1e3151a38..7f8acfae4036d0f77ae1eca73994c8d188bfb44e
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -464,6 +464,145 @@
         cx.notify();
     }
 
+    /// Show script generation chat prompt with built-in AI provider support.
+    /// This opens ChatPrompt in script-generation mode with the script generation system prompt.
+    pub fn show_script_generation_chat(
+        &mut self,
+        initial_query: Option<String>,
+        cx: &mut Context<Self>,
+    ) {
+        use crate::ai::ProviderRegistry;
+        use crate::prompts::{ChatEscapeCallback, ChatPrompt, ChatSubmitCallback};
+
+        // Mark as opened from main menu so ESC returns to main menu
+        self.opened_from_main_menu = true;
+
+        // Create escape callback that signals via channel
+        let escape_sender = self.inline_chat_escape_sender.clone();
+        let escape_callback: ChatEscapeCallback = std::sync::Arc::new(move |_id| {
+            let _ = escape_sender.try_send(());
+        });
+
+        // Use cached registry if available, otherwise build synchronously as fallback
+        let registry = self
+            .cached_provider_registry
+            .clone()
+            .unwrap_or_else(|| ProviderRegistry::from_environment_with_config(Some(&self.config)));
+
+        if !registry.has_any_provider() {
+            crate::logging::log(
+                "CHAT_SCRIPT_GEN",
+                "No AI providers configured - showing setup card",
+            );
+
+            // Create configure callback that signals via channel
+            let configure_sender = self.inline_chat_configure_sender.clone();
+            let configure_callback: crate::prompts::ChatConfigureCallback =
+                std::sync::Arc::new(move || {
+                    crate::logging::log(
+                        "CHAT_SCRIPT_GEN",
+                        "Configure callback triggered - sending signal",
+                    );
+                    let _ = configure_sender.try_send(());
+                });
+
+            // Create Claude Code callback that signals via channel
+            let claude_code_sender = self.inline_chat_claude_code_sender.clone();
+            let claude_code_callback: crate::prompts::ChatClaudeCodeCallback =
+                std::sync::Arc::new(move || {
+                    crate::logging::log(
+                        "CHAT_SCRIPT_GEN",
+                        "Claude Code callback triggered - sending signal",
+                    );
+                    let _ = claude_code_sender.try_send(());
+                });
+
+            // Create a no-op submit callback since we're in setup mode
+            let noop_callback: ChatSubmitCallback = std::sync::Arc::new(|_id, _text| {
+                crate::logging::log(
+                    "CHAT_SCRIPT_GEN",
+                    "No providers - submission ignored (setup mode)",
+                );
+            });
+
+            let chat_prompt = ChatPrompt::new(
+                "script-generation-setup".to_string(),
+                Some("Configure API key to generate scripts...".to_string()),
+                vec![],
+                None, // No hint needed - setup card is the UI
+                None,
+                self.focus_handle.clone(),
+                noop_callback,
+                std::sync::Arc::clone(&self.theme),
+            )
+            .with_title("Generate Script with AI")
+            .with_save_history(false) // Don't save setup state to history
+            .with_escape_callback(escape_callback.clone())
+            .with_needs_setup(true)
+            .with_configure_callback(configure_callback)
+            .with_claude_code_callback(claude_code_callback);
+
+            let entity = cx.new(|_| chat_prompt);
+            self.current_view = AppView::ChatPrompt {
+                id: "script-generation-setup".to_string(),
+                entity,
+            };
+            self.focused_input = FocusedInput::None;
+            self.pending_focus = Some(FocusTarget::ChatPrompt);
+            resize_to_view_sync(ViewType::DivPrompt, 0);
+            cx.notify();
+            return;
+        }
+
+        crate::logging::log(
+            "CHAT_SCRIPT_GEN",
+            &format!(
+                "Showing script generation chat with {} providers",
+                registry.provider_ids().len()
+            ),
+        );
+
+        // Create a no-op callback since built-in AI handles submissions internally
+        let noop_callback: ChatSubmitCallback = std::sync::Arc::new(|_id, _text| {
+            // Built-in AI mode handles this internally
+        });
+
+        let placeholder = Some("Describe the script you want to generate...".to_string());
+
+        let mut chat_prompt = ChatPrompt::new(
+            "script-generation".to_string(),
+            placeholder,
+            vec![],
+            None,
+            None,
+            self.focus_handle.clone(),
+            noop_callback,
+            std::sync::Arc::clone(&self.theme),
+        )
+        .with_title("Generate Script with AI")
+        .with_save_history(false)
+        .with_escape_callback(escape_callback)
+        .with_builtin_ai(registry, true) // true = prefer Vercel AI Gateway
+        .with_builtin_system_prompt(crate::ai::script_generation::AI_SCRIPT_GENERATION_SYSTEM_PROMPT)
+        .with_script_generation_mode(true);
+
+        // If there's an initial query, set it in the input and auto-submit
+        if let Some(query) = initial_query {
+            chat_prompt.input.set_text(&query);
+            chat_prompt = chat_prompt.with_pending_submit(true);
+        }
+
+        let entity = cx.new(|_| chat_prompt);
+        self.current_view = AppView::ChatPrompt {
+            id: "script-generation".to_string(),
+            entity,
+        };
+        self.focused_input = FocusedInput::None;
+        self.pending_focus = Some(FocusTarget::ChatPrompt);
+        resize_to_view_sync(ViewType::DivPrompt, 0);
+        cx.notify();
+    }
+
     /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
     /// The generated script is saved to disk and opened in the configured editor.
     pub fn generate_script_from_ai_prompt(
diff --git a/src/app_impl/startup.rs b/src/app_impl/startup.rs
index 441b4f23bca1b6294025c698eeb11d45815e85e0..4fd4dc8c23d8df925c657b8db59f888b855ea6a2
--- a/src/app_impl/startup.rs
+++ b/src/app_impl/startup.rs
@@ -723,7 +723,7 @@
                             }
 
                             // Handle Tab/Shift+Tab in ScriptList view for AI actions.
-                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
+                            // Tab opens Ask AI chat, Shift+Tab opens script generation chat mode.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
@@ -734,7 +734,7 @@
                                 this.filter_text.clear();
 
                                 if has_shift {
-                                    this.generate_script_from_ai_prompt(query, cx);
+                                    this.show_script_generation_chat(Some(query), cx);
                                 } else {
                                     // Show inline AI chat with the query as initial input
                                     this.show_inline_ai_chat(Some(query), cx);
diff --git a/src/app_impl/startup_new_tab.rs b/src/app_impl/startup_new_tab.rs
index 7c84270cbe91c0b1ddd460aa29db8c788766d0cf..c75a27bf67a5448f59c7700079e8818210722bde
--- a/src/app_impl/startup_new_tab.rs
+++ b/src/app_impl/startup_new_tab.rs
@@ -174,7 +174,7 @@
                             }
 
                             // Handle Tab/Shift+Tab in ScriptList view for AI actions.
-                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
+                            // Tab opens Ask AI chat, Shift+Tab opens script generation chat mode.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
@@ -185,7 +185,7 @@
                                 this.filter_text.clear();
 
                                 if has_shift {
-                                    this.generate_script_from_ai_prompt(query, cx);
+                                    this.show_script_generation_chat(Some(query), cx);
                                 } else {
                                     // Show inline AI chat with the query as initial input
                                     this.show_inline_ai_chat(Some(query), cx);
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..f38da4434a11c5693a2baaca2ad71f17d6e04ad8
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,50 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;

file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index c985a5090574816267103019d1b272e1e3151a38..7f8acfae4036d0f77ae1eca73994c8d188bfb44e
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -464,6 +464,145 @@
         cx.notify();
     }
 
+    /// Show script generation chat prompt with built-in AI provider support.
+    /// This opens ChatPrompt in script-generation mode with the script generation system prompt.
+    pub fn show_script_generation_chat(
+        &mut self,
+        initial_query: Option<String>,
+        cx: &mut Context<Self>,
+    ) {
+        use crate::ai::ProviderRegistry;
+        use crate::prompts::{ChatEscapeCallback, ChatPrompt, ChatSubmitCallback};
+
+        // Mark as opened from main menu so ESC returns to main menu
+        self.opened_from_main_menu = true;
+
+        // Create escape callback that signals via channel
+        let escape_sender = self.inline_chat_escape_sender.clone();
+        let escape_callback: ChatEscapeCallback = std::sync::Arc::new(move |_id| {
+            let _ = escape_sender.try_send(());
+        });
+
+        // Use cached registry if available, otherwise build synchronously as fallback
+        let registry = self
+            .cached_provider_registry
+            .clone()
+            .unwrap_or_else(|| ProviderRegistry::from_environment_with_config(Some(&self.config)));
+
+        if !registry.has_any_provider() {
+            crate::logging::log(
+                "CHAT_SCRIPT_GEN",
+                "No AI providers configured - showing setup card",
+            );
+
+            // Create configure callback that signals via channel
+            let configure_sender = self.inline_chat_configure_sender.clone();
+            let configure_callback: crate::prompts::ChatConfigureCallback =
+                std::sync::Arc::new(move || {
+                    crate::logging::log(
+                        "CHAT_SCRIPT_GEN",
+                        "Configure callback triggered - sending signal",
+                    );
+                    let _ = configure_sender.try_send(());
+                });
+
+            // Create Claude Code callback that signals via channel
+            let claude_code_sender = self.inline_chat_claude_code_sender.clone();
+            let claude_code_callback: crate::prompts::ChatClaudeCodeCallback =
+                std::sync::Arc::new(move || {
+                    crate::logging::log(
+                        "CHAT_SCRIPT_GEN",
+                        "Claude Code callback triggered - sending signal",
+                    );
+                    let _ = claude_code_sender.try_send(());
+                });
+
+            // Create a no-op submit callback since we're in setup mode
+            let noop_callback: ChatSubmitCallback = std::sync::Arc::new(|_id, _text| {
+                crate::logging::log(
+                    "CHAT_SCRIPT_GEN",
+                    "No providers - submission ignored (setup mode)",
+                );
+            });
+
+            let chat_prompt = ChatPrompt::new(
+                "script-generation-setup".to_string(),
+                Some("Configure API key to generate scripts...".to_string()),
+                vec![],
+                None, // No hint needed - setup card is the UI
+                None,
+                self.focus_handle.clone(),
+                noop_callback,
+                std::sync::Arc::clone(&self.theme),
+            )
+            .with_title("Generate Script with AI")
+            .with_save_history(false) // Don't save setup state to history
+            .with_escape_callback(escape_callback.clone())
+            .with_needs_setup(true)
+            .with_configure_callback(configure_callback)
+            .with_claude_code_callback(claude_code_callback);
+
+            let entity = cx.new(|_| chat_prompt);
+            self.current_view = AppView::ChatPrompt {
+                id: "script-generation-setup".to_string(),
+                entity,
+            };
+            self.focused_input = FocusedInput::None;
+            self.pending_focus = Some(FocusTarget::ChatPrompt);
+            resize_to_view_sync(ViewType::DivPrompt, 0);
+            cx.notify();
+            return;
+        }
+
+        crate::logging::log(
+            "CHAT_SCRIPT_GEN",
+            &format!(
+                "Showing script generation chat with {} providers",
+                registry.provider_ids().len()
+            ),
+        );
+
+        // Create a no-op callback since built-in AI handles submissions internally
+        let noop_callback: ChatSubmitCallback = std::sync::Arc::new(|_id, _text| {
+            // Built-in AI mode handles this internally
+        });
+
+        let placeholder = Some("Describe the script you want to generate...".to_string());
+
+        let mut chat_prompt = ChatPrompt::new(
+            "script-generation".to_string(),
+            placeholder,
+            vec![],
+            None,
+            None,
+            self.focus_handle.clone(),
+            noop_callback,
+            std::sync::Arc::clone(&self.theme),
+        )
+        .with_title("Generate Script with AI")
+        .with_save_history(false)
+        .with_escape_callback(escape_callback)
+        .with_builtin_ai(registry, true) // true = prefer Vercel AI Gateway
+        .with_builtin_system_prompt(crate::ai::script_generation::AI_SCRIPT_GENERATION_SYSTEM_PROMPT)
+        .with_script_generation_mode(true);
+
+        // If there's an initial query, set it in the input and auto-submit
+        if let Some(query) = initial_query {
+            chat_prompt.input.set_text(&query);
+            chat_prompt = chat_prompt.with_pending_submit(true);
+        }
+
+        let entity = cx.new(|_| chat_prompt);
+        self.current_view = AppView::ChatPrompt {
+            id: "script-generation".to_string(),
+            entity,
+        };
+        self.focused_input = FocusedInput::None;
+        self.pending_focus = Some(FocusTarget::ChatPrompt);
+        resize_to_view_sync(ViewType::DivPrompt, 0);
+        cx.notify();
+    }
+
     /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
     /// The generated script is saved to disk and opened in the configured editor.
     pub fn generate_script_from_ai_prompt(
diff --git a/src/app_impl/startup.rs b/src/app_impl/startup.rs
index 441b4f23bca1b6294025c698eeb11d45815e85e0..4fd4dc8c23d8df925c657b8db59f888b855ea6a2
--- a/src/app_impl/startup.rs
+++ b/src/app_impl/startup.rs
@@ -723,7 +723,7 @@
                             }
 
                             // Handle Tab/Shift+Tab in ScriptList view for AI actions.
-                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
+                            // Tab opens Ask AI chat, Shift+Tab opens script generation chat mode.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
@@ -734,7 +734,7 @@
                                 this.filter_text.clear();
 
                                 if has_shift {
-                                    this.generate_script_from_ai_prompt(query, cx);
+                                    this.show_script_generation_chat(Some(query), cx);
                                 } else {
                                     // Show inline AI chat with the query as initial input
                                     this.show_inline_ai_chat(Some(query), cx);
diff --git a/src/app_impl/startup_new_tab.rs b/src/app_impl/startup_new_tab.rs
index 7c84270cbe91c0b1ddd460aa29db8c788766d0cf..c75a27bf67a5448f59c7700079e8818210722bde
--- a/src/app_impl/startup_new_tab.rs
+++ b/src/app_impl/startup_new_tab.rs
@@ -174,7 +174,7 @@
                             }
 
                             // Handle Tab/Shift+Tab in ScriptList view for AI actions.
-                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
+                            // Tab opens Ask AI chat, Shift+Tab opens script generation chat mode.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
@@ -185,7 +185,7 @@
                                 this.filter_text.clear();
 
                                 if has_shift {
-                                    this.generate_script_from_ai_prompt(query, cx);
+                                    this.show_script_generation_chat(Some(query), cx);
                                 } else {
                                     // Show inline AI chat with the query as initial input
                                     this.show_inline_ai_chat(Some(query), cx);
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..f38da4434a11c5693a2baaca2ad71f17d6e04ad8
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,50 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;


thinking
**Adding enum and helper to types**
file update
M /Users/johnlindquist/dev/script-kit-gpui/src/prompts/chat/types.rs
@@ -120,2 +120,23 @@
 #[derive(Clone, Copy, Debug, PartialEq, Eq)]
+pub(crate) enum ScriptGenerationAction {
+    Save,
+    Run,
+    SaveAndRun,
+}
+
+impl ScriptGenerationAction {
+    pub(crate) fn should_run_after_save(self) -> bool {
+        matches!(self, Self::Run | Self::SaveAndRun)
+    }
+}
+
+pub(crate) fn should_show_script_generation_actions(
+    script_generation_mode: bool,
+    is_streaming: bool,
+    has_draft: bool,
+) -> bool {
+    script_generation_mode && !is_streaming && has_draft
+}
+
+#[derive(Clone, Copy, Debug, PartialEq, Eq)]
 pub(crate) enum ChatInputKeyAction {
apply_patch(auto_approved=true) exited 0 in 43ms:
Success. Updated the following files:
M /Users/johnlindquist/dev/script-kit-gpui/src/prompts/chat/types.rs
file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index c985a5090574816267103019d1b272e1e3151a38..7f8acfae4036d0f77ae1eca73994c8d188bfb44e
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -464,6 +464,145 @@
         cx.notify();
     }
 
+    /// Show script generation chat prompt with built-in AI provider support.
+    /// This opens ChatPrompt in script-generation mode with the script generation system prompt.
+    pub fn show_script_generation_chat(
+        &mut self,
+        initial_query: Option<String>,
+        cx: &mut Context<Self>,
+    ) {
+        use crate::ai::ProviderRegistry;
+        use crate::prompts::{ChatEscapeCallback, ChatPrompt, ChatSubmitCallback};
+
+        // Mark as opened from main menu so ESC returns to main menu
+        self.opened_from_main_menu = true;
+
+        // Create escape callback that signals via channel
+        let escape_sender = self.inline_chat_escape_sender.clone();
+        let escape_callback: ChatEscapeCallback = std::sync::Arc::new(move |_id| {
+            let _ = escape_sender.try_send(());
+        });
+
+        // Use cached registry if available, otherwise build synchronously as fallback
+        let registry = self
+            .cached_provider_registry
+            .clone()
+            .unwrap_or_else(|| ProviderRegistry::from_environment_with_config(Some(&self.config)));
+
+        if !registry.has_any_provider() {
+            crate::logging::log(
+                "CHAT_SCRIPT_GEN",
+                "No AI providers configured - showing setup card",
+            );
+
+            // Create configure callback that signals via channel
+            let configure_sender = self.inline_chat_configure_sender.clone();
+            let configure_callback: crate::prompts::ChatConfigureCallback =
+                std::sync::Arc::new(move || {
+                    crate::logging::log(
+                        "CHAT_SCRIPT_GEN",
+                        "Configure callback triggered - sending signal",
+                    );
+                    let _ = configure_sender.try_send(());
+                });
+
+            // Create Claude Code callback that signals via channel
+            let claude_code_sender = self.inline_chat_claude_code_sender.clone();
+            let claude_code_callback: crate::prompts::ChatClaudeCodeCallback =
+                std::sync::Arc::new(move || {
+                    crate::logging::log(
+                        "CHAT_SCRIPT_GEN",
+                        "Claude Code callback triggered - sending signal",
+                    );
+                    let _ = claude_code_sender.try_send(());
+                });
+
+            // Create a no-op submit callback since we're in setup mode
+            let noop_callback: ChatSubmitCallback = std::sync::Arc::new(|_id, _text| {
+                crate::logging::log(
+                    "CHAT_SCRIPT_GEN",
+                    "No providers - submission ignored (setup mode)",
+                );
+            });
+
+            let chat_prompt = ChatPrompt::new(
+                "script-generation-setup".to_string(),
+                Some("Configure API key to generate scripts...".to_string()),
+                vec![],
+                None, // No hint needed - setup card is the UI
+                None,
+                self.focus_handle.clone(),
+                noop_callback,
+                std::sync::Arc::clone(&self.theme),
+            )
+            .with_title("Generate Script with AI")
+            .with_save_history(false) // Don't save setup state to history
+            .with_escape_callback(escape_callback.clone())
+            .with_needs_setup(true)
+            .with_configure_callback(configure_callback)
+            .with_claude_code_callback(claude_code_callback);
+
+            let entity = cx.new(|_| chat_prompt);
+            self.current_view = AppView::ChatPrompt {
+                id: "script-generation-setup".to_string(),
+                entity,
+            };
+            self.focused_input = FocusedInput::None;
+            self.pending_focus = Some(FocusTarget::ChatPrompt);
+            resize_to_view_sync(ViewType::DivPrompt, 0);
+            cx.notify();
+            return;
+        }
+
+        crate::logging::log(
+            "CHAT_SCRIPT_GEN",
+            &format!(
+                "Showing script generation chat with {} providers",
+                registry.provider_ids().len()
+            ),
+        );
+
+        // Create a no-op callback since built-in AI handles submissions internally
+        let noop_callback: ChatSubmitCallback = std::sync::Arc::new(|_id, _text| {
+            // Built-in AI mode handles this internally
+        });
+
+        let placeholder = Some("Describe the script you want to generate...".to_string());
+
+        let mut chat_prompt = ChatPrompt::new(
+            "script-generation".to_string(),
+            placeholder,
+            vec![],
+            None,
+            None,
+            self.focus_handle.clone(),
+            noop_callback,
+            std::sync::Arc::clone(&self.theme),
+        )
+        .with_title("Generate Script with AI")
+        .with_save_history(false)
+        .with_escape_callback(escape_callback)
+        .with_builtin_ai(registry, true) // true = prefer Vercel AI Gateway
+        .with_builtin_system_prompt(crate::ai::script_generation::AI_SCRIPT_GENERATION_SYSTEM_PROMPT)
+        .with_script_generation_mode(true);
+
+        // If there's an initial query, set it in the input and auto-submit
+        if let Some(query) = initial_query {
+            chat_prompt.input.set_text(&query);
+            chat_prompt = chat_prompt.with_pending_submit(true);
+        }
+
+        let entity = cx.new(|_| chat_prompt);
+        self.current_view = AppView::ChatPrompt {
+            id: "script-generation".to_string(),
+            entity,
+        };
+        self.focused_input = FocusedInput::None;
+        self.pending_focus = Some(FocusTarget::ChatPrompt);
+        resize_to_view_sync(ViewType::DivPrompt, 0);
+        cx.notify();
+    }
+
     /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
     /// The generated script is saved to disk and opened in the configured editor.
     pub fn generate_script_from_ai_prompt(
diff --git a/src/app_impl/startup.rs b/src/app_impl/startup.rs
index 441b4f23bca1b6294025c698eeb11d45815e85e0..4fd4dc8c23d8df925c657b8db59f888b855ea6a2
--- a/src/app_impl/startup.rs
+++ b/src/app_impl/startup.rs
@@ -723,7 +723,7 @@
                             }
 
                             // Handle Tab/Shift+Tab in ScriptList view for AI actions.
-                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
+                            // Tab opens Ask AI chat, Shift+Tab opens script generation chat mode.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
@@ -734,7 +734,7 @@
                                 this.filter_text.clear();
 
                                 if has_shift {
-                                    this.generate_script_from_ai_prompt(query, cx);
+                                    this.show_script_generation_chat(Some(query), cx);
                                 } else {
                                     // Show inline AI chat with the query as initial input
                                     this.show_inline_ai_chat(Some(query), cx);
diff --git a/src/app_impl/startup_new_tab.rs b/src/app_impl/startup_new_tab.rs
index 7c84270cbe91c0b1ddd460aa29db8c788766d0cf..c75a27bf67a5448f59c7700079e8818210722bde
--- a/src/app_impl/startup_new_tab.rs
+++ b/src/app_impl/startup_new_tab.rs
@@ -174,7 +174,7 @@
                             }
 
                             // Handle Tab/Shift+Tab in ScriptList view for AI actions.
-                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
+                            // Tab opens Ask AI chat, Shift+Tab opens script generation chat mode.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
@@ -185,7 +185,7 @@
                                 this.filter_text.clear();
 
                                 if has_shift {
-                                    this.generate_script_from_ai_prompt(query, cx);
+                                    this.show_script_generation_chat(Some(query), cx);
                                 } else {
                                     // Show inline AI chat with the query as initial input
                                     this.show_inline_ai_chat(Some(query), cx);
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..f38da4434a11c5693a2baaca2ad71f17d6e04ad8
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,50 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;
diff --git a/src/prompts/chat/types.rs b/src/prompts/chat/types.rs
index 286c060bbf893558e05d81e932ed3ab61a09d06a..2ab2ee2e26318b52e2014c73ae18176ef4e45c6d
--- a/src/prompts/chat/types.rs
+++ b/src/prompts/chat/types.rs
@@ -118,6 +118,27 @@
 }
 
 #[derive(Clone, Copy, Debug, PartialEq, Eq)]
+pub(crate) enum ScriptGenerationAction {
+    Save,
+    Run,
+    SaveAndRun,
+}
+
+impl ScriptGenerationAction {
+    pub(crate) fn should_run_after_save(self) -> bool {
+        matches!(self, Self::Run | Self::SaveAndRun)
+    }
+}
+
+pub(crate) fn should_show_script_generation_actions(
+    script_generation_mode: bool,
+    is_streaming: bool,
+    has_draft: bool,
+) -> bool {
+    script_generation_mode && !is_streaming && has_draft
+}
+
+#[derive(Clone, Copy, Debug, PartialEq, Eq)]
 pub(crate) enum ChatInputKeyAction {
     Escape,
     StopStreaming,

file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index c985a5090574816267103019d1b272e1e3151a38..7f8acfae4036d0f77ae1eca73994c8d188bfb44e
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -464,6 +464,145 @@
         cx.notify();
     }
 
+    /// Show script generation chat prompt with built-in AI provider support.
+    /// This opens ChatPrompt in script-generation mode with the script generation system prompt.
+    pub fn show_script_generation_chat(
+        &mut self,
+        initial_query: Option<String>,
+        cx: &mut Context<Self>,
+    ) {
+        use crate::ai::ProviderRegistry;
+        use crate::prompts::{ChatEscapeCallback, ChatPrompt, ChatSubmitCallback};
+
+        // Mark as opened from main menu so ESC returns to main menu
+        self.opened_from_main_menu = true;
+
+        // Create escape callback that signals via channel
+        let escape_sender = self.inline_chat_escape_sender.clone();
+        let escape_callback: ChatEscapeCallback = std::sync::Arc::new(move |_id| {
+            let _ = escape_sender.try_send(());
+        });
+
+        // Use cached registry if available, otherwise build synchronously as fallback
+        let registry = self
+            .cached_provider_registry
+            .clone()
+            .unwrap_or_else(|| ProviderRegistry::from_environment_with_config(Some(&self.config)));
+
+        if !registry.has_any_provider() {
+            crate::logging::log(
+                "CHAT_SCRIPT_GEN",
+                "No AI providers configured - showing setup card",
+            );
+
+            // Create configure callback that signals via channel
+            let configure_sender = self.inline_chat_configure_sender.clone();
+            let configure_callback: crate::prompts::ChatConfigureCallback =
+                std::sync::Arc::new(move || {
+                    crate::logging::log(
+                        "CHAT_SCRIPT_GEN",
+                        "Configure callback triggered - sending signal",
+                    );
+                    let _ = configure_sender.try_send(());
+                });
+
+            // Create Claude Code callback that signals via channel
+            let claude_code_sender = self.inline_chat_claude_code_sender.clone();
+            let claude_code_callback: crate::prompts::ChatClaudeCodeCallback =
+                std::sync::Arc::new(move || {
+                    crate::logging::log(
+                        "CHAT_SCRIPT_GEN",
+                        "Claude Code callback triggered - sending signal",
+                    );
+                    let _ = claude_code_sender.try_send(());
+                });
+
+            // Create a no-op submit callback since we're in setup mode
+            let noop_callback: ChatSubmitCallback = std::sync::Arc::new(|_id, _text| {
+                crate::logging::log(
+                    "CHAT_SCRIPT_GEN",
+                    "No providers - submission ignored (setup mode)",
+                );
+            });
+
+            let chat_prompt = ChatPrompt::new(
+                "script-generation-setup".to_string(),
+                Some("Configure API key to generate scripts...".to_string()),
+                vec![],
+                None, // No hint needed - setup card is the UI
+                None,
+                self.focus_handle.clone(),
+                noop_callback,
+                std::sync::Arc::clone(&self.theme),
+            )
+            .with_title("Generate Script with AI")
+            .with_save_history(false) // Don't save setup state to history
+            .with_escape_callback(escape_callback.clone())
+            .with_needs_setup(true)
+            .with_configure_callback(configure_callback)
+            .with_claude_code_callback(claude_code_callback);
+
+            let entity = cx.new(|_| chat_prompt);
+            self.current_view = AppView::ChatPrompt {
+                id: "script-generation-setup".to_string(),
+                entity,
+            };
+            self.focused_input = FocusedInput::None;
+            self.pending_focus = Some(FocusTarget::ChatPrompt);
+            resize_to_view_sync(ViewType::DivPrompt, 0);
+            cx.notify();
+            return;
+        }
+
+        crate::logging::log(
+            "CHAT_SCRIPT_GEN",
+            &format!(
+                "Showing script generation chat with {} providers",
+                registry.provider_ids().len()
+            ),
+        );
+
+        // Create a no-op callback since built-in AI handles submissions internally
+        let noop_callback: ChatSubmitCallback = std::sync::Arc::new(|_id, _text| {
+            // Built-in AI mode handles this internally
+        });
+
+        let placeholder = Some("Describe the script you want to generate...".to_string());
+
+        let mut chat_prompt = ChatPrompt::new(
+            "script-generation".to_string(),
+            placeholder,
+            vec![],
+            None,
+            None,
+            self.focus_handle.clone(),
+            noop_callback,
+            std::sync::Arc::clone(&self.theme),
+        )
+        .with_title("Generate Script with AI")
+        .with_save_history(false)
+        .with_escape_callback(escape_callback)
+        .with_builtin_ai(registry, true) // true = prefer Vercel AI Gateway
+        .with_builtin_system_prompt(crate::ai::script_generation::AI_SCRIPT_GENERATION_SYSTEM_PROMPT)
+        .with_script_generation_mode(true);
+
+        // If there's an initial query, set it in the input and auto-submit
+        if let Some(query) = initial_query {
+            chat_prompt.input.set_text(&query);
+            chat_prompt = chat_prompt.with_pending_submit(true);
+        }
+
+        let entity = cx.new(|_| chat_prompt);
+        self.current_view = AppView::ChatPrompt {
+            id: "script-generation".to_string(),
+            entity,
+        };
+        self.focused_input = FocusedInput::None;
+        self.pending_focus = Some(FocusTarget::ChatPrompt);
+        resize_to_view_sync(ViewType::DivPrompt, 0);
+        cx.notify();
+    }
+
     /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
     /// The generated script is saved to disk and opened in the configured editor.
     pub fn generate_script_from_ai_prompt(
diff --git a/src/app_impl/startup.rs b/src/app_impl/startup.rs
index 441b4f23bca1b6294025c698eeb11d45815e85e0..4fd4dc8c23d8df925c657b8db59f888b855ea6a2
--- a/src/app_impl/startup.rs
+++ b/src/app_impl/startup.rs
@@ -723,7 +723,7 @@
                             }
 
                             // Handle Tab/Shift+Tab in ScriptList view for AI actions.
-                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
+                            // Tab opens Ask AI chat, Shift+Tab opens script generation chat mode.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
@@ -734,7 +734,7 @@
                                 this.filter_text.clear();
 
                                 if has_shift {
-                                    this.generate_script_from_ai_prompt(query, cx);
+                                    this.show_script_generation_chat(Some(query), cx);
                                 } else {
                                     // Show inline AI chat with the query as initial input
                                     this.show_inline_ai_chat(Some(query), cx);
diff --git a/src/app_impl/startup_new_tab.rs b/src/app_impl/startup_new_tab.rs
index 7c84270cbe91c0b1ddd460aa29db8c788766d0cf..c75a27bf67a5448f59c7700079e8818210722bde
--- a/src/app_impl/startup_new_tab.rs
+++ b/src/app_impl/startup_new_tab.rs
@@ -174,7 +174,7 @@
                             }
 
                             // Handle Tab/Shift+Tab in ScriptList view for AI actions.
-                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
+                            // Tab opens Ask AI chat, Shift+Tab opens script generation chat mode.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
@@ -185,7 +185,7 @@
                                 this.filter_text.clear();
 
                                 if has_shift {
-                                    this.generate_script_from_ai_prompt(query, cx);
+                                    this.show_script_generation_chat(Some(query), cx);
                                 } else {
                                     // Show inline AI chat with the query as initial input
                                     this.show_inline_ai_chat(Some(query), cx);
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..f38da4434a11c5693a2baaca2ad71f17d6e04ad8
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,50 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;
diff --git a/src/prompts/chat/types.rs b/src/prompts/chat/types.rs
index 286c060bbf893558e05d81e932ed3ab61a09d06a..2ab2ee2e26318b52e2014c73ae18176ef4e45c6d
--- a/src/prompts/chat/types.rs
+++ b/src/prompts/chat/types.rs
@@ -118,6 +118,27 @@
 }
 
 #[derive(Clone, Copy, Debug, PartialEq, Eq)]
+pub(crate) enum ScriptGenerationAction {
+    Save,
+    Run,
+    SaveAndRun,
+}
+
+impl ScriptGenerationAction {
+    pub(crate) fn should_run_after_save(self) -> bool {
+        matches!(self, Self::Run | Self::SaveAndRun)
+    }
+}
+
+pub(crate) fn should_show_script_generation_actions(
+    script_generation_mode: bool,
+    is_streaming: bool,
+    has_draft: bool,
+) -> bool {
+    script_generation_mode && !is_streaming && has_draft
+}
+
+#[derive(Clone, Copy, Debug, PartialEq, Eq)]
 pub(crate) enum ChatInputKeyAction {
     Escape,
     StopStreaming,


thinking
**Updating import list for helper enum**
file update
M /Users/johnlindquist/dev/script-kit-gpui/src/prompts/chat/mod.rs
@@ -51,3 +51,4 @@
     next_reveal_boundary, resolve_chat_input_key_action, resolve_setup_card_key,
-    should_ignore_stream_reveal_update, ChatInputKeyAction, ChatScrollDirection, SetupCardAction,
+    should_ignore_stream_reveal_update, should_show_script_generation_actions,
+    ChatInputKeyAction, ChatScrollDirection, ScriptGenerationAction, SetupCardAction,
 };
apply_patch(auto_approved=true) exited 0 in 49ms:
Success. Updated the following files:
M /Users/johnlindquist/dev/script-kit-gpui/src/prompts/chat/mod.rs
file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index c985a5090574816267103019d1b272e1e3151a38..7f8acfae4036d0f77ae1eca73994c8d188bfb44e
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -464,6 +464,145 @@
         cx.notify();
     }
 
+    /// Show script generation chat prompt with built-in AI provider support.
+    /// This opens ChatPrompt in script-generation mode with the script generation system prompt.
+    pub fn show_script_generation_chat(
+        &mut self,
+        initial_query: Option<String>,
+        cx: &mut Context<Self>,
+    ) {
+        use crate::ai::ProviderRegistry;
+        use crate::prompts::{ChatEscapeCallback, ChatPrompt, ChatSubmitCallback};
+
+        // Mark as opened from main menu so ESC returns to main menu
+        self.opened_from_main_menu = true;
+
+        // Create escape callback that signals via channel
+        let escape_sender = self.inline_chat_escape_sender.clone();
+        let escape_callback: ChatEscapeCallback = std::sync::Arc::new(move |_id| {
+            let _ = escape_sender.try_send(());
+        });
+
+        // Use cached registry if available, otherwise build synchronously as fallback
+        let registry = self
+            .cached_provider_registry
+            .clone()
+            .unwrap_or_else(|| ProviderRegistry::from_environment_with_config(Some(&self.config)));
+
+        if !registry.has_any_provider() {
+            crate::logging::log(
+                "CHAT_SCRIPT_GEN",
+                "No AI providers configured - showing setup card",
+            );
+
+            // Create configure callback that signals via channel
+            let configure_sender = self.inline_chat_configure_sender.clone();
+            let configure_callback: crate::prompts::ChatConfigureCallback =
+                std::sync::Arc::new(move || {
+                    crate::logging::log(
+                        "CHAT_SCRIPT_GEN",
+                        "Configure callback triggered - sending signal",
+                    );
+                    let _ = configure_sender.try_send(());
+                });
+
+            // Create Claude Code callback that signals via channel
+            let claude_code_sender = self.inline_chat_claude_code_sender.clone();
+            let claude_code_callback: crate::prompts::ChatClaudeCodeCallback =
+                std::sync::Arc::new(move || {
+                    crate::logging::log(
+                        "CHAT_SCRIPT_GEN",
+                        "Claude Code callback triggered - sending signal",
+                    );
+                    let _ = claude_code_sender.try_send(());
+                });
+
+            // Create a no-op submit callback since we're in setup mode
+            let noop_callback: ChatSubmitCallback = std::sync::Arc::new(|_id, _text| {
+                crate::logging::log(
+                    "CHAT_SCRIPT_GEN",
+                    "No providers - submission ignored (setup mode)",
+                );
+            });
+
+            let chat_prompt = ChatPrompt::new(
+                "script-generation-setup".to_string(),
+                Some("Configure API key to generate scripts...".to_string()),
+                vec![],
+                None, // No hint needed - setup card is the UI
+                None,
+                self.focus_handle.clone(),
+                noop_callback,
+                std::sync::Arc::clone(&self.theme),
+            )
+            .with_title("Generate Script with AI")
+            .with_save_history(false) // Don't save setup state to history
+            .with_escape_callback(escape_callback.clone())
+            .with_needs_setup(true)
+            .with_configure_callback(configure_callback)
+            .with_claude_code_callback(claude_code_callback);
+
+            let entity = cx.new(|_| chat_prompt);
+            self.current_view = AppView::ChatPrompt {
+                id: "script-generation-setup".to_string(),
+                entity,
+            };
+            self.focused_input = FocusedInput::None;
+            self.pending_focus = Some(FocusTarget::ChatPrompt);
+            resize_to_view_sync(ViewType::DivPrompt, 0);
+            cx.notify();
+            return;
+        }
+
+        crate::logging::log(
+            "CHAT_SCRIPT_GEN",
+            &format!(
+                "Showing script generation chat with {} providers",
+                registry.provider_ids().len()
+            ),
+        );
+
+        // Create a no-op callback since built-in AI handles submissions internally
+        let noop_callback: ChatSubmitCallback = std::sync::Arc::new(|_id, _text| {
+            // Built-in AI mode handles this internally
+        });
+
+        let placeholder = Some("Describe the script you want to generate...".to_string());
+
+        let mut chat_prompt = ChatPrompt::new(
+            "script-generation".to_string(),
+            placeholder,
+            vec![],
+            None,
+            None,
+            self.focus_handle.clone(),
+            noop_callback,
+            std::sync::Arc::clone(&self.theme),
+        )
+        .with_title("Generate Script with AI")
+        .with_save_history(false)
+        .with_escape_callback(escape_callback)
+        .with_builtin_ai(registry, true) // true = prefer Vercel AI Gateway
+        .with_builtin_system_prompt(crate::ai::script_generation::AI_SCRIPT_GENERATION_SYSTEM_PROMPT)
+        .with_script_generation_mode(true);
+
+        // If there's an initial query, set it in the input and auto-submit
+        if let Some(query) = initial_query {
+            chat_prompt.input.set_text(&query);
+            chat_prompt = chat_prompt.with_pending_submit(true);
+        }
+
+        let entity = cx.new(|_| chat_prompt);
+        self.current_view = AppView::ChatPrompt {
+            id: "script-generation".to_string(),
+            entity,
+        };
+        self.focused_input = FocusedInput::None;
+        self.pending_focus = Some(FocusTarget::ChatPrompt);
+        resize_to_view_sync(ViewType::DivPrompt, 0);
+        cx.notify();
+    }
+
     /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
     /// The generated script is saved to disk and opened in the configured editor.
     pub fn generate_script_from_ai_prompt(
diff --git a/src/app_impl/startup.rs b/src/app_impl/startup.rs
index 441b4f23bca1b6294025c698eeb11d45815e85e0..4fd4dc8c23d8df925c657b8db59f888b855ea6a2
--- a/src/app_impl/startup.rs
+++ b/src/app_impl/startup.rs
@@ -723,7 +723,7 @@
                             }
 
                             // Handle Tab/Shift+Tab in ScriptList view for AI actions.
-                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
+                            // Tab opens Ask AI chat, Shift+Tab opens script generation chat mode.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
@@ -734,7 +734,7 @@
                                 this.filter_text.clear();
 
                                 if has_shift {
-                                    this.generate_script_from_ai_prompt(query, cx);
+                                    this.show_script_generation_chat(Some(query), cx);
                                 } else {
                                     // Show inline AI chat with the query as initial input
                                     this.show_inline_ai_chat(Some(query), cx);
diff --git a/src/app_impl/startup_new_tab.rs b/src/app_impl/startup_new_tab.rs
index 7c84270cbe91c0b1ddd460aa29db8c788766d0cf..c75a27bf67a5448f59c7700079e8818210722bde
--- a/src/app_impl/startup_new_tab.rs
+++ b/src/app_impl/startup_new_tab.rs
@@ -174,7 +174,7 @@
                             }
 
                             // Handle Tab/Shift+Tab in ScriptList view for AI actions.
-                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
+                            // Tab opens Ask AI chat, Shift+Tab opens script generation chat mode.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
@@ -185,7 +185,7 @@
                                 this.filter_text.clear();
 
                                 if has_shift {
-                                    this.generate_script_from_ai_prompt(query, cx);
+                                    this.show_script_generation_chat(Some(query), cx);
                                 } else {
                                     // Show inline AI chat with the query as initial input
                                     this.show_inline_ai_chat(Some(query), cx);
diff --git a/src/prompts/chat/mod.rs b/src/prompts/chat/mod.rs
index 7dd2dc3e82c2339c0af98762e6c00e680be18a93..62192b6f95c7250cce9ab48d271e12ace5ee4647
--- a/src/prompts/chat/mod.rs
+++ b/src/prompts/chat/mod.rs
@@ -49,7 +49,8 @@
 use self::types::{
     build_conversation_turns, default_conversation_starters, next_chat_scroll_follow_state,
     next_reveal_boundary, resolve_chat_input_key_action, resolve_setup_card_key,
-    should_ignore_stream_reveal_update, ChatInputKeyAction, ChatScrollDirection, SetupCardAction,
+    should_ignore_stream_reveal_update, should_show_script_generation_actions,
+    ChatInputKeyAction, ChatScrollDirection, ScriptGenerationAction, SetupCardAction,
 };
 
 pub use prompt::ChatPrompt;
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..f38da4434a11c5693a2baaca2ad71f17d6e04ad8
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,50 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;
diff --git a/src/prompts/chat/types.rs b/src/prompts/chat/types.rs
index 286c060bbf893558e05d81e932ed3ab61a09d06a..2ab2ee2e26318b52e2014c73ae18176ef4e45c6d
--- a/src/prompts/chat/types.rs
+++ b/src/prompts/chat/types.rs
@@ -118,6 +118,27 @@
 }
 
 #[derive(Clone, Copy, Debug, PartialEq, Eq)]
+pub(crate) enum ScriptGenerationAction {
+    Save,
+    Run,
+    SaveAndRun,
+}
+
+impl ScriptGenerationAction {
+    pub(crate) fn should_run_after_save(self) -> bool {
+        matches!(self, Self::Run | Self::SaveAndRun)
+    }
+}
+
+pub(crate) fn should_show_script_generation_actions(
+    script_generation_mode: bool,
+    is_streaming: bool,
+    has_draft: bool,
+) -> bool {
+    script_generation_mode && !is_streaming && has_draft
+}
+
+#[derive(Clone, Copy, Debug, PartialEq, Eq)]
 pub(crate) enum ChatInputKeyAction {
     Escape,
     StopStreaming,

file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index c985a5090574816267103019d1b272e1e3151a38..7f8acfae4036d0f77ae1eca73994c8d188bfb44e
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -464,6 +464,145 @@
         cx.notify();
     }
 
+    /// Show script generation chat prompt with built-in AI provider support.
+    /// This opens ChatPrompt in script-generation mode with the script generation system prompt.
+    pub fn show_script_generation_chat(
+        &mut self,
+        initial_query: Option<String>,
+        cx: &mut Context<Self>,
+    ) {
+        use crate::ai::ProviderRegistry;
+        use crate::prompts::{ChatEscapeCallback, ChatPrompt, ChatSubmitCallback};
+
+        // Mark as opened from main menu so ESC returns to main menu
+        self.opened_from_main_menu = true;
+
+        // Create escape callback that signals via channel
+        let escape_sender = self.inline_chat_escape_sender.clone();
+        let escape_callback: ChatEscapeCallback = std::sync::Arc::new(move |_id| {
+            let _ = escape_sender.try_send(());
+        });
+
+        // Use cached registry if available, otherwise build synchronously as fallback
+        let registry = self
+            .cached_provider_registry
+            .clone()
+            .unwrap_or_else(|| ProviderRegistry::from_environment_with_config(Some(&self.config)));
+
+        if !registry.has_any_provider() {
+            crate::logging::log(
+                "CHAT_SCRIPT_GEN",
+                "No AI providers configured - showing setup card",
+            );
+
+            // Create configure callback that signals via channel
+            let configure_sender = self.inline_chat_configure_sender.clone();
+            let configure_callback: crate::prompts::ChatConfigureCallback =
+                std::sync::Arc::new(move || {
+                    crate::logging::log(
+                        "CHAT_SCRIPT_GEN",
+                        "Configure callback triggered - sending signal",
+                    );
+                    let _ = configure_sender.try_send(());
+                });
+
+            // Create Claude Code callback that signals via channel
+            let claude_code_sender = self.inline_chat_claude_code_sender.clone();
+            let claude_code_callback: crate::prompts::ChatClaudeCodeCallback =
+                std::sync::Arc::new(move || {
+                    crate::logging::log(
+                        "CHAT_SCRIPT_GEN",
+                        "Claude Code callback triggered - sending signal",
+                    );
+                    let _ = claude_code_sender.try_send(());
+                });
+
+            // Create a no-op submit callback since we're in setup mode
+            let noop_callback: ChatSubmitCallback = std::sync::Arc::new(|_id, _text| {
+                crate::logging::log(
+                    "CHAT_SCRIPT_GEN",
+                    "No providers - submission ignored (setup mode)",
+                );
+            });
+
+            let chat_prompt = ChatPrompt::new(
+                "script-generation-setup".to_string(),
+                Some("Configure API key to generate scripts...".to_string()),
+                vec![],
+                None, // No hint needed - setup card is the UI
+                None,
+                self.focus_handle.clone(),
+                noop_callback,
+                std::sync::Arc::clone(&self.theme),
+            )
+            .with_title("Generate Script with AI")
+            .with_save_history(false) // Don't save setup state to history
+            .with_escape_callback(escape_callback.clone())
+            .with_needs_setup(true)
+            .with_configure_callback(configure_callback)
+            .with_claude_code_callback(claude_code_callback);
+
+            let entity = cx.new(|_| chat_prompt);
+            self.current_view = AppView::ChatPrompt {
+                id: "script-generation-setup".to_string(),
+                entity,
+            };
+            self.focused_input = FocusedInput::None;
+            self.pending_focus = Some(FocusTarget::ChatPrompt);
+            resize_to_view_sync(ViewType::DivPrompt, 0);
+            cx.notify();
+            return;
+        }
+
+        crate::logging::log(
+            "CHAT_SCRIPT_GEN",
+            &format!(
+                "Showing script generation chat with {} providers",
+                registry.provider_ids().len()
+            ),
+        );
+
+        // Create a no-op callback since built-in AI handles submissions internally
+        let noop_callback: ChatSubmitCallback = std::sync::Arc::new(|_id, _text| {
+            // Built-in AI mode handles this internally
+        });
+
+        let placeholder = Some("Describe the script you want to generate...".to_string());
+
+        let mut chat_prompt = ChatPrompt::new(
+            "script-generation".to_string(),
+            placeholder,
+            vec![],
+            None,
+            None,
+            self.focus_handle.clone(),
+            noop_callback,
+            std::sync::Arc::clone(&self.theme),
+        )
+        .with_title("Generate Script with AI")
+        .with_save_history(false)
+        .with_escape_callback(escape_callback)
+        .with_builtin_ai(registry, true) // true = prefer Vercel AI Gateway
+        .with_builtin_system_prompt(crate::ai::script_generation::AI_SCRIPT_GENERATION_SYSTEM_PROMPT)
+        .with_script_generation_mode(true);
+
+        // If there's an initial query, set it in the input and auto-submit
+        if let Some(query) = initial_query {
+            chat_prompt.input.set_text(&query);
+            chat_prompt = chat_prompt.with_pending_submit(true);
+        }
+
+        let entity = cx.new(|_| chat_prompt);
+        self.current_view = AppView::ChatPrompt {
+            id: "script-generation".to_string(),
+            entity,
+        };
+        self.focused_input = FocusedInput::None;
+        self.pending_focus = Some(FocusTarget::ChatPrompt);
+        resize_to_view_sync(ViewType::DivPrompt, 0);
+        cx.notify();
+    }
+
     /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
     /// The generated script is saved to disk and opened in the configured editor.
     pub fn generate_script_from_ai_prompt(
diff --git a/src/app_impl/startup.rs b/src/app_impl/startup.rs
index 441b4f23bca1b6294025c698eeb11d45815e85e0..4fd4dc8c23d8df925c657b8db59f888b855ea6a2
--- a/src/app_impl/startup.rs
+++ b/src/app_impl/startup.rs
@@ -723,7 +723,7 @@
                             }
 
                             // Handle Tab/Shift+Tab in ScriptList view for AI actions.
-                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
+                            // Tab opens Ask AI chat, Shift+Tab opens script generation chat mode.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
@@ -734,7 +734,7 @@
                                 this.filter_text.clear();
 
                                 if has_shift {
-                                    this.generate_script_from_ai_prompt(query, cx);
+                                    this.show_script_generation_chat(Some(query), cx);
                                 } else {
                                     // Show inline AI chat with the query as initial input
                                     this.show_inline_ai_chat(Some(query), cx);
diff --git a/src/app_impl/startup_new_tab.rs b/src/app_impl/startup_new_tab.rs
index 7c84270cbe91c0b1ddd460aa29db8c788766d0cf..c75a27bf67a5448f59c7700079e8818210722bde
--- a/src/app_impl/startup_new_tab.rs
+++ b/src/app_impl/startup_new_tab.rs
@@ -174,7 +174,7 @@
                             }
 
                             // Handle Tab/Shift+Tab in ScriptList view for AI actions.
-                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
+                            // Tab opens Ask AI chat, Shift+Tab opens script generation chat mode.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
@@ -185,7 +185,7 @@
                                 this.filter_text.clear();
 
                                 if has_shift {
-                                    this.generate_script_from_ai_prompt(query, cx);
+                                    this.show_script_generation_chat(Some(query), cx);
                                 } else {
                                     // Show inline AI chat with the query as initial input
                                     this.show_inline_ai_chat(Some(query), cx);
diff --git a/src/prompts/chat/mod.rs b/src/prompts/chat/mod.rs
index 7dd2dc3e82c2339c0af98762e6c00e680be18a93..62192b6f95c7250cce9ab48d271e12ace5ee4647
--- a/src/prompts/chat/mod.rs
+++ b/src/prompts/chat/mod.rs
@@ -49,7 +49,8 @@
 use self::types::{
     build_conversation_turns, default_conversation_starters, next_chat_scroll_follow_state,
     next_reveal_boundary, resolve_chat_input_key_action, resolve_setup_card_key,
-    should_ignore_stream_reveal_update, ChatInputKeyAction, ChatScrollDirection, SetupCardAction,
+    should_ignore_stream_reveal_update, should_show_script_generation_actions,
+    ChatInputKeyAction, ChatScrollDirection, ScriptGenerationAction, SetupCardAction,
 };
 
 pub use prompt::ChatPrompt;
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..f38da4434a11c5693a2baaca2ad71f17d6e04ad8
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,50 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;
diff --git a/src/prompts/chat/types.rs b/src/prompts/chat/types.rs
index 286c060bbf893558e05d81e932ed3ab61a09d06a..2ab2ee2e26318b52e2014c73ae18176ef4e45c6d
--- a/src/prompts/chat/types.rs
+++ b/src/prompts/chat/types.rs
@@ -118,6 +118,27 @@
 }
 
 #[derive(Clone, Copy, Debug, PartialEq, Eq)]
+pub(crate) enum ScriptGenerationAction {
+    Save,
+    Run,
+    SaveAndRun,
+}
+
+impl ScriptGenerationAction {
+    pub(crate) fn should_run_after_save(self) -> bool {
+        matches!(self, Self::Run | Self::SaveAndRun)
+    }
+}
+
+pub(crate) fn should_show_script_generation_actions(
+    script_generation_mode: bool,
+    is_streaming: bool,
+    has_draft: bool,
+) -> bool {
+    script_generation_mode && !is_streaming && has_draft
+}
+
+#[derive(Clone, Copy, Debug, PartialEq, Eq)]
 pub(crate) enum ChatInputKeyAction {
     Escape,
     StopStreaming,


thinking
**Refactoring show method to use helper**
file update
M /Users/johnlindquist/dev/script-kit-gpui/src/prompts/chat/prompt.rs
@@ -506,3 +506,7 @@
     pub(super) fn should_show_script_generation_actions(&self) -> bool {
-        self.script_generation_mode && !self.is_streaming() && self.latest_script_generation_draft().is_some()
+        should_show_script_generation_actions(
+            self.script_generation_mode,
+            self.is_streaming(),
+            self.latest_script_generation_draft().is_some(),
+        )
     }
apply_patch(auto_approved=true) exited 0 in 53ms:
Success. Updated the following files:
M /Users/johnlindquist/dev/script-kit-gpui/src/prompts/chat/prompt.rs
file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index c985a5090574816267103019d1b272e1e3151a38..7f8acfae4036d0f77ae1eca73994c8d188bfb44e
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -464,6 +464,145 @@
         cx.notify();
     }
 
+    /// Show script generation chat prompt with built-in AI provider support.
+    /// This opens ChatPrompt in script-generation mode with the script generation system prompt.
+    pub fn show_script_generation_chat(
+        &mut self,
+        initial_query: Option<String>,
+        cx: &mut Context<Self>,
+    ) {
+        use crate::ai::ProviderRegistry;
+        use crate::prompts::{ChatEscapeCallback, ChatPrompt, ChatSubmitCallback};
+
+        // Mark as opened from main menu so ESC returns to main menu
+        self.opened_from_main_menu = true;
+
+        // Create escape callback that signals via channel
+        let escape_sender = self.inline_chat_escape_sender.clone();
+        let escape_callback: ChatEscapeCallback = std::sync::Arc::new(move |_id| {
+            let _ = escape_sender.try_send(());
+        });
+
+        // Use cached registry if available, otherwise build synchronously as fallback
+        let registry = self
+            .cached_provider_registry
+            .clone()
+            .unwrap_or_else(|| ProviderRegistry::from_environment_with_config(Some(&self.config)));
+
+        if !registry.has_any_provider() {
+            crate::logging::log(
+                "CHAT_SCRIPT_GEN",
+                "No AI providers configured - showing setup card",
+            );
+
+            // Create configure callback that signals via channel
+            let configure_sender = self.inline_chat_configure_sender.clone();
+            let configure_callback: crate::prompts::ChatConfigureCallback =
+                std::sync::Arc::new(move || {
+                    crate::logging::log(
+                        "CHAT_SCRIPT_GEN",
+                        "Configure callback triggered - sending signal",
+                    );
+                    let _ = configure_sender.try_send(());
+                });
+
+            // Create Claude Code callback that signals via channel
+            let claude_code_sender = self.inline_chat_claude_code_sender.clone();
+            let claude_code_callback: crate::prompts::ChatClaudeCodeCallback =
+                std::sync::Arc::new(move || {
+                    crate::logging::log(
+                        "CHAT_SCRIPT_GEN",
+                        "Claude Code callback triggered - sending signal",
+                    );
+                    let _ = claude_code_sender.try_send(());
+                });
+
+            // Create a no-op submit callback since we're in setup mode
+            let noop_callback: ChatSubmitCallback = std::sync::Arc::new(|_id, _text| {
+                crate::logging::log(
+                    "CHAT_SCRIPT_GEN",
+                    "No providers - submission ignored (setup mode)",
+                );
+            });
+
+            let chat_prompt = ChatPrompt::new(
+                "script-generation-setup".to_string(),
+                Some("Configure API key to generate scripts...".to_string()),
+                vec![],
+                None, // No hint needed - setup card is the UI
+                None,
+                self.focus_handle.clone(),
+                noop_callback,
+                std::sync::Arc::clone(&self.theme),
+            )
+            .with_title("Generate Script with AI")
+            .with_save_history(false) // Don't save setup state to history
+            .with_escape_callback(escape_callback.clone())
+            .with_needs_setup(true)
+            .with_configure_callback(configure_callback)
+            .with_claude_code_callback(claude_code_callback);
+
+            let entity = cx.new(|_| chat_prompt);
+            self.current_view = AppView::ChatPrompt {
+                id: "script-generation-setup".to_string(),
+                entity,
+            };
+            self.focused_input = FocusedInput::None;
+            self.pending_focus = Some(FocusTarget::ChatPrompt);
+            resize_to_view_sync(ViewType::DivPrompt, 0);
+            cx.notify();
+            return;
+        }
+
+        crate::logging::log(
+            "CHAT_SCRIPT_GEN",
+            &format!(
+                "Showing script generation chat with {} providers",
+                registry.provider_ids().len()
+            ),
+        );
+
+        // Create a no-op callback since built-in AI handles submissions internally
+        let noop_callback: ChatSubmitCallback = std::sync::Arc::new(|_id, _text| {
+            // Built-in AI mode handles this internally
+        });
+
+        let placeholder = Some("Describe the script you want to generate...".to_string());
+
+        let mut chat_prompt = ChatPrompt::new(
+            "script-generation".to_string(),
+            placeholder,
+            vec![],
+            None,
+            None,
+            self.focus_handle.clone(),
+            noop_callback,
+            std::sync::Arc::clone(&self.theme),
+        )
+        .with_title("Generate Script with AI")
+        .with_save_history(false)
+        .with_escape_callback(escape_callback)
+        .with_builtin_ai(registry, true) // true = prefer Vercel AI Gateway
+        .with_builtin_system_prompt(crate::ai::script_generation::AI_SCRIPT_GENERATION_SYSTEM_PROMPT)
+        .with_script_generation_mode(true);
+
+        // If there's an initial query, set it in the input and auto-submit
+        if let Some(query) = initial_query {
+            chat_prompt.input.set_text(&query);
+            chat_prompt = chat_prompt.with_pending_submit(true);
+        }
+
+        let entity = cx.new(|_| chat_prompt);
+        self.current_view = AppView::ChatPrompt {
+            id: "script-generation".to_string(),
+            entity,
+        };
+        self.focused_input = FocusedInput::None;
+        self.pending_focus = Some(FocusTarget::ChatPrompt);
+        resize_to_view_sync(ViewType::DivPrompt, 0);
+        cx.notify();
+    }
+
     /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
     /// The generated script is saved to disk and opened in the configured editor.
     pub fn generate_script_from_ai_prompt(
diff --git a/src/app_impl/startup.rs b/src/app_impl/startup.rs
index 441b4f23bca1b6294025c698eeb11d45815e85e0..4fd4dc8c23d8df925c657b8db59f888b855ea6a2
--- a/src/app_impl/startup.rs
+++ b/src/app_impl/startup.rs
@@ -723,7 +723,7 @@
                             }
 
                             // Handle Tab/Shift+Tab in ScriptList view for AI actions.
-                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
+                            // Tab opens Ask AI chat, Shift+Tab opens script generation chat mode.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
@@ -734,7 +734,7 @@
                                 this.filter_text.clear();
 
                                 if has_shift {
-                                    this.generate_script_from_ai_prompt(query, cx);
+                                    this.show_script_generation_chat(Some(query), cx);
                                 } else {
                                     // Show inline AI chat with the query as initial input
                                     this.show_inline_ai_chat(Some(query), cx);
diff --git a/src/app_impl/startup_new_tab.rs b/src/app_impl/startup_new_tab.rs
index 7c84270cbe91c0b1ddd460aa29db8c788766d0cf..c75a27bf67a5448f59c7700079e8818210722bde
--- a/src/app_impl/startup_new_tab.rs
+++ b/src/app_impl/startup_new_tab.rs
@@ -174,7 +174,7 @@
                             }
 
                             // Handle Tab/Shift+Tab in ScriptList view for AI actions.
-                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
+                            // Tab opens Ask AI chat, Shift+Tab opens script generation chat mode.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
@@ -185,7 +185,7 @@
                                 this.filter_text.clear();
 
                                 if has_shift {
-                                    this.generate_script_from_ai_prompt(query, cx);
+                                    this.show_script_generation_chat(Some(query), cx);
                                 } else {
                                     // Show inline AI chat with the query as initial input
                                     this.show_inline_ai_chat(Some(query), cx);
diff --git a/src/prompts/chat/mod.rs b/src/prompts/chat/mod.rs
index 7dd2dc3e82c2339c0af98762e6c00e680be18a93..62192b6f95c7250cce9ab48d271e12ace5ee4647
--- a/src/prompts/chat/mod.rs
+++ b/src/prompts/chat/mod.rs
@@ -49,7 +49,8 @@
 use self::types::{
     build_conversation_turns, default_conversation_starters, next_chat_scroll_follow_state,
     next_reveal_boundary, resolve_chat_input_key_action, resolve_setup_card_key,
-    should_ignore_stream_reveal_update, ChatInputKeyAction, ChatScrollDirection, SetupCardAction,
+    should_ignore_stream_reveal_update, should_show_script_generation_actions,
+    ChatInputKeyAction, ChatScrollDirection, ScriptGenerationAction, SetupCardAction,
 };
 
 pub use prompt::ChatPrompt;
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..4ef4ac1aae4123a9bdd9771500998f87f424660b
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,54 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        should_show_script_generation_actions(
+            self.script_generation_mode,
+            self.is_streaming(),
+            self.latest_script_generation_draft().is_some(),
+        )
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;
diff --git a/src/prompts/chat/types.rs b/src/prompts/chat/types.rs
index 286c060bbf893558e05d81e932ed3ab61a09d06a..2ab2ee2e26318b52e2014c73ae18176ef4e45c6d
--- a/src/prompts/chat/types.rs
+++ b/src/prompts/chat/types.rs
@@ -118,6 +118,27 @@
 }
 
 #[derive(Clone, Copy, Debug, PartialEq, Eq)]
+pub(crate) enum ScriptGenerationAction {
+    Save,
+    Run,
+    SaveAndRun,
+}
+
+impl ScriptGenerationAction {
+    pub(crate) fn should_run_after_save(self) -> bool {
+        matches!(self, Self::Run | Self::SaveAndRun)
+    }
+}
+
+pub(crate) fn should_show_script_generation_actions(
+    script_generation_mode: bool,
+    is_streaming: bool,
+    has_draft: bool,
+) -> bool {
+    script_generation_mode && !is_streaming && has_draft
+}
+
+#[derive(Clone, Copy, Debug, PartialEq, Eq)]
 pub(crate) enum ChatInputKeyAction {
     Escape,
     StopStreaming,

file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index c985a5090574816267103019d1b272e1e3151a38..7f8acfae4036d0f77ae1eca73994c8d188bfb44e
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -464,6 +464,145 @@
         cx.notify();
     }
 
+    /// Show script generation chat prompt with built-in AI provider support.
+    /// This opens ChatPrompt in script-generation mode with the script generation system prompt.
+    pub fn show_script_generation_chat(
+        &mut self,
+        initial_query: Option<String>,
+        cx: &mut Context<Self>,
+    ) {
+        use crate::ai::ProviderRegistry;
+        use crate::prompts::{ChatEscapeCallback, ChatPrompt, ChatSubmitCallback};
+
+        // Mark as opened from main menu so ESC returns to main menu
+        self.opened_from_main_menu = true;
+
+        // Create escape callback that signals via channel
+        let escape_sender = self.inline_chat_escape_sender.clone();
+        let escape_callback: ChatEscapeCallback = std::sync::Arc::new(move |_id| {
+            let _ = escape_sender.try_send(());
+        });
+
+        // Use cached registry if available, otherwise build synchronously as fallback
+        let registry = self
+            .cached_provider_registry
+            .clone()
+            .unwrap_or_else(|| ProviderRegistry::from_environment_with_config(Some(&self.config)));
+
+        if !registry.has_any_provider() {
+            crate::logging::log(
+                "CHAT_SCRIPT_GEN",
+                "No AI providers configured - showing setup card",
+            );
+
+            // Create configure callback that signals via channel
+            let configure_sender = self.inline_chat_configure_sender.clone();
+            let configure_callback: crate::prompts::ChatConfigureCallback =
+                std::sync::Arc::new(move || {
+                    crate::logging::log(
+                        "CHAT_SCRIPT_GEN",
+                        "Configure callback triggered - sending signal",
+                    );
+                    let _ = configure_sender.try_send(());
+                });
+
+            // Create Claude Code callback that signals via channel
+            let claude_code_sender = self.inline_chat_claude_code_sender.clone();
+            let claude_code_callback: crate::prompts::ChatClaudeCodeCallback =
+                std::sync::Arc::new(move || {
+                    crate::logging::log(
+                        "CHAT_SCRIPT_GEN",
+                        "Claude Code callback triggered - sending signal",
+                    );
+                    let _ = claude_code_sender.try_send(());
+                });
+
+            // Create a no-op submit callback since we're in setup mode
+            let noop_callback: ChatSubmitCallback = std::sync::Arc::new(|_id, _text| {
+                crate::logging::log(
+                    "CHAT_SCRIPT_GEN",
+                    "No providers - submission ignored (setup mode)",
+                );
+            });
+
+            let chat_prompt = ChatPrompt::new(
+                "script-generation-setup".to_string(),
+                Some("Configure API key to generate scripts...".to_string()),
+                vec![],
+                None, // No hint needed - setup card is the UI
+                None,
+                self.focus_handle.clone(),
+                noop_callback,
+                std::sync::Arc::clone(&self.theme),
+            )
+            .with_title("Generate Script with AI")
+            .with_save_history(false) // Don't save setup state to history
+            .with_escape_callback(escape_callback.clone())
+            .with_needs_setup(true)
+            .with_configure_callback(configure_callback)
+            .with_claude_code_callback(claude_code_callback);
+
+            let entity = cx.new(|_| chat_prompt);
+            self.current_view = AppView::ChatPrompt {
+                id: "script-generation-setup".to_string(),
+                entity,
+            };
+            self.focused_input = FocusedInput::None;
+            self.pending_focus = Some(FocusTarget::ChatPrompt);
+            resize_to_view_sync(ViewType::DivPrompt, 0);
+            cx.notify();
+            return;
+        }
+
+        crate::logging::log(
+            "CHAT_SCRIPT_GEN",
+            &format!(
+                "Showing script generation chat with {} providers",
+                registry.provider_ids().len()
+            ),
+        );
+
+        // Create a no-op callback since built-in AI handles submissions internally
+        let noop_callback: ChatSubmitCallback = std::sync::Arc::new(|_id, _text| {
+            // Built-in AI mode handles this internally
+        });
+
+        let placeholder = Some("Describe the script you want to generate...".to_string());
+
+        let mut chat_prompt = ChatPrompt::new(
+            "script-generation".to_string(),
+            placeholder,
+            vec![],
+            None,
+            None,
+            self.focus_handle.clone(),
+            noop_callback,
+            std::sync::Arc::clone(&self.theme),
+        )
+        .with_title("Generate Script with AI")
+        .with_save_history(false)
+        .with_escape_callback(escape_callback)
+        .with_builtin_ai(registry, true) // true = prefer Vercel AI Gateway
+        .with_builtin_system_prompt(crate::ai::script_generation::AI_SCRIPT_GENERATION_SYSTEM_PROMPT)
+        .with_script_generation_mode(true);
+
+        // If there's an initial query, set it in the input and auto-submit
+        if let Some(query) = initial_query {
+            chat_prompt.input.set_text(&query);
+            chat_prompt = chat_prompt.with_pending_submit(true);
+        }
+
+        let entity = cx.new(|_| chat_prompt);
+        self.current_view = AppView::ChatPrompt {
+            id: "script-generation".to_string(),
+            entity,
+        };
+        self.focused_input = FocusedInput::None;
+        self.pending_focus = Some(FocusTarget::ChatPrompt);
+        resize_to_view_sync(ViewType::DivPrompt, 0);
+        cx.notify();
+    }
+
     /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
     /// The generated script is saved to disk and opened in the configured editor.
     pub fn generate_script_from_ai_prompt(
diff --git a/src/app_impl/startup.rs b/src/app_impl/startup.rs
index 441b4f23bca1b6294025c698eeb11d45815e85e0..4fd4dc8c23d8df925c657b8db59f888b855ea6a2
--- a/src/app_impl/startup.rs
+++ b/src/app_impl/startup.rs
@@ -723,7 +723,7 @@
                             }
 
                             // Handle Tab/Shift+Tab in ScriptList view for AI actions.
-                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
+                            // Tab opens Ask AI chat, Shift+Tab opens script generation chat mode.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
@@ -734,7 +734,7 @@
                                 this.filter_text.clear();
 
                                 if has_shift {
-                                    this.generate_script_from_ai_prompt(query, cx);
+                                    this.show_script_generation_chat(Some(query), cx);
                                 } else {
                                     // Show inline AI chat with the query as initial input
                                     this.show_inline_ai_chat(Some(query), cx);
diff --git a/src/app_impl/startup_new_tab.rs b/src/app_impl/startup_new_tab.rs
index 7c84270cbe91c0b1ddd460aa29db8c788766d0cf..c75a27bf67a5448f59c7700079e8818210722bde
--- a/src/app_impl/startup_new_tab.rs
+++ b/src/app_impl/startup_new_tab.rs
@@ -174,7 +174,7 @@
                             }
 
                             // Handle Tab/Shift+Tab in ScriptList view for AI actions.
-                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
+                            // Tab opens Ask AI chat, Shift+Tab opens script generation chat mode.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
@@ -185,7 +185,7 @@
                                 this.filter_text.clear();
 
                                 if has_shift {
-                                    this.generate_script_from_ai_prompt(query, cx);
+                                    this.show_script_generation_chat(Some(query), cx);
                                 } else {
                                     // Show inline AI chat with the query as initial input
                                     this.show_inline_ai_chat(Some(query), cx);
diff --git a/src/prompts/chat/mod.rs b/src/prompts/chat/mod.rs
index 7dd2dc3e82c2339c0af98762e6c00e680be18a93..62192b6f95c7250cce9ab48d271e12ace5ee4647
--- a/src/prompts/chat/mod.rs
+++ b/src/prompts/chat/mod.rs
@@ -49,7 +49,8 @@
 use self::types::{
     build_conversation_turns, default_conversation_starters, next_chat_scroll_follow_state,
     next_reveal_boundary, resolve_chat_input_key_action, resolve_setup_card_key,
-    should_ignore_stream_reveal_update, ChatInputKeyAction, ChatScrollDirection, SetupCardAction,
+    should_ignore_stream_reveal_update, should_show_script_generation_actions,
+    ChatInputKeyAction, ChatScrollDirection, ScriptGenerationAction, SetupCardAction,
 };
 
 pub use prompt::ChatPrompt;
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..4ef4ac1aae4123a9bdd9771500998f87f424660b
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,54 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        should_show_script_generation_actions(
+            self.script_generation_mode,
+            self.is_streaming(),
+            self.latest_script_generation_draft().is_some(),
+        )
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;
diff --git a/src/prompts/chat/types.rs b/src/prompts/chat/types.rs
index 286c060bbf893558e05d81e932ed3ab61a09d06a..2ab2ee2e26318b52e2014c73ae18176ef4e45c6d
--- a/src/prompts/chat/types.rs
+++ b/src/prompts/chat/types.rs
@@ -118,6 +118,27 @@
 }
 
 #[derive(Clone, Copy, Debug, PartialEq, Eq)]
+pub(crate) enum ScriptGenerationAction {
+    Save,
+    Run,
+    SaveAndRun,
+}
+
+impl ScriptGenerationAction {
+    pub(crate) fn should_run_after_save(self) -> bool {
+        matches!(self, Self::Run | Self::SaveAndRun)
+    }
+}
+
+pub(crate) fn should_show_script_generation_actions(
+    script_generation_mode: bool,
+    is_streaming: bool,
+    has_draft: bool,
+) -> bool {
+    script_generation_mode && !is_streaming && has_draft
+}
+
+#[derive(Clone, Copy, Debug, PartialEq, Eq)]
 pub(crate) enum ChatInputKeyAction {
     Escape,
     StopStreaming,


thinking
**Planning handler addition**
exec
/bin/zsh -lc "sed -n '1,260p' src/prompts/chat/actions.rs" in /Users/johnlindquist/dev/script-kit-gpui succeeded in 51ms:
use super::*;

impl ChatPrompt {
    pub(super) fn handle_escape(&mut self, _cx: &mut Context<Self>) {
        logging::log("CHAT", "Escape pressed - closing chat");

        // Save conversation to database if save_history is enabled
        if self.save_history {
            self.save_to_database();
        }

        if let Some(ref callback) = self.on_escape {
            callback(self.id.clone());
        }
    }

    /// Save the current conversation to the AI chats database
    pub(super) fn save_to_database(&self) {
        // Only save if we have messages
        if self.messages.is_empty() {
file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index c985a5090574816267103019d1b272e1e3151a38..7f8acfae4036d0f77ae1eca73994c8d188bfb44e
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -464,6 +464,145 @@
         cx.notify();
     }
 
+    /// Show script generation chat prompt with built-in AI provider support.
+    /// This opens ChatPrompt in script-generation mode with the script generation system prompt.
+    pub fn show_script_generation_chat(
+        &mut self,
+        initial_query: Option<String>,
+        cx: &mut Context<Self>,
+    ) {
+        use crate::ai::ProviderRegistry;
+        use crate::prompts::{ChatEscapeCallback, ChatPrompt, ChatSubmitCallback};
+
+        // Mark as opened from main menu so ESC returns to main menu
+        self.opened_from_main_menu = true;
+
+        // Create escape callback that signals via channel
+        let escape_sender = self.inline_chat_escape_sender.clone();
+        let escape_callback: ChatEscapeCallback = std::sync::Arc::new(move |_id| {
+            let _ = escape_sender.try_send(());
+        });
+
+        // Use cached registry if available, otherwise build synchronously as fallback
+        let registry = self
+            .cached_provider_registry
+            .clone()
+            .unwrap_or_else(|| ProviderRegistry::from_environment_with_config(Some(&self.config)));
+
+        if !registry.has_any_provider() {
+            crate::logging::log(
+                "CHAT_SCRIPT_GEN",
+                "No AI providers configured - showing setup card",
+            );
+
+            // Create configure callback that signals via channel
+            let configure_sender = self.inline_chat_configure_sender.clone();
+            let configure_callback: crate::prompts::ChatConfigureCallback =
+                std::sync::Arc::new(move || {
+                    crate::logging::log(
+                        "CHAT_SCRIPT_GEN",
+                        "Configure callback triggered - sending signal",
+                    );
+                    let _ = configure_sender.try_send(());
+                });
+
+            // Create Claude Code callback that signals via channel
+            let claude_code_sender = self.inline_chat_claude_code_sender.clone();
+            let claude_code_callback: crate::prompts::ChatClaudeCodeCallback =
+                std::sync::Arc::new(move || {
+                    crate::logging::log(
+                        "CHAT_SCRIPT_GEN",
+                        "Claude Code callback triggered - sending signal",
+                    );
+                    let _ = claude_code_sender.try_send(());
+                });
+
+            // Create a no-op submit callback since we're in setup mode
+            let noop_callback: ChatSubmitCallback = std::sync::Arc::new(|_id, _text| {
+                crate::logging::log(
+                    "CHAT_SCRIPT_GEN",
+                    "No providers - submission ignored (setup mode)",
+                );
+            });
+
+            let chat_prompt = ChatPrompt::new(
+                "script-generation-setup".to_string(),
+                Some("Configure API key to generate scripts...".to_string()),
+                vec![],
+                None, // No hint needed - setup card is the UI
+                None,
+                self.focus_handle.clone(),
+                noop_callback,
+                std::sync::Arc::clone(&self.theme),
+            )
+            .with_title("Generate Script with AI")
+            .with_save_history(false) // Don't save setup state to history
+            .with_escape_callback(escape_callback.clone())
+            .with_needs_setup(true)
+            .with_configure_callback(configure_callback)
+            .with_claude_code_callback(claude_code_callback);
+
+            let entity = cx.new(|_| chat_prompt);
+            self.current_view = AppView::ChatPrompt {
+                id: "script-generation-setup".to_string(),
+                entity,
+            };
+            self.focused_input = FocusedInput::None;
+            self.pending_focus = Some(FocusTarget::ChatPrompt);
+            resize_to_view_sync(ViewType::DivPrompt, 0);
+            cx.notify();
+            return;
+        }
+
+        crate::logging::log(
+            "CHAT_SCRIPT_GEN",
+            &format!(
+                "Showing script generation chat with {} providers",
+                registry.provider_ids().len()
+            ),
+        );
+
+        // Create a no-op callback since built-in AI handles submissions internally
+        let noop_callback: ChatSubmitCallback = std::sync::Arc::new(|_id, _text| {
+            // Built-in AI mode handles this internally
+        });
+
+        let placeholder = Some("Describe the script you want to generate...".to_string());
+
+        let mut chat_prompt = ChatPrompt::new(
+            "script-generation".to_string(),
+            placeholder,
+            vec![],
+            None,
+            None,
+            self.focus_handle.clone(),
+            noop_callback,
+            std::sync::Arc::clone(&self.theme),
+        )
+        .with_title("Generate Script with AI")
+        .with_save_history(false)
+        .with_escape_callback(escape_callback)
+        .with_builtin_ai(registry, true) // true = prefer Vercel AI Gateway
+        .with_builtin_system_prompt(crate::ai::script_generation::AI_SCRIPT_GENERATION_SYSTEM_PROMPT)
+        .with_script_generation_mode(true);
+
+        // If there's an initial query, set it in the input and auto-submit
+        if let Some(query) = initial_query {
+            chat_prompt.input.set_text(&query);
+            chat_prompt = chat_prompt.with_pending_submit(true);
+        }
+
+        let entity = cx.new(|_| chat_prompt);
+        self.current_view = AppView::ChatPrompt {
+            id: "script-generation".to_string(),
+            entity,
+        };
+        self.focused_input = FocusedInput::None;
+        self.pending_focus = Some(FocusTarget::ChatPrompt);
+        resize_to_view_sync(ViewType::DivPrompt, 0);
+        cx.notify();
+    }
+
     /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
     /// The generated script is saved to disk and opened in the configured editor.
     pub fn generate_script_from_ai_prompt(
diff --git a/src/app_impl/startup.rs b/src/app_impl/startup.rs
index 441b4f23bca1b6294025c698eeb11d45815e85e0..4fd4dc8c23d8df925c657b8db59f888b855ea6a2
--- a/src/app_impl/startup.rs
+++ b/src/app_impl/startup.rs
@@ -723,7 +723,7 @@
                             }
 
                             // Handle Tab/Shift+Tab in ScriptList view for AI actions.
-                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
+                            // Tab opens Ask AI chat, Shift+Tab opens script generation chat mode.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
@@ -734,7 +734,7 @@
                                 this.filter_text.clear();
 
                                 if has_shift {
-                                    this.generate_script_from_ai_prompt(query, cx);
+                                    this.show_script_generation_chat(Some(query), cx);
                                 } else {
                                     // Show inline AI chat with the query as initial input
                                     this.show_inline_ai_chat(Some(query), cx);
diff --git a/src/app_impl/startup_new_tab.rs b/src/app_impl/startup_new_tab.rs
index 7c84270cbe91c0b1ddd460aa29db8c788766d0cf..c75a27bf67a5448f59c7700079e8818210722bde
--- a/src/app_impl/startup_new_tab.rs
+++ b/src/app_impl/startup_new_tab.rs
@@ -174,7 +174,7 @@
                             }
 
                             // Handle Tab/Shift+Tab in ScriptList view for AI actions.
-                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
+                            // Tab opens Ask AI chat, Shift+Tab opens script generation chat mode.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
@@ -185,7 +185,7 @@
                                 this.filter_text.clear();
 
                                 if has_shift {
-                                    this.generate_script_from_ai_prompt(query, cx);
+                                    this.show_script_generation_chat(Some(query), cx);
                                 } else {
                                     // Show inline AI chat with the query as initial input
                                     this.show_inline_ai_chat(Some(query), cx);
diff --git a/src/prompts/chat/mod.rs b/src/prompts/chat/mod.rs
index 7dd2dc3e82c2339c0af98762e6c00e680be18a93..62192b6f95c7250cce9ab48d271e12ace5ee4647
--- a/src/prompts/chat/mod.rs
+++ b/src/prompts/chat/mod.rs
@@ -49,7 +49,8 @@
 use self::types::{
     build_conversation_turns, default_conversation_starters, next_chat_scroll_follow_state,
     next_reveal_boundary, resolve_chat_input_key_action, resolve_setup_card_key,
-    should_ignore_stream_reveal_update, ChatInputKeyAction, ChatScrollDirection, SetupCardAction,
+    should_ignore_stream_reveal_update, should_show_script_generation_actions,
+    ChatInputKeyAction, ChatScrollDirection, ScriptGenerationAction, SetupCardAction,
 };
 
 pub use prompt::ChatPrompt;
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..4ef4ac1aae4123a9bdd9771500998f87f424660b
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,54 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        should_show_script_generation_actions(
+            self.script_generation_mode,
+            self.is_streaming(),
+            self.latest_script_generation_draft().is_some(),
+        )
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;
diff --git a/src/prompts/chat/types.rs b/src/prompts/chat/types.rs
index 286c060bbf893558e05d81e932ed3ab61a09d06a..2ab2ee2e26318b52e2014c73ae18176ef4e45c6d
--- a/src/prompts/chat/types.rs
+++ b/src/prompts/chat/types.rs
@@ -118,6 +118,27 @@
 }
 
 #[derive(Clone, Copy, Debug, PartialEq, Eq)]
+pub(crate) enum ScriptGenerationAction {
+    Save,
+    Run,
+    SaveAndRun,
+}
+
+impl ScriptGenerationAction {
+    pub(crate) fn should_run_after_save(self) -> bool {
+        matches!(self, Self::Run | Self::SaveAndRun)
+    }
+}
+
+pub(crate) fn should_show_script_generation_actions(
+    script_generation_mode: bool,
+    is_streaming: bool,
+    has_draft: bool,
+) -> bool {
+    script_generation_mode && !is_streaming && has_draft
+}
+
+#[derive(Clone, Copy, Debug, PartialEq, Eq)]
 pub(crate) enum ChatInputKeyAction {
     Escape,
     StopStreaming,


thinking
**Planning handler insertion point**
file update
M /Users/johnlindquist/dev/script-kit-gpui/src/prompts/chat/actions.rs
@@ -131,2 +131,91 @@
 
+    pub(super) fn handle_script_generation_action(
+        &mut self,
+        action: ScriptGenerationAction,
+        cx: &mut Context<Self>,
+    ) {
+        let Some((prompt_description, raw_response)) = self.latest_script_generation_draft() else {
+            self.set_script_generation_status(true, "No generated script to save yet.", cx);
+            return;
+        };
+
+        logging::log(
+            "CHAT_SCRIPT_GEN",
+            &format!(
+                "state=save_requested action={:?} prompt_len={} response_len={}",
+                action,
+                prompt_description.len(),
+                raw_response.len()
+            ),
+        );
+
+        let script_path = match crate::ai::script_generation::save_generated_script_from_response(
+            &prompt_description,
+            &raw_response,
+        ) {
+            Ok(path) => path,
+            Err(error) => {
+                self.set_script_generation_status(
+                    true,
+                    format!("Failed to save script: {}", error),
+                    cx,
+                );
+                return;
+            }
+        };
+
+        let script_name = script_path
+            .file_name()
+            .and_then(|name| name.to_str())
+            .unwrap_or("generated script")
+            .to_string();
+
+        if action.should_run_after_save() {
+            match crate::ai::script_generation::run_saved_script_detached(&script_path) {
+                Ok(()) => {
+                    self.set_script_generation_status(
+                        false,
+                        format!("Saved and running {}", script_name),
+                        cx,
+                    );
+                    logging::log(
+                        "CHAT_SCRIPT_GEN",
+                        &format!(
+                            "state=run_started action={:?} path={}",
+                            action,
+                            script_path.display()
+                        ),
+                    );
+                }
+                Err(error) => {
+                    self.set_script_generation_status(
+                        true,
+                        format!("Saved {} but failed to run: {}", script_name, error),
+                        cx,
+                    );
+                    logging::log(
+                        "CHAT_SCRIPT_GEN",
+                        &format!(
+                            "state=run_failed action={:?} path={} error={}",
+                            action,
+                            script_path.display(),
+                            error
+                        ),
+                    );
+                }
+            }
+            return;
+        }
+
+        self.set_script_generation_status(false, format!("Saved {}", script_name), cx);
+        logging::log(
+            "CHAT_SCRIPT_GEN",
+            &format!(
+                "state=saved_only action={:?} path={}",
+                action,
+                script_path.display()
+            ),
+        );
+    }
+
     // ============================================
apply_patch(auto_approved=true) exited 0 in 51ms:
Success. Updated the following files:
M /Users/johnlindquist/dev/script-kit-gpui/src/prompts/chat/actions.rs
file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index c985a5090574816267103019d1b272e1e3151a38..7f8acfae4036d0f77ae1eca73994c8d188bfb44e
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -464,6 +464,145 @@
         cx.notify();
     }
 
+    /// Show script generation chat prompt with built-in AI provider support.
+    /// This opens ChatPrompt in script-generation mode with the script generation system prompt.
+    pub fn show_script_generation_chat(
+        &mut self,
+        initial_query: Option<String>,
+        cx: &mut Context<Self>,
+    ) {
+        use crate::ai::ProviderRegistry;
+        use crate::prompts::{ChatEscapeCallback, ChatPrompt, ChatSubmitCallback};
+
+        // Mark as opened from main menu so ESC returns to main menu
+        self.opened_from_main_menu = true;
+
+        // Create escape callback that signals via channel
+        let escape_sender = self.inline_chat_escape_sender.clone();
+        let escape_callback: ChatEscapeCallback = std::sync::Arc::new(move |_id| {
+            let _ = escape_sender.try_send(());
+        });
+
+        // Use cached registry if available, otherwise build synchronously as fallback
+        let registry = self
+            .cached_provider_registry
+            .clone()
+            .unwrap_or_else(|| ProviderRegistry::from_environment_with_config(Some(&self.config)));
+
+        if !registry.has_any_provider() {
+            crate::logging::log(
+                "CHAT_SCRIPT_GEN",
+                "No AI providers configured - showing setup card",
+            );
+
+            // Create configure callback that signals via channel
+            let configure_sender = self.inline_chat_configure_sender.clone();
+            let configure_callback: crate::prompts::ChatConfigureCallback =
+                std::sync::Arc::new(move || {
+                    crate::logging::log(
+                        "CHAT_SCRIPT_GEN",
+                        "Configure callback triggered - sending signal",
+                    );
+                    let _ = configure_sender.try_send(());
+                });
+
+            // Create Claude Code callback that signals via channel
+            let claude_code_sender = self.inline_chat_claude_code_sender.clone();
+            let claude_code_callback: crate::prompts::ChatClaudeCodeCallback =
+                std::sync::Arc::new(move || {
+                    crate::logging::log(
+                        "CHAT_SCRIPT_GEN",
+                        "Claude Code callback triggered - sending signal",
+                    );
+                    let _ = claude_code_sender.try_send(());
+                });
+
+            // Create a no-op submit callback since we're in setup mode
+            let noop_callback: ChatSubmitCallback = std::sync::Arc::new(|_id, _text| {
+                crate::logging::log(
+                    "CHAT_SCRIPT_GEN",
+                    "No providers - submission ignored (setup mode)",
+                );
+            });
+
+            let chat_prompt = ChatPrompt::new(
+                "script-generation-setup".to_string(),
+                Some("Configure API key to generate scripts...".to_string()),
+                vec![],
+                None, // No hint needed - setup card is the UI
+                None,
+                self.focus_handle.clone(),
+                noop_callback,
+                std::sync::Arc::clone(&self.theme),
+            )
+            .with_title("Generate Script with AI")
+            .with_save_history(false) // Don't save setup state to history
+            .with_escape_callback(escape_callback.clone())
+            .with_needs_setup(true)
+            .with_configure_callback(configure_callback)
+            .with_claude_code_callback(claude_code_callback);
+
+            let entity = cx.new(|_| chat_prompt);
+            self.current_view = AppView::ChatPrompt {
+                id: "script-generation-setup".to_string(),
+                entity,
+            };
+            self.focused_input = FocusedInput::None;
+            self.pending_focus = Some(FocusTarget::ChatPrompt);
+            resize_to_view_sync(ViewType::DivPrompt, 0);
+            cx.notify();
+            return;
+        }
+
+        crate::logging::log(
+            "CHAT_SCRIPT_GEN",
+            &format!(
+                "Showing script generation chat with {} providers",
+                registry.provider_ids().len()
+            ),
+        );
+
+        // Create a no-op callback since built-in AI handles submissions internally
+        let noop_callback: ChatSubmitCallback = std::sync::Arc::new(|_id, _text| {
+            // Built-in AI mode handles this internally
+        });
+
+        let placeholder = Some("Describe the script you want to generate...".to_string());
+
+        let mut chat_prompt = ChatPrompt::new(
+            "script-generation".to_string(),
+            placeholder,
+            vec![],
+            None,
+            None,
+            self.focus_handle.clone(),
+            noop_callback,
+            std::sync::Arc::clone(&self.theme),
+        )
+        .with_title("Generate Script with AI")
+        .with_save_history(false)
+        .with_escape_callback(escape_callback)
+        .with_builtin_ai(registry, true) // true = prefer Vercel AI Gateway
+        .with_builtin_system_prompt(crate::ai::script_generation::AI_SCRIPT_GENERATION_SYSTEM_PROMPT)
+        .with_script_generation_mode(true);
+
+        // If there's an initial query, set it in the input and auto-submit
+        if let Some(query) = initial_query {
+            chat_prompt.input.set_text(&query);
+            chat_prompt = chat_prompt.with_pending_submit(true);
+        }
+
+        let entity = cx.new(|_| chat_prompt);
+        self.current_view = AppView::ChatPrompt {
+            id: "script-generation".to_string(),
+            entity,
+        };
+        self.focused_input = FocusedInput::None;
+        self.pending_focus = Some(FocusTarget::ChatPrompt);
+        resize_to_view_sync(ViewType::DivPrompt, 0);
+        cx.notify();
+    }
+
     /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
     /// The generated script is saved to disk and opened in the configured editor.
     pub fn generate_script_from_ai_prompt(
diff --git a/src/app_impl/startup.rs b/src/app_impl/startup.rs
index 441b4f23bca1b6294025c698eeb11d45815e85e0..4fd4dc8c23d8df925c657b8db59f888b855ea6a2
--- a/src/app_impl/startup.rs
+++ b/src/app_impl/startup.rs
@@ -723,7 +723,7 @@
                             }
 
                             // Handle Tab/Shift+Tab in ScriptList view for AI actions.
-                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
+                            // Tab opens Ask AI chat, Shift+Tab opens script generation chat mode.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
@@ -734,7 +734,7 @@
                                 this.filter_text.clear();
 
                                 if has_shift {
-                                    this.generate_script_from_ai_prompt(query, cx);
+                                    this.show_script_generation_chat(Some(query), cx);
                                 } else {
                                     // Show inline AI chat with the query as initial input
                                     this.show_inline_ai_chat(Some(query), cx);
diff --git a/src/app_impl/startup_new_tab.rs b/src/app_impl/startup_new_tab.rs
index 7c84270cbe91c0b1ddd460aa29db8c788766d0cf..c75a27bf67a5448f59c7700079e8818210722bde
--- a/src/app_impl/startup_new_tab.rs
+++ b/src/app_impl/startup_new_tab.rs
@@ -174,7 +174,7 @@
                             }
 
                             // Handle Tab/Shift+Tab in ScriptList view for AI actions.
-                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
+                            // Tab opens Ask AI chat, Shift+Tab opens script generation chat mode.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
@@ -185,7 +185,7 @@
                                 this.filter_text.clear();
 
                                 if has_shift {
-                                    this.generate_script_from_ai_prompt(query, cx);
+                                    this.show_script_generation_chat(Some(query), cx);
                                 } else {
                                     // Show inline AI chat with the query as initial input
                                     this.show_inline_ai_chat(Some(query), cx);
diff --git a/src/prompts/chat/actions.rs b/src/prompts/chat/actions.rs
index 74a2c7b68f58e938dfe8c80add589e83861a64f3..9192b2250531aa674b529af2b11a26d1690f8307
--- a/src/prompts/chat/actions.rs
+++ b/src/prompts/chat/actions.rs
@@ -129,6 +129,95 @@
         self.clear_messages(cx);
     }
 
+    pub(super) fn handle_script_generation_action(
+        &mut self,
+        action: ScriptGenerationAction,
+        cx: &mut Context<Self>,
+    ) {
+        let Some((prompt_description, raw_response)) = self.latest_script_generation_draft() else {
+            self.set_script_generation_status(true, "No generated script to save yet.", cx);
+            return;
+        };
+
+        logging::log(
+            "CHAT_SCRIPT_GEN",
+            &format!(
+                "state=save_requested action={:?} prompt_len={} response_len={}",
+                action,
+                prompt_description.len(),
+                raw_response.len()
+            ),
+        );
+
+        let script_path = match crate::ai::script_generation::save_generated_script_from_response(
+            &prompt_description,
+            &raw_response,
+        ) {
+            Ok(path) => path,
+            Err(error) => {
+                self.set_script_generation_status(
+                    true,
+                    format!("Failed to save script: {}", error),
+                    cx,
+                );
+                return;
+            }
+        };
+
+        let script_name = script_path
+            .file_name()
+            .and_then(|name| name.to_str())
+            .unwrap_or("generated script")
+            .to_string();
+
+        if action.should_run_after_save() {
+            match crate::ai::script_generation::run_saved_script_detached(&script_path) {
+                Ok(()) => {
+                    self.set_script_generation_status(
+                        false,
+                        format!("Saved and running {}", script_name),
+                        cx,
+                    );
+                    logging::log(
+                        "CHAT_SCRIPT_GEN",
+                        &format!(
+                            "state=run_started action={:?} path={}",
+                            action,
+                            script_path.display()
+                        ),
+                    );
+                }
+                Err(error) => {
+                    self.set_script_generation_status(
+                        true,
+                        format!("Saved {} but failed to run: {}", script_name, error),
+                        cx,
+                    );
+                    logging::log(
+                        "CHAT_SCRIPT_GEN",
+                        &format!(
+                            "state=run_failed action={:?} path={} error={}",
+                            action,
+                            script_path.display(),
+                            error
+                        ),
+                    );
+                }
+            }
+            return;
+        }
+
+        self.set_script_generation_status(false, format!("Saved {}", script_name), cx);
+        logging::log(
+            "CHAT_SCRIPT_GEN",
+            &format!(
+                "state=saved_only action={:?} path={}",
+                action,
+                script_path.display()
+            ),
+        );
+    }
+
     // ============================================
     // Actions Menu Methods
     // ============================================
diff --git a/src/prompts/chat/mod.rs b/src/prompts/chat/mod.rs
index 7dd2dc3e82c2339c0af98762e6c00e680be18a93..62192b6f95c7250cce9ab48d271e12ace5ee4647
--- a/src/prompts/chat/mod.rs
+++ b/src/prompts/chat/mod.rs
@@ -49,7 +49,8 @@
 use self::types::{
     build_conversation_turns, default_conversation_starters, next_chat_scroll_follow_state,
     next_reveal_boundary, resolve_chat_input_key_action, resolve_setup_card_key,
-    should_ignore_stream_reveal_update, ChatInputKeyAction, ChatScrollDirection, SetupCardAction,
+    should_ignore_stream_reveal_update, should_show_script_generation_actions,
+    ChatInputKeyAction, ChatScrollDirection, ScriptGenerationAction, SetupCardAction,
 };
 
 pub use prompt::ChatPrompt;
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..4ef4ac1aae4123a9bdd9771500998f87f424660b
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,54 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        should_show_script_generation_actions(
+            self.script_generation_mode,
+            self.is_streaming(),
+            self.latest_script_generation_draft().is_some(),
+        )
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;
diff --git a/src/prompts/chat/types.rs b/src/prompts/chat/types.rs
index 286c060bbf893558e05d81e932ed3ab61a09d06a..2ab2ee2e26318b52e2014c73ae18176ef4e45c6d
--- a/src/prompts/chat/types.rs
+++ b/src/prompts/chat/types.rs
@@ -118,6 +118,27 @@
 }
 
 #[derive(Clone, Copy, Debug, PartialEq, Eq)]
+pub(crate) enum ScriptGenerationAction {
+    Save,
+    Run,
+    SaveAndRun,
+}
+
+impl ScriptGenerationAction {
+    pub(crate) fn should_run_after_save(self) -> bool {
+        matches!(self, Self::Run | Self::SaveAndRun)
+    }
+}
+
+pub(crate) fn should_show_script_generation_actions(
+    script_generation_mode: bool,
+    is_streaming: bool,
+    has_draft: bool,
+) -> bool {
+    script_generation_mode && !is_streaming && has_draft
+}
+
+#[derive(Clone, Copy, Debug, PartialEq, Eq)]
 pub(crate) enum ChatInputKeyAction {
     Escape,
     StopStreaming,

file update:
diff --git a/src/ai/script_generation.rs b/src/ai/script_generation.rs
index 8338b9cbb096368183d0be326e2f73ccefd03a80..5cbd63a49561f2d99da3a8c0766ede57d69dfa2a
--- a/src/ai/script_generation.rs
+++ b/src/ai/script_generation.rs
@@ -11,7 +11,7 @@
 const AI_SCRIPT_DEFAULT_SLUG: &str = "ai-script";
 const AI_SCRIPT_MAX_SLUG_LEN: usize = 64;
 
-const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
+pub(crate) const AI_SCRIPT_GENERATION_SYSTEM_PROMPT: &str = r#"You write production-ready Script Kit scripts.
 
 Return ONLY TypeScript source code for one Script Kit script. Do not include explanations.
 
@@ -76,13 +76,7 @@
             )
         })?;
 
-    let extracted = extract_script_code(&raw_response);
-    if extracted.trim().is_empty() {
-        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
-    }
-
-    let slug = slugify_script_name(normalized_prompt);
-    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    let (slug, finalized) = prepare_script_from_ai_response(normalized_prompt, &raw_response)?;
     let path = write_generated_script(&slug, &finalized).with_context(|| {
         format!(
             "Failed writing AI-generated script (state=write_failed, slug={})",
@@ -107,6 +101,77 @@
     })
 }
 
+pub(crate) fn prepare_script_from_ai_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<(String, String)> {
+    let normalized_prompt = prompt.trim();
+    if normalized_prompt.is_empty() {
+        anyhow::bail!("AI script generation requires a non-empty prompt");
+    }
+
+    let extracted = extract_script_code(raw_response);
+    if extracted.trim().is_empty() {
+        anyhow::bail!("AI returned an empty response for script generation (state=empty_response)");
+    }
+
+    let slug = slugify_script_name(normalized_prompt);
+    let finalized = enforce_script_kit_conventions(&extracted, normalized_prompt, &slug);
+    Ok((slug, finalized))
+}
+
+pub(crate) fn save_generated_script_from_response(
+    prompt: &str,
+    raw_response: &str,
+) -> Result<PathBuf> {
+    let (slug, script_source) = prepare_script_from_ai_response(prompt, raw_response)?;
+    let script_path = crate::script_creation::create_new_script(&slug).with_context(|| {
+        format!(
+            "Failed to create script for AI response (state=create_failed, slug={})",
+            slug
+        )
+    })?;
+
+    fs::write(&script_path, script_source).with_context(|| {
+        format!(
+            "Failed writing script for AI response (state=write_failed, path={})",
+            script_path.display()
+        )
+    })?;
+
+    Ok(script_path)
+}
+
+pub(crate) fn run_saved_script_detached(path: &Path) -> Result<()> {
+    let script_path = path.to_string_lossy().to_string();
+    let attempts = vec![
+        ("kit", vec!["run".to_string(), script_path.clone()]),
+        ("bun", vec!["run".to_string(), script_path.clone()]),
+    ];
+
+    let mut failures = Vec::new();
+
+    for (cmd, args) in attempts {
+        let mut command = std::process::Command::new(cmd);
+        command.args(&args);
+        match command.spawn() {
+            Ok(child) => {
+                drop(child);
+                return Ok(());
+            }
+            Err(error) => {
+                failures.push(format!("{} {:?}: {}", cmd, args, error));
+            }
+        }
+    }
+
+    anyhow::bail!(
+        "Failed to execute saved script (path={}, attempts={})",
+        path.display(),
+        failures.join(" | ")
+    );
+}
+
 fn select_generation_model(
     registry: &ProviderRegistry,
 ) -> Result<(ModelInfo, Arc<dyn AiProvider>)> {
@@ -425,4 +490,31 @@
         assert_eq!(first.file_name().unwrap(), "my-script.ts");
         assert_eq!(second.file_name().unwrap(), "my-script-1.ts");
     }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_adds_conventions_when_ai_omits_them() {
+        let prompt = "Create a weather checker";
+        let response = "await div(\"Sunny\");";
+
+        let (slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert_eq!(slug, "create-a-weather-checker");
+        assert!(source.contains("// Name: Create A Weather Checker"));
+        assert!(source.contains("// Description: Create a weather checker"));
+        assert!(source.contains("import \"@johnlindquist/kit\";"));
+        assert!(source.contains("await div(\"Sunny\");"));
+    }
+
+    #[test]
+    fn test_prepare_script_from_ai_response_extracts_typescript_fence_when_present() {
+        let prompt = "Build script";
+        let response = r#"
+```typescript
+await arg("Name?");
+```
+"#;
+
+        let (_slug, source) = prepare_script_from_ai_response(prompt, response).unwrap();
+        assert!(source.contains("await arg(\"Name?\");"));
+        assert!(!source.contains("```"));
+    }
 }
diff --git a/src/app_impl/prompt_ai.rs b/src/app_impl/prompt_ai.rs
index c985a5090574816267103019d1b272e1e3151a38..7f8acfae4036d0f77ae1eca73994c8d188bfb44e
--- a/src/app_impl/prompt_ai.rs
+++ b/src/app_impl/prompt_ai.rs
@@ -464,6 +464,145 @@
         cx.notify();
     }
 
+    /// Show script generation chat prompt with built-in AI provider support.
+    /// This opens ChatPrompt in script-generation mode with the script generation system prompt.
+    pub fn show_script_generation_chat(
+        &mut self,
+        initial_query: Option<String>,
+        cx: &mut Context<Self>,
+    ) {
+        use crate::ai::ProviderRegistry;
+        use crate::prompts::{ChatEscapeCallback, ChatPrompt, ChatSubmitCallback};
+
+        // Mark as opened from main menu so ESC returns to main menu
+        self.opened_from_main_menu = true;
+
+        // Create escape callback that signals via channel
+        let escape_sender = self.inline_chat_escape_sender.clone();
+        let escape_callback: ChatEscapeCallback = std::sync::Arc::new(move |_id| {
+            let _ = escape_sender.try_send(());
+        });
+
+        // Use cached registry if available, otherwise build synchronously as fallback
+        let registry = self
+            .cached_provider_registry
+            .clone()
+            .unwrap_or_else(|| ProviderRegistry::from_environment_with_config(Some(&self.config)));
+
+        if !registry.has_any_provider() {
+            crate::logging::log(
+                "CHAT_SCRIPT_GEN",
+                "No AI providers configured - showing setup card",
+            );
+
+            // Create configure callback that signals via channel
+            let configure_sender = self.inline_chat_configure_sender.clone();
+            let configure_callback: crate::prompts::ChatConfigureCallback =
+                std::sync::Arc::new(move || {
+                    crate::logging::log(
+                        "CHAT_SCRIPT_GEN",
+                        "Configure callback triggered - sending signal",
+                    );
+                    let _ = configure_sender.try_send(());
+                });
+
+            // Create Claude Code callback that signals via channel
+            let claude_code_sender = self.inline_chat_claude_code_sender.clone();
+            let claude_code_callback: crate::prompts::ChatClaudeCodeCallback =
+                std::sync::Arc::new(move || {
+                    crate::logging::log(
+                        "CHAT_SCRIPT_GEN",
+                        "Claude Code callback triggered - sending signal",
+                    );
+                    let _ = claude_code_sender.try_send(());
+                });
+
+            // Create a no-op submit callback since we're in setup mode
+            let noop_callback: ChatSubmitCallback = std::sync::Arc::new(|_id, _text| {
+                crate::logging::log(
+                    "CHAT_SCRIPT_GEN",
+                    "No providers - submission ignored (setup mode)",
+                );
+            });
+
+            let chat_prompt = ChatPrompt::new(
+                "script-generation-setup".to_string(),
+                Some("Configure API key to generate scripts...".to_string()),
+                vec![],
+                None, // No hint needed - setup card is the UI
+                None,
+                self.focus_handle.clone(),
+                noop_callback,
+                std::sync::Arc::clone(&self.theme),
+            )
+            .with_title("Generate Script with AI")
+            .with_save_history(false) // Don't save setup state to history
+            .with_escape_callback(escape_callback.clone())
+            .with_needs_setup(true)
+            .with_configure_callback(configure_callback)
+            .with_claude_code_callback(claude_code_callback);
+
+            let entity = cx.new(|_| chat_prompt);
+            self.current_view = AppView::ChatPrompt {
+                id: "script-generation-setup".to_string(),
+                entity,
+            };
+            self.focused_input = FocusedInput::None;
+            self.pending_focus = Some(FocusTarget::ChatPrompt);
+            resize_to_view_sync(ViewType::DivPrompt, 0);
+            cx.notify();
+            return;
+        }
+
+        crate::logging::log(
+            "CHAT_SCRIPT_GEN",
+            &format!(
+                "Showing script generation chat with {} providers",
+                registry.provider_ids().len()
+            ),
+        );
+
+        // Create a no-op callback since built-in AI handles submissions internally
+        let noop_callback: ChatSubmitCallback = std::sync::Arc::new(|_id, _text| {
+            // Built-in AI mode handles this internally
+        });
+
+        let placeholder = Some("Describe the script you want to generate...".to_string());
+
+        let mut chat_prompt = ChatPrompt::new(
+            "script-generation".to_string(),
+            placeholder,
+            vec![],
+            None,
+            None,
+            self.focus_handle.clone(),
+            noop_callback,
+            std::sync::Arc::clone(&self.theme),
+        )
+        .with_title("Generate Script with AI")
+        .with_save_history(false)
+        .with_escape_callback(escape_callback)
+        .with_builtin_ai(registry, true) // true = prefer Vercel AI Gateway
+        .with_builtin_system_prompt(crate::ai::script_generation::AI_SCRIPT_GENERATION_SYSTEM_PROMPT)
+        .with_script_generation_mode(true);
+
+        // If there's an initial query, set it in the input and auto-submit
+        if let Some(query) = initial_query {
+            chat_prompt.input.set_text(&query);
+            chat_prompt = chat_prompt.with_pending_submit(true);
+        }
+
+        let entity = cx.new(|_| chat_prompt);
+        self.current_view = AppView::ChatPrompt {
+            id: "script-generation".to_string(),
+            entity,
+        };
+        self.focused_input = FocusedInput::None;
+        self.pending_focus = Some(FocusTarget::ChatPrompt);
+        resize_to_view_sync(ViewType::DivPrompt, 0);
+        cx.notify();
+    }
+
     /// Generate a Script Kit script from a natural-language prompt using the built-in AI backend.
     /// The generated script is saved to disk and opened in the configured editor.
     pub fn generate_script_from_ai_prompt(
diff --git a/src/app_impl/startup.rs b/src/app_impl/startup.rs
index 441b4f23bca1b6294025c698eeb11d45815e85e0..4fd4dc8c23d8df925c657b8db59f888b855ea6a2
--- a/src/app_impl/startup.rs
+++ b/src/app_impl/startup.rs
@@ -723,7 +723,7 @@
                             }
 
                             // Handle Tab/Shift+Tab in ScriptList view for AI actions.
-                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
+                            // Tab opens Ask AI chat, Shift+Tab opens script generation chat mode.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
@@ -734,7 +734,7 @@
                                 this.filter_text.clear();
 
                                 if has_shift {
-                                    this.generate_script_from_ai_prompt(query, cx);
+                                    this.show_script_generation_chat(Some(query), cx);
                                 } else {
                                     // Show inline AI chat with the query as initial input
                                     this.show_inline_ai_chat(Some(query), cx);
diff --git a/src/app_impl/startup_new_tab.rs b/src/app_impl/startup_new_tab.rs
index 7c84270cbe91c0b1ddd460aa29db8c788766d0cf..c75a27bf67a5448f59c7700079e8818210722bde
--- a/src/app_impl/startup_new_tab.rs
+++ b/src/app_impl/startup_new_tab.rs
@@ -174,7 +174,7 @@
                             }
 
                             // Handle Tab/Shift+Tab in ScriptList view for AI actions.
-                            // Tab opens Ask AI chat, Shift+Tab generates a script from the input text.
+                            // Tab opens Ask AI chat, Shift+Tab opens script generation chat mode.
                             if matches!(this.current_view, AppView::ScriptList)
                                 && !this.filter_text.is_empty()
                                 && !this.show_actions_popup
@@ -185,7 +185,7 @@
                                 this.filter_text.clear();
 
                                 if has_shift {
-                                    this.generate_script_from_ai_prompt(query, cx);
+                                    this.show_script_generation_chat(Some(query), cx);
                                 } else {
                                     // Show inline AI chat with the query as initial input
                                     this.show_inline_ai_chat(Some(query), cx);
diff --git a/src/prompts/chat/actions.rs b/src/prompts/chat/actions.rs
index 74a2c7b68f58e938dfe8c80add589e83861a64f3..9192b2250531aa674b529af2b11a26d1690f8307
--- a/src/prompts/chat/actions.rs
+++ b/src/prompts/chat/actions.rs
@@ -129,6 +129,95 @@
         self.clear_messages(cx);
     }
 
+    pub(super) fn handle_script_generation_action(
+        &mut self,
+        action: ScriptGenerationAction,
+        cx: &mut Context<Self>,
+    ) {
+        let Some((prompt_description, raw_response)) = self.latest_script_generation_draft() else {
+            self.set_script_generation_status(true, "No generated script to save yet.", cx);
+            return;
+        };
+
+        logging::log(
+            "CHAT_SCRIPT_GEN",
+            &format!(
+                "state=save_requested action={:?} prompt_len={} response_len={}",
+                action,
+                prompt_description.len(),
+                raw_response.len()
+            ),
+        );
+
+        let script_path = match crate::ai::script_generation::save_generated_script_from_response(
+            &prompt_description,
+            &raw_response,
+        ) {
+            Ok(path) => path,
+            Err(error) => {
+                self.set_script_generation_status(
+                    true,
+                    format!("Failed to save script: {}", error),
+                    cx,
+                );
+                return;
+            }
+        };
+
+        let script_name = script_path
+            .file_name()
+            .and_then(|name| name.to_str())
+            .unwrap_or("generated script")
+            .to_string();
+
+        if action.should_run_after_save() {
+            match crate::ai::script_generation::run_saved_script_detached(&script_path) {
+                Ok(()) => {
+                    self.set_script_generation_status(
+                        false,
+                        format!("Saved and running {}", script_name),
+                        cx,
+                    );
+                    logging::log(
+                        "CHAT_SCRIPT_GEN",
+                        &format!(
+                            "state=run_started action={:?} path={}",
+                            action,
+                            script_path.display()
+                        ),
+                    );
+                }
+                Err(error) => {
+                    self.set_script_generation_status(
+                        true,
+                        format!("Saved {} but failed to run: {}", script_name, error),
+                        cx,
+                    );
+                    logging::log(
+                        "CHAT_SCRIPT_GEN",
+                        &format!(
+                            "state=run_failed action={:?} path={} error={}",
+                            action,
+                            script_path.display(),
+                            error
+                        ),
+                    );
+                }
+            }
+            return;
+        }
+
+        self.set_script_generation_status(false, format!("Saved {}", script_name), cx);
+        logging::log(
+            "CHAT_SCRIPT_GEN",
+            &format!(
+                "state=saved_only action={:?} path={}",
+                action,
+                script_path.display()
+            ),
+        );
+    }
+
     // ============================================
     // Actions Menu Methods
     // ============================================
diff --git a/src/prompts/chat/mod.rs b/src/prompts/chat/mod.rs
index 7dd2dc3e82c2339c0af98762e6c00e680be18a93..62192b6f95c7250cce9ab48d271e12ace5ee4647
--- a/src/prompts/chat/mod.rs
+++ b/src/prompts/chat/mod.rs
@@ -49,7 +49,8 @@
 use self::types::{
     build_conversation_turns, default_conversation_starters, next_chat_scroll_follow_state,
     next_reveal_boundary, resolve_chat_input_key_action, resolve_setup_card_key,
-    should_ignore_stream_reveal_update, ChatInputKeyAction, ChatScrollDirection, SetupCardAction,
+    should_ignore_stream_reveal_update, should_show_script_generation_actions,
+    ChatInputKeyAction, ChatScrollDirection, ScriptGenerationAction, SetupCardAction,
 };
 
 pub use prompt::ChatPrompt;
diff --git a/src/prompts/chat/prompt.rs b/src/prompts/chat/prompt.rs
index c22d02f04059a37be0367233c16e620a64bfe483..4ef4ac1aae4123a9bdd9771500998f87f424660b
--- a/src/prompts/chat/prompt.rs
+++ b/src/prompts/chat/prompt.rs
@@ -27,6 +27,7 @@
     pub(super) provider_registry: Option<ProviderRegistry>,
     pub(super) available_models: Vec<ModelInfo>,
     pub(super) selected_model: Option<ModelInfo>,
+    pub(super) builtin_system_prompt: Option<String>,
     pub(super) builtin_streaming_content: String,
     pub(super) builtin_is_streaming: bool,
     // Word-buffered reveal: full accumulated content from provider and reveal watermark
@@ -48,6 +49,10 @@
     pub(super) loading_providers: bool,
     // Setup mode: when true, shows API key configuration card instead of chat
     pub(super) needs_setup: bool,
+    // Script generation mode: enables post-response Save/Run actions
+    pub(super) script_generation_mode: bool,
+    pub(super) script_generation_status: Option<String>,
+    pub(super) script_generation_status_is_error: bool,
     // Setup card keyboard focus (0 = Configure Vercel, 1 = Claude Code)
     pub(super) setup_focus_index: usize,
     pub(super) on_configure: Option<ChatConfigureCallback>,
@@ -109,6 +114,7 @@
             provider_registry: None,
             available_models: Vec::new(),
             selected_model: None,
+            builtin_system_prompt: None,
             builtin_streaming_content: String::new(),
             builtin_is_streaming: false,
             builtin_accumulated_content: String::new(),
@@ -121,6 +127,9 @@
             cursor_blink_started: false,
             loading_providers: false,
             needs_setup: false,
+            script_generation_mode: false,
+            script_generation_status: None,
+            script_generation_status_is_error: false,
             setup_focus_index: 0,
             on_configure: None,
             on_claude_code: None,
@@ -307,6 +316,18 @@
         self
     }
 
+    /// Set a fixed system prompt used for built-in AI submissions.
+    pub fn with_builtin_system_prompt(mut self, prompt: impl Into<String>) -> Self {
+        self.builtin_system_prompt = Some(prompt.into());
+        self
+    }
+
+    /// Enable script generation mode, which shows Save/Run actions after responses complete.
+    pub fn with_script_generation_mode(mut self, enabled: bool) -> Self {
+        self.script_generation_mode = enabled;
+        self
+    }
+
     /// Set pending_submit flag - when true, auto-submit input on first render
     /// Used for Tab from main menu to immediately send the query to AI
     pub fn with_pending_submit(mut self, submit: bool) -> Self {
@@ -439,4 +460,54 @@
     pub fn has_builtin_ai(&self) -> bool {
         self.provider_registry.is_some()
     }
+
+    pub(super) fn clear_script_generation_status(&mut self) {
+        self.script_generation_status = None;
+        self.script_generation_status_is_error = false;
+    }
+
+    pub(super) fn set_script_generation_status(
+        &mut self,
+        is_error: bool,
+        message: impl Into<String>,
+        cx: &mut Context<Self>,
+    ) {
+        self.script_generation_status = Some(message.into());
+        self.script_generation_status_is_error = is_error;
+        cx.notify();
+    }
+
+    pub(super) fn latest_script_generation_draft(&self) -> Option<(String, String)> {
+        if !self.script_generation_mode {
+            return None;
+        }
+
+        for (index, message) in self.messages.iter().enumerate().rev() {
+            if message.is_user() || message.streaming || message.error.is_some() {
+                continue;
+            }
+
+            let script_source = message.get_content().trim();
+            if script_source.is_empty() {
+                continue;
+            }
+
+            if let Some(user_message) = self.messages[..index].iter().rev().find(|m| m.is_user()) {
+                let prompt_description = user_message.get_content().trim();
+                if !prompt_description.is_empty() {
+                    return Some((prompt_description.to_string(), script_source.to_string()));
+                }
+            }
+        }
+
+        None
+    }
+
+    pub(super) fn should_show_script_generation_actions(&self) -> bool {
+        should_show_script_generation_actions(
+            self.script_generation_mode,
+            self.is_streaming(),
+            self.latest_script_generation_draft().is_some(),
+        )
+    }
 }
diff --git a/src/prompts/chat/streaming.rs b/src/prompts/chat/streaming.rs
index 8d317a6bd7e6d806965e8d625efec4a936ee0242..da143e9e2bb06463f21e81bb81c3588dc76d9091
--- a/src/prompts/chat/streaming.rs
+++ b/src/prompts/chat/streaming.rs
@@ -11,6 +11,7 @@
         }
         logging::log("CHAT", &format!("User submitted: {}", text));
         self.input.clear();
+        self.clear_script_generation_status();
 
         // If built-in AI mode is enabled, handle the AI call directly
         if self.has_builtin_ai() {
@@ -111,6 +112,10 @@
         // Build messages for the API call (convert our messages to provider format)
         let mut api_messages: Vec<ProviderMessage> = Vec::new();
 
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+
         // If slash command detected, prepend system context
         if let Some(ref ctx) = system_context {
             api_messages.push(ProviderMessage::system(ctx.clone()));
@@ -234,7 +239,11 @@
         };
 
         // Build messages for the API call (convert our messages to provider format)
-        let api_messages: Vec<ProviderMessage> = self
+        let mut api_messages: Vec<ProviderMessage> = Vec::new();
+        if let Some(system_prompt) = self.builtin_system_prompt.clone() {
+            api_messages.push(ProviderMessage::system(system_prompt));
+        }
+        api_messages.extend(self
             .messages
             .iter()
             .map(|m| {
@@ -245,8 +254,7 @@
                 } else {
                     ProviderMessage::assistant(m.get_content())
                 }
-            })
-            .collect();
+            }));
 
         // Set streaming state
         self.builtin_is_streaming = true;
diff --git a/src/prompts/chat/types.rs b/src/prompts/chat/types.rs
index 286c060bbf893558e05d81e932ed3ab61a09d06a..2ab2ee2e26318b52e2014c73ae18176ef4e45c6d
--- a/src/prompts/chat/types.rs
+++ b/src/prompts/chat/types.rs
@@ -118,6 +118,27 @@
 }
 
 #[derive(Clone, Copy, Debug, PartialEq, Eq)]
+pub(crate) enum ScriptGenerationAction {
+    Save,
+    Run,
+    SaveAndRun,
+}
+
+impl ScriptGenerationAction {
+    pub(crate) fn should_run_after_save(self) -> bool {
+        matches!(self, Self::Run | Self::SaveAndRun)
+    }
+}
+
+pub(crate) fn should_show_script_generation_actions(
+    script_generation_mode: bool,
+    is_streaming: bool,
+    has_draft: bool,
+) -> bool {
+    script_generation_mode && !is_streaming && has_draft
+}
+
+#[derive(Clone, Copy, Debug, PartialEq, Eq)]
 pub(crate) enum ChatInputKeyAction {
     Escape,
     StopStreaming,


thinking
**Adding rendering methods near footer**

[swarm-timeout] max runtime 600s reached; sending SIGTERM


==== exit @ 2026-02-08T03:36:51.458Z code=0 status=needs_split ====

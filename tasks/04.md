Below is a blunt, code-level review of what you actually have in these 6 files (not the trait snippet in the prompt). I’m going to call out correctness bugs first, then design/architecture, then security + UX.

---

## Biggest correctness gaps (fix these first)

### 1) BYOK isn’t actually “per chat” right now

You **store** `chat.model_id` + `chat.provider` in SQLite (`chats` table)… but when you send a message you **ignore it**.

In `AiApp::start_streaming_response()` you always use `self.selected_model` (the “new chats” picker), not the current chat’s stored model/provider.

**Impact:** existing chats can silently switch providers/models depending on whatever the picker is set to at the time you type. That’s not what users expect, and it breaks your stated BYOK architecture.

**Fix direction:**

* On `select_chat()`, set `selected_model` to match that chat’s `model_id` (if present in `available_models`).
* On send/stream, use `chat.model_id` (and provider derived from it), not the global picker.
* If you want to allow switching mid-chat, make that explicit and persist it: update `chats.model_id/provider` when the user changes the model.

### 2) Your sidebar ordering / date grouping will drift stale

You cache `self.chats` once at startup (and on search-clear). But when new messages arrive:

* SQLite updates `chats.updated_at` (via `save_message_internal()`).
* Your in-memory `Chat` objects **do not** get `updated_at` updated (except when you auto-title the first message).
* You also never reorder `self.chats` after activity.

**Impact:** chat list won’t float active conversations to the top, and date grouping can be wrong.

**Fix direction (minimal):**

* After saving a user message and after finishing the assistant message, do:

  * `touch()` the in-memory chat (or reload it from DB)
  * move it to the top of `self.chats`
* Or simplest and safest: reload `self.chats = storage::get_all_chats()` after message send + after assistant finish.

### 3) Deleting a selected chat doesn’t update the main panel messages

`delete_selected_chat()` sets `selected_chat_id` to the first remaining chat, but it **does not load messages** for that new selection (or clear them).

**Impact:** UI can show messages from a chat you just deleted until something else triggers a reload.

**Fix direction:**
Call `select_chat(new_id, window, cx)` (or at least reload `current_messages`) after selecting the next chat.

### 4) `init_ai_db()` is not idempotent, but you call it like it is

`AiApp::new()` calls `storage::init_ai_db()` every time. `init_ai_db()` will return an error if `AI_DB` is already set (“AI database already initialized”).

**Impact:** you log an error on normal operation (reopening window, etc.). This will train you to ignore real errors.

**Fix direction:**
Make init idempotent:

* If `AI_DB.get().is_some()` just return `Ok(())`
* Or change init to “get or init” semantics.

### 5) `search_chats()` SQL likely doesn’t work as written (aliasing)

In `storage::search_chats()`, you alias `chats_fts` as `fts` and `messages_fts` as `mfts`, but your WHERE clause uses:

```sql
(chats_fts MATCH ?1 OR messages_fts MATCH ?1)
```

Once a table has an alias, SQLite expects you to use the alias. This query is very likely broken.

Also: the UI never calls `storage::search_chats()` anyway — it only does title substring matching. So you currently **don’t** have message-content search in the product.

**Fix direction:**

* Fix the query to use aliases (`fts MATCH ?1 OR mfts MATCH ?1`)
* Add sanitization / fallback logic (FTS MATCH is fragile with special characters)
* Then actually use it from `on_search_change()` (maybe with debounce)

---

## 1) Provider abstraction review

### Current trait design: decent for “text-only chat MVP”, but it will corner you

Your actual trait is:

* `available_models() -> Vec<ModelInfo>`
* `send_message(messages, model_id) -> Result<String>`
* `stream_message(messages, model_id, on_chunk) -> Result<()>`

This is workable as a thin “text completion” interface. But the minute you add:

* tools / function calling
* vision / images
* structured outputs (JSON mode)
* usage accounting (input/output tokens)
* refusal metadata / finish reasons
  you’ll need more structure than `String`.

#### What I’d change now (without overengineering)

Introduce typed request/response/event types, even if you only fill a subset initially.

**Example shape:**

```rust
pub struct AiRequest {
    pub model_id: String,
    pub messages: Vec<AiMessage>,
    pub max_output_tokens: Option<u32>,
    pub temperature: Option<f32>,
    pub system: Option<String>,
    pub tools: Vec<AiTool>,          // empty for now
    pub attachments: Vec<Attachment> // empty for now (vision later)
}

pub struct AiResponse {
    pub content: Vec<AiContentBlock>, // text blocks, tool blocks later
    pub usage: Option<Usage>,
    pub finish_reason: Option<String>,
}

pub enum AiStreamEvent {
    TextDelta(String),
    ToolCallDelta(ToolCallDelta),
    Usage(Usage),
    Done,
}
```

Then your provider interface becomes stable even as capabilities grow.

### Async vs sync: you’re currently fine, but future you will want async

Right now you’re using blocking `ureq`, and you correctly push it onto a background thread. That’s acceptable.

But the moment you want:

* cancellation that doesn’t “hang” on reads
* streaming that parses real SSE events reliably
* unified retries/backoff with timers
* concurrent streams (multiple chats, background summarization)
  async becomes dramatically easier.

**Practical recommendation:**

* Keep a synchronous façade for the UI layer if you want.
* Internally, move providers to async (`async_trait`) and stream via `futures::Stream`.
* Or keep sync but implement a proper SSE parser + request timeouts and a cancellation flag checked frequently.

### Provider-specific features (tools, vision): don’t cram into the trait with `Option<T>` soup

The clean approach is:

* Put *capabilities* on `ModelInfo` (or a `ModelCapabilities` struct).
* Put optional fields on `AiRequest`.
* Providers return a clear `UnsupportedFeature` error if requested features aren’t supported.

That keeps the UI consistent and avoids “if provider == anthropic { ... }” logic in your window code.

### Rate limiting & error handling: you need a shared error vocabulary

Right now you bubble `anyhow::Result` with little classification. That’s fine for development but poor UX.

Create a `AiError` enum (or struct with fields) that captures:

* `kind`: Auth, RateLimited, QuotaExceeded, BadRequest, Network, Timeout, Unknown
* `retry_after`: optional
* `provider`: string
* `status`: optional http status
* `message`: safe-to-display message
* `raw`: optional raw body (not displayed)

Then map provider responses into that.

**Why:** the UI can render “Your key is invalid” vs “Rate-limited, retry in 12s” vs “Model not found”.

---

## 2) Streaming implementation audit

### SSE parsing is “happy path only”

Both OpenAI and Anthropic parsing assumes:

* each event is exactly one `data: {json}` line
* line endings are clean
* no multi-line `data:` payloads
* no need to stop on `[DONE]`

This is fragile.

#### Concrete issues in your code

* You don’t trim `\r` (CRLF). `BufRead::lines()` strips `\n` but can leave `\r`, which breaks JSON parsing and the `[DONE]` check.
* You ignore multi-line SSE data (legal per SSE spec).
* You don’t break on `[DONE]`. You just stop when the socket closes (usually fine, but not guaranteed).

#### A robust SSE parser is small and worth it

The correct approach is: accumulate lines until a blank line; concatenate all `data:` lines with `\n`; then parse once per event.

Even if OpenAI/Anthropic don’t *currently* send multi-line JSON, this gives you correctness and future-proofing.

### Partial JSON in chunks

With `lines()`, you *usually* won’t see partial JSON because it reads until newline. But you can still see partial or split events if the server emits weird framing or proxies interfere.

A proper SSE event parser handles this.

### UI performance: you’re doing O(n²) work for long outputs

Your streaming thread builds a single growing `String`. Your poller clones the **entire** accumulated string every ~50ms:

```rust
let current = content.clone();
app.streaming_content = current;
```

For long outputs, this is wasteful.

**Better pattern:**

* Stream deltas over a channel (or keep a `last_len` and only append the new substring).
* Update UI at a cadence (e.g., 30–60ms) but only apply incremental appends.

### Stream interruptions: you throw away partial output

On error, you clear `streaming_content` and don’t save anything.

**Better UX:**

* Keep partial content visible
* Save it as an assistant message flagged “incomplete”
* Show an inline error banner with actions: Retry / Continue / Copy partial

### Cancellation: currently missing

You have no stop button and no cancellation path in the provider code.

If you stay synchronous:

* add an `Arc<AtomicBool>` cancellation flag
* in the SSE read loop, check it between reads and break
* set short-ish read timeouts so you’re not blocked forever waiting for a line

If you go async:

* cancellation is straightforward by dropping the task / using a cancellation token

---

## 3) Security assessment (keys, logs, stored chats)

### API keys: env vars are okay, plaintext `.env` is risky unless you enforce permissions

Environment variables are a reasonable BYOK story, but `.env` files are often:

* checked into backups
* readable with lax permissions
* leaked via support bundles

If you support `~/.sk/kit/.env`, enforce:

* file mode `0600` (owner read/write only)
* warn loudly if permissions are broader
* never log key presence beyond “configured”

**Better:** support OS credential storage (Keychain on macOS, Credential Manager on Windows, libsecret on Linux). It’s worth it because this is literally the product’s authentication.

### Key rotation

Today, key changes require restarting the AI window (or the whole app). Users will do that, but you can do better:

* Add “Reload providers” action
* On auth failure, allow user to paste/update key and retry

### Logging: you are currently logging user-derived content in a few places

You do **not** log message content (good), but you do log:

* chat titles (which are derived from the first user message)
* search queries in `set_ai_search`
* debug logs include `title = %chat.title`

That can leak PII/secrets into logs.

**Recommendation:**

* never log any user text by default (even at debug)
* if you absolutely need it, gate behind an explicit “diagnostics mode” toggle

### Stored chats: plaintext SQLite means secrets live on disk forever

Users will paste keys, tokens, passwords, proprietary code. It will happen.

At minimum:

* Provide a “Disable history” or “Incognito mode”
* Provide “Clear chat / clear all” (you have it in storage but not in UI)
* Consider encryption-at-rest for the DB (harder, but doable with OS keychain + an encryption key)

Also note: your FTS tables duplicate message content in index structures, which further increases the footprint of sensitive text.

---

## 4) Chat persistence review

### Schema is okay for MVP; a few fixes improve correctness and scale

**Timestamp storage**
You store RFC3339 strings and order by them. That’s mostly okay, but there’s a subtle risk: RFC3339 strings with and without fractional seconds don’t always sort lexicographically the way you expect.

**Best practice:** store timestamps as `INTEGER` (unix millis) and convert at the edges.

**Foreign keys**
SQLite foreign keys are off by default. You declare `FOREIGN KEY ... ON DELETE CASCADE` but don’t enable it.

Add:

* `PRAGMA foreign_keys = ON;`
* and consider `PRAGMA journal_mode = WAL;` for better concurrent reads/writes.

**FTS**

* Fix the alias issue
* Add a sanitization function and fallback to `LIKE` on FTS parse errors
* Consider searching both title and content, then ranking results (FTS5 `bm25()`)

### Export/import

You’re already using `serde` for the models. Export is easy:

* dump `Chat` + `Vec<Message>` to JSON
* optionally include a “portable format version”
  Import can:
* reassign IDs or preserve them
* handle collisions by generating new ChatId

This is a real UX win and de-risks “local-only” storage.

### Context length limits (not implemented)

You have `ModelInfo.context_window`, but you never use it.

If you want this to feel like modern chat UIs:

* estimate input tokens before sending
* trim oldest messages when you exceed budget
* optionally maintain a rolling summary message stored per chat

Even a crude heuristic (“keep last N messages”) is better than silently hitting provider errors.

---

## 5) UI/UX recommendations (compared to ChatGPT/Claude expectations)

### You’re close on layout, but missing the things users notice immediately

**Model picker**

* Right now it’s not a picker, it’s a “cycle button”.
* Users want a dropdown with:

  * provider grouping
  * model name
  * context window size
  * capability badges (streaming, vision, tools)

**Markdown rendering**

* Your UI renders raw text. Code blocks and headings look bad.
* If you can’t do full markdown immediately, at least:

  * render fenced code blocks in monospace with a background
  * preserve newlines and indentation
  * linkify URLs (careful: privacy + accidental clicks)

**Streaming controls**

* Add “Stop generating”
* Add “Regenerate response”
* Add “Copy” on assistant messages

**System prompts**

* You have `MessageRole::System` and provider support for system, but no UX to set it.
* A per-chat “Instructions” drawer (like ChatGPT’s system/developer separation) is enough.

**Search**

* Title-only search isn’t competitive.
* Use FTS for message content and highlight matches in preview.

**Chat list behavior**

* Must move active chats to top.
* Must reflect updated_at correctly (see bug above).

**Conversation branching**

* Not required for parity, but if you want it:

  * “Fork” button duplicates the chat up to a selected message
  * store `parent_chat_id` + `forked_from_message_id`

**Multi-modal**

* Don’t bolt it onto `content: String`.
* Model `Message` content as blocks or attachments. Even if you only support text now, the data model decision matters.

---

## 6) Architecture suggestions for scaling to more providers

### Put an “AI service” layer between UI and providers

Right now `window.rs` knows too much:

* provider selection
* message mapping
* streaming orchestration
* storage writes

Introduce `AiChatService` that owns:

* selecting provider/model per chat
* building request payloads
* trimming context / summarization
* retry/backoff
* streaming event normalization
* persisting messages + updating chat metadata

Then the UI just:

* dispatches “send message”
* subscribes to `AiStreamEvent`s
* renders state

This is how you avoid a 3,000-line UI file that becomes impossible to change safely.

### Normalize streaming to events, not “shared growing string”

Instead of `shared_content: Mutex<String>`, use a channel:

* worker thread reads SSE → emits `TextDelta("...")`
* UI consumes deltas and appends to state
* you can also emit `Usage(...)` and `Done`

This fixes performance and simplifies cancellation/retries.

### Cache model → provider mapping

`find_provider_for_model()` rebuilds vectors each time. It’s small now, but it’s a smell. Cache it once when registry builds:

* `HashMap<String, Arc<dyn AiProvider>>` keyed by model_id
  Then lookup is O(1), no allocation.

### Testing that matters

You already have decent unit tests on parsing functions. Add:

* SSE parser tests with CRLF, multi-line events, `[DONE]`
* DB migration tests / FTS search tests
* “BYOK per chat” tests (select a chat with model A, ensure send uses A even if picker set to B)

---

## Quick “do this next” checklist (highest leverage)

1. Make DB init idempotent (stop logging false errors).
2. Make send/stream use the chat’s stored model/provider.
3. Update in-memory chat updated_at + reorder chat list on new messages.
4. Fix delete-selected-chat to reload/clear `current_messages`.
5. Implement a real SSE event parser (trim CRLF, stop on DONE, handle multi-line).
6. Normalize errors into user-visible categories (auth / rate limit / network).
7. Add a “Stop” button and preserve partial output on errors.
8. Upgrade search to include message content (FTS + sanitization + fallback).
9. Stop logging user-derived text (chat titles, search queries) by default.
10. Add at least minimal markdown rendering for code blocks.

If you want, I can sketch the exact refactor steps to move streaming to an event channel and get rid of the “clone the whole string every 50ms” pattern without changing your UI structure too much.

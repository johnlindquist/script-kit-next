Here’s the “bundle” you actually need: a clean UX flow + a concrete implementation blueprint that fits the file layout you listed, and doesn’t force users to hand-edit env vars just to try Vercel AI Gateway.

The main design choice that keeps this simple: **treat Vercel AI Gateway as an OpenAI‑compatible backend** (same chat-completions shape), so you can reuse almost all of your existing OpenAI provider code—just swap `base_url` and the credential source. Vercel AI Gateway supports **API key auth** and **OIDC token auth** (dev tokens expire and need refreshing). ([Vercel][1])

---

## Target UX

### When the setup wizard shows

Show the wizard when **no working provider config** is present, instead of dumping users into a dead chat window.

**Rule of thumb:**

* If user has any valid provider configured → show chat normally.
* If not → show “Set up AI” wizard with provider cards.

### Wizard step flow (Vercel AI Gateway)

**Step 1: Choose provider**

* Cards: OpenAI, Anthropic, **Vercel AI Gateway**
* Small subtext under Vercel: “One endpoint, many models.”

**Step 2: Choose auth method**
Two tabs (or radio buttons):

1. **API Key**
2. **OIDC Token** (recommended when linked to a Vercel project)

Vercel supports both methods. ([Vercel][1])

**Step 3: Enter credential**

* API Key: password-style input
* OIDC Token: password-style input + warning text:

  * “Dev OIDC tokens expire (typically 12 hours). Refresh by running `vercel env pull` again, or use `vercel dev` for auto-refresh.” ([Vercel][1])

**Step 4: Pick a model (optional but strongly recommended)**

* Populate model dropdown by calling Vercel’s `/models`.
* Vercel docs explicitly say you can query the models endpoint via REST and it “requires no authentication.” ([Vercel][2])
  This is huge for UX: users can browse models before they’ve entered a secret.

**Step 5: “Test connection”**
One button:

* Runs a *minimal* chat completion against the selected model (`max_tokens: 1`).
* If 200 → show “Success” and enable “Save”.
* If 401/403 → show “Invalid token/key or not permitted”.
* If payment/credits issue → show it clearly (don’t call it “invalid key”).

**Step 6: Save**
Store:

* Provider = Vercel AI Gateway
* Credential (api key or oidc token)
* Base URL (default)
* Selected model

Also add a small “Advanced” accordion:

* Base URL override (rarely needed)
* Optional headers (if you support them elsewhere)

---

## What to implement in each file

### 1) `src/ai/config.rs` — add Vercel config + detection

You need:

* A new provider variant
* A new `VercelGatewayConfig`
* Precedence rules (API key > OIDC token)

Vercel’s AI Gateway provider conventions:

* API key env var: `AI_GATEWAY_API_KEY` ([AI SDK][3])
* OIDC token env var used in local/dev setups: `VERCEL_OIDC_TOKEN` (commonly produced by `vercel env pull`) ([Vercel][4])
* If an API key is set, it should take precedence even if an OIDC token exists (this precedence is called out in docs for the gateway tooling). ([AI SDK][3])

**Config shape (suggested):**

```rust
pub enum AiProviderKind {
    OpenAI,
    Anthropic,
    VercelAIGateway,
}

pub struct VercelGatewayConfig {
    pub base_url: String,           // default: https://ai-gateway.vercel.sh/v1
    pub api_key: Option<String>,    // store securely if possible
    pub oidc_token: Option<String>, // store securely if possible
    pub default_model: Option<String>,
}

impl VercelGatewayConfig {
    pub fn credential(&self) -> Option<VercelCredential> {
        if let Some(k) = self.api_key.as_ref().filter(|s| !s.trim().is_empty()) {
            return Some(VercelCredential::ApiKey(k.clone()));
        }
        if let Some(t) = self.oidc_token.as_ref().filter(|s| !s.trim().is_empty()) {
            return Some(VercelCredential::OidcToken(t.clone()));
        }
        None
    }
}
```

**Environment detection (suggested logic):**

* If user explicitly selected provider in app settings → respect it.
* Else if `AI_GATEWAY_API_KEY` or `VERCEL_OIDC_TOKEN` present → Vercel is “available”.
* Else fall back to current OpenAI/Anthropic checks.

(You don’t want “accidental Vercel selection” just because someone has a stray token—so treat it as “available” unless chosen.)

---

### 2) `src/ai/providers.rs` — implement Vercel provider as OpenAI-compatible

Implementation strategy:

* Reuse your OpenAI provider request/response structs.
* Inject:

  * `base_url = https://ai-gateway.vercel.sh/v1` (default)
  * `Authorization: Bearer <token>`

AI SDK’s gateway provider docs state the API key is sent using the `Authorization` header and defaults to `AI_GATEWAY_API_KEY`. ([AI SDK][3])
And multiple integrations use the OpenAI schema with base URL `https://ai-gateway.vercel.sh/v1`. ([LiteLLM][5])

**Provider behavior:**

* `model` is the Vercel Gateway model ID like `openai/gpt-5` or `anthropic/claude-sonnet-4` (you pass it straight through).
* Endpoint: `/chat/completions` (OpenAI schema).

**One important UX/polish detail:**
When users pick Vercel, **don’t reuse `OPENAI_API_KEY`** internally. That creates confusion and collisions. Use the correct Vercel-centric env vars and config fields.

---

### 3) `src/ai/window.rs` — add setup wizard UI

You’ll add a state machine to the window:

```rust
enum AiWindowMode {
    SetupWizard(SetupState),
    Chat(ChatState),
}
```

The wizard should:

* allow selecting provider
* show Vercel auth tabs
* fetch model list (no-auth) for dropdown ([Vercel][2])
* test credential
* save + switch to chat

**Copy that matters (don’t overthink it):**

**Vercel API Key tab**

* Title: “Vercel AI Gateway API Key”
* Helper: “Create it in Vercel Dashboard → AI Gateway → API Keys.”
* Button: “Test & Save”

**Vercel OIDC tab**

* Title: “Vercel OIDC Token”
* Helper (explicit):
  “For local dev, run `vercel env pull` to generate `VERCEL_OIDC_TOKEN`. Dev tokens expire (typically 12 hours).” ([Vercel][1])
* Optional: show “expires in …” if you parse JWT (below)

---

### 4) `src/ai/vercel_auth.rs` — keep it focused: credential selection + expiry parsing + validation

Don’t try to recreate Vercel CLI inside your app unless you’re sure Script Kit always has a “project directory” concept. The practical implementation is:

**Responsibilities:**

1. Resolve which credential to use:

   * API key (preferred) or OIDC token
2. Build auth header:

   * `Authorization: Bearer <credential>`
3. If credential is OIDC token, optionally decode JWT `exp` and surface expiry warnings
4. Provide `test_connection()` used by the wizard

**Why expiry parsing is worth it:**
OIDC dev tokens expire. That’s the #1 reason users will “it worked yesterday but not today.” Make the UI tell them before it breaks. Docs emphasize refresh requirements and expiry constraints for OIDC tokens. ([Vercel][1])

**Validation approach (recommended):**

* POST `/chat/completions` with:

  * chosen `model`
  * one short user message
  * `max_tokens = 1`
  * `temperature = 0`

That is the only reliable way to confirm both:

* the secret is accepted, and
* the chosen model is actually usable for that account.

---

## Key validation behavior

Be blunt in the UI about what “test” does:

* ✅ “Success: credential accepted and model responded.”
* ❌ “Unauthorized (401): credential rejected.”
* ❌ “Forbidden (403): credential valid but not permitted for this operation/model.”
* ⚠️ “Payment/Credits issue: credential accepted but account can’t run this model right now.”

This avoids the classic support nightmare: “it says my key is invalid” when it’s actually “out of credits”.

---

## Small-but-important product decisions

These make the UX feel “done” instead of “bolted on”:

1. **Model browsing without auth**

   * Use `/models` to populate the dropdown even before the user enters a secret. Vercel says the models endpoint requires no authentication. ([Vercel][2])

2. **Don’t hide the auth precedence**

   * If user has both API key and OIDC token, show a note:
     “API key takes precedence over OIDC token.” ([Vercel][6])

3. **Make it reversible**

   * Add a “Change Provider…” button in the chat UI that reopens the wizard (pre-filled).

4. **Store secrets properly**

   * If you’re currently using env vars only: that’s fine for an MVP, but it’s not great UX.
   * Best practice: store in OS keychain (macOS Keychain / Windows Credential Manager / libsecret). If you can’t do that now, at least store in a local config file with clear warnings.

---

## What I’d ship first (MVP checklist)

If you want this to land quickly without becoming a month-long auth project:

* [ ] Add Vercel provider config + selection
* [ ] Add wizard screen with Vercel API Key + OIDC Token inputs
* [ ] Fetch model list via `/models` (no-auth)
* [ ] Implement “Test connection” via `/chat/completions`
* [ ] Save provider + model + credential
* [ ] Show OIDC expiry warning (optional but very worth it)

That gets you the “simple UX” you asked for, without dragging you into implementing Sign-in-with-Vercel OAuth flows that may not even be necessary for your users.

---

## URLs

* [https://vercel.com/docs/ai-gateway/authentication](https://vercel.com/docs/ai-gateway/authentication)
* [https://vercel.com/docs/ai-gateway/models-and-providers](https://vercel.com/docs/ai-gateway/models-and-providers)
* [https://ai-sdk.dev/providers/ai-sdk-providers/ai-gateway](https://ai-sdk.dev/providers/ai-sdk-providers/ai-gateway)
* [https://vercel.com/docs/ai-gateway/openai-compat](https://vercel.com/docs/ai-gateway/openai-compat)
* [https://vercel.com/kb/guide/using-vercel-sandbox-claude-agent-sdk](https://vercel.com/kb/guide/using-vercel-sandbox-claude-agent-sdk)
* [https://vercel.com/docs/cli/env](https://vercel.com/docs/cli/env)
* [https://langfuse.com/integrations/gateways/vercel-ai-gateway](https://langfuse.com/integrations/gateways/vercel-ai-gateway)
* [https://docs.litellm.ai/docs/providers/vercel_ai_gateway](https://docs.litellm.ai/docs/providers/vercel_ai_gateway)

[1]: https://vercel.com/docs/ai-gateway/authentication "Authentication"
[2]: https://vercel.com/docs/ai-gateway/models-and-providers "Models & Providers"
[3]: https://ai-sdk.dev/providers/ai-sdk-providers/ai-gateway "AI SDK Providers: AI Gateway"
[4]: https://vercel.com/kb/guide/using-vercel-sandbox-claude-agent-sdk?utm_source=chatgpt.com "Using Vercel Sandbox to run Claude's Agent SDK"
[5]: https://docs.litellm.ai/docs/providers/vercel_ai_gateway?utm_source=chatgpt.com "Vercel AI Gateway"
[6]: https://vercel.com/docs/ai-gateway/openai-compat "OpenAI-Compatible API"
